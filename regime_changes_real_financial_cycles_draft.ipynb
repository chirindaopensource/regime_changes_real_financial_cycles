{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BBnzqjeY1d0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "\n",
        "# Regime Changes and Real-Financial Cycles: Searching Minsky's Hypothesis in a Nonlinear Setting\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2511.04348-b31b1b.svg)](https://arxiv.org/abs/2511.04348)\n",
        "[![Journal](https://img.shields.io/badge/Journal-arXiv%20Preprint-003366)](https://arxiv.org/abs/2511.04348)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/regime_changes_real_financial_cycles)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Macroeconomics%20%7C%20Financial%20Economics-00529B)](https://github.com/chirindaopensource/regime_changes_real_financial_cycles)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-OECD%20Statistics-lightgrey)](https://stats.oecd.org/)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-BIS%20Credit%20Statistics-lightgrey)](https://www.bis.org/statistics/totcredit.htm)\n",
        "[![Data Sources](https://img.shields.io/badge/Data-FRED%20Economic%20Data-lightgrey)](https://fred.stlouisfed.org/)\n",
        "[![Core Method](https://img.shields.io/badge/Method-MS--VAR%20%7C%20EM%20Algorithm%20%7C%20HP%20Filter-orange)](https://github.com/chirindaopensource/regime_changes_real_financial_cycles)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Nonlinear%20Dynamics%20%7C%20Minsky%20Cycles-red)](https://github.com/chirindaopensource/regime_changes_real_financial_cycles)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/matplotlib-%2311557c.svg?style=flat&logo=matplotlib&logoColor=white)](https://matplotlib.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/statsmodels-blue.svg)](https://www.statsmodels.org/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-gray?logo=yaml&logoColor=white)](https://pyyaml.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/regime_changes_real_financial_cycles`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Regime Changes and Real-Financial Cycles: Searching Minsky's Hypothesis in a Nonlinear Setting\"** by:\n",
        "\n",
        "*   Domenico Delli Gatti\n",
        "*   Filippo Gusella\n",
        "*   Giorgio Ricchiuti\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous data validation and spectral decomposition to the core nonlinear Markov-Switching Vector Autoregression (MS-VAR) estimation, Minsky regime classification, and Monte Carlo robustness analysis.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `execute_minsky_research_project`](#key-callable-execute_minsky_research_project)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the analytical framework presented in Delli Gatti, Gusella, and Ricchiuti (2025). The core of this repository is the iPython Notebook `regime_changes_real_financial_cycles_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed to be a generalizable toolkit for detecting endogenous financial fragility and nonlinear regime shifts in macroeconomic data.\n",
        "\n",
        "The paper investigates Minsky's Financial Instability Hypothesis (FIH) by extending linear models to a nonlinear setting. It captures local real-financial endogenous cycles where stability breeds instability. This codebase operationalizes the paper's framework, allowing users to:\n",
        "-   Rigorously validate and manage the entire experimental configuration via a single `config.yaml` file.\n",
        "-   Process raw macroeconomic time series, applying Hodrick-Prescott (HP) filtering to extract cyclical components.\n",
        "-   Estimate a bivariate **Markov-Switching Vector Autoregression (MS-VAR)** model to distinguish between \"Minsky\" (interaction) and \"No Minsky\" (independence) regimes.\n",
        "-   Empirically verify the mathematical conditions for endogenous oscillations (complex eigenvalues) and Minskyan feedback loops.\n",
        "-   Trace the evolution of financial fragility over time using filtered and smoothed regime probabilities.\n",
        "-   Validate the robustness of findings via parametric bootstrap Monte Carlo simulations.\n",
        "-   Automatically generate all key tables and figures from the paper.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in nonlinear dynamic macroeconomic modeling and time series econometrics.\n",
        "\n",
        "**1. The Minsky Cycle Mechanism:**\n",
        "The core hypothesis is that financial fragility builds endogenously during expansions. This is captured by a dynamic interaction between a real variable ($y_t$, e.g., GDP) and a financial variable ($f_t$, e.g., debt).\n",
        "$$\n",
        "\\begin{bmatrix} y_t \\\\ f_t \\end{bmatrix} = \\mathbf{A}(s_t) \\begin{bmatrix} y_{t-1} \\\\ f_{t-1} \\end{bmatrix} + \\boldsymbol{\\epsilon}_t\n",
        "$$\n",
        "A \"Minsky Regime\" ($s_t=1$) is characterized by a specific sign pattern in the interaction matrix $\\mathbf{A}_1$:\n",
        "-   $\\beta_1 > 0$: Economic expansion leads to rising debt/interest rates (pro-cyclical leverage).\n",
        "-   $\\alpha_2 < 0$: Rising financial fragility drags down real activity.\n",
        "-   **Oscillation Condition:** The discriminant $\\Delta = (\\alpha_1 - \\beta_2)^2 + 4\\alpha_2\\beta_1 < 0$, implying complex eigenvalues and endogenous cycles.\n",
        "\n",
        "**2. Regime Switching:**\n",
        "The economy transitions between the interaction regime and an independence regime ($s_t=2$) where real and financial variables decouple ($\\mathbf{A}_2$ is diagonal). This switching is governed by a first-order Markov chain with transition probabilities $p_{ij}$.\n",
        "\n",
        "**3. Estimation Strategy:**\n",
        "The model is estimated using the **Expectation-Maximization (EM) Algorithm**, which iteratively maximizes the likelihood function. The **Hamilton Filter** is used for the Expectation step to infer the latent state probabilities, and the **Kim Smoother** provides the optimal inference using the full sample.\n",
        "\n",
        "Below is a diagram which sums up the Inputs-Processes-Outputs of the proposed approach:\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/chirindaopensource/regime_changes_real_financial_cycles/blob/main/regime_changes_real_financial_cycles_inputs_processes_outputs.png\" alt=\"Minsky MS-VAR Process Flow\" width=\"100%\">\n",
        "</div>\n",
        "\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`regime_changes_real_financial_cycles_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 24 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All study parameters are managed in an external `config.yaml` file.\n",
        "-   **Rigorous Data Validation:** A multi-stage validation process checks the schema, temporal consistency, and domain constraints of all input data.\n",
        "-   **Advanced Signal Extraction:** Implements a sparse-matrix version of the Hodrick-Prescott filter for efficient cycle extraction.\n",
        "-   **Robust Econometric Engine:** A custom EM algorithm implementation with numerical safeguards (regularization, log-sum-exp) for stable MS-VAR estimation.\n",
        "-   **Comprehensive Diagnostics:** Includes Ljung-Box residual tests and rigorous eigenvalue analysis for Minsky condition verification.\n",
        "-   **Monte Carlo Robustness:** A fully integrated parametric bootstrap module to assess the stability of parameter estimates and regime classifications.\n",
        "-   **Automated Reporting:** Generates publication-ready LaTeX tables and technical documentation.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Preprocessing (Tasks 1-4):** Ingests raw data, validates schemas, cleanses missing values via interpolation/truncation, and applies logarithmic transformations.\n",
        "2.  **Signal Extraction (Task 5):** Applies the HP filter ($\\lambda=100$) to extract stationary cyclical components from annual data.\n",
        "3.  **Model Setup (Tasks 6-10):** Constructs bivariate systems, verifies stationarity via ADF tests, and initializes EM parameters using baseline VAR estimates.\n",
        "4.  **Estimation (Tasks 11-15):** Executes the EM algorithm, utilizing the Hamilton Filter and Kim Smoother to estimate regime-dependent parameters and state probabilities.\n",
        "5.  **Diagnostics & Analysis (Tasks 16-18):** Validates residuals, checks mathematical conditions for Minsky cycles (eigenvalues, signs), and analyzes regime dominance over time.\n",
        "6.  **Robustness & Synthesis (Tasks 19-24):** Runs Monte Carlo simulations, cross-validates against paper benchmarks, and compiles the final report.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `regime_changes_real_financial_cycles_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 24 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `execute_minsky_research_project`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`execute_minsky_research_project`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `pyyaml`, `scipy`, `statsmodels`, `matplotlib`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/regime_changes_real_financial_cycles.git\n",
        "    cd regime_changes_real_financial_cycles\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy pyyaml scipy statsmodels matplotlib\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a dictionary of DataFrames (`raw_datasets`) where keys are country names and values are DataFrames with a `DatetimeIndex` (annual frequency) and the following columns:\n",
        "1.  **`real_gdp`**: Real Gross Domestic Product.\n",
        "2.  **`nfcd`**: Non-financial Corporate Debt.\n",
        "3.  **`household_debt`**: Household Debt.\n",
        "4.  **`stir`**: Short-Term Interest Rate.\n",
        "\n",
        "All other parameters are controlled by the `config.yaml` file.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `regime_changes_real_financial_cycles_draft.ipynb` notebook provides a complete, step-by-step guide. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `execute_minsky_research_project` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Load the master configuration from the YAML file.\n",
        "    with open('config.yaml', 'r') as f:\n",
        "        study_config = yaml.safe_load(f)\n",
        "    \n",
        "    # 2. Load or generate raw datasets (Example using synthetic generator)\n",
        "    # In production, load from CSV/Parquet: pd.read_csv(...)\n",
        "    raw_datasets = load_country_data()\n",
        "    \n",
        "    # 3. Execute the entire replication study.\n",
        "    final_results = execute_minsky_research_project(\n",
        "        study_config=study_config,\n",
        "        raw_datasets=raw_datasets\n",
        "    )\n",
        "    \n",
        "    # 4. Access results\n",
        "    print(final_results[\"master_results\"][\"analysis\"][\"minsky_conditions\"])\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline returns a comprehensive dictionary containing all analytical artifacts, structured as follows:\n",
        "-   **`master_results`**: The aggregated dictionary of all outputs.\n",
        "    -   **`estimation`**: Contains estimated parameters ($A, \\Sigma, P$) for all models.\n",
        "    -   **`diagnostics`**: Ljung-Box test results.\n",
        "    -   **`analysis`**: Minsky condition verification and regime probabilities.\n",
        "    -   **`robustness`**: Monte Carlo simulation statistics.\n",
        "    -   **`validation`**: Cross-validation report against paper benchmarks.\n",
        "-   **`latex_tables`**: Ready-to-compile LaTeX code for parameter estimates and classification tables.\n",
        "-   **`technical_appendix`**: A markdown summary of implementation choices.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "regime_changes_real_financial_cycles/\n",
        "│\n",
        "├── regime_changes_real_financial_cycles_draft.ipynb  # Main implementation notebook\n",
        "├── config.yaml                                       # Master configuration file\n",
        "├── requirements.txt                                  # Python package dependencies\n",
        "│\n",
        "├── data/                                             # Directory for raw data (optional)\n",
        "│   ├── usa_macro_data.csv\n",
        "│   └── ...\n",
        "│\n",
        "├── LICENSE                                           # MIT Project License File\n",
        "└── README.md                                         # This file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify study parameters such as the HP filter lambda, convergence tolerances, Monte Carlo iterations, and statistical thresholds without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Filtering:** Implementing the Hamilton regression filter or Baxter-King filter as alternatives to HP.\n",
        "-   **Model Selection:** Adding information criteria (AIC/BIC) to select the optimal number of regimes or lags.\n",
        "-   **Time-Varying Transition Probabilities:** Extending the MS-VAR to allow transition probabilities to depend on exogenous variables (TVTP-MS-VAR).\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{delligatti2025regime,\n",
        "  title={Regime Changes and Real-Financial Cycles: Searching Minsky's Hypothesis in a Nonlinear Setting},\n",
        "  author={Delli Gatti, Domenico and Gusella, Filippo and Ricchiuti, Giorgio},\n",
        "  journal={arXiv preprint arXiv:2511.04348},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). Regime Changes and Real-Financial Cycles: An Open Source Implementation.\n",
        "GitHub repository: https://github.com/chirindaopensource/regime_changes_real_financial_cycles\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Domenico Delli Gatti, Filippo Gusella, and Giorgio Ricchiuti** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, Statsmodels, and Matplotlib**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `regime_changes_real_financial_cycles_draft.ipynb` notebook and follows best practices for research software documentation.*\n",
        "```"
      ],
      "metadata": {
        "id": "k6wkZQ0_yvLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Regime Changes and Real-Financial Cycles: Searching Minsky's Hypothesis in a Nonlinear Setting*\"\n",
        "\n",
        "Authors: Domenico delli Gatti, Filippo Gusella, Giorgio Ricchiuti\n",
        "\n",
        "E-Journal Submission Date: 6 November 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2511.04348\n",
        "\n",
        "Abstract:\n",
        "\n",
        "This paper investigates Minsky's cycles by extending the paper of stockhammer et al. (2019) with a nonlinear model to capture possible local real-financial endogenous cycles. We trace nonlinear regime changes and check the presence of Minsky cycles from the 1970s to 2020 for the USA, France, Germany, Canada, Australia, and the UK, linking the GDP with corporate debt, interest rate, and household debt. When considering corporate debt, the results reveal real-financial endogenous cycles in all countries, except Australia, and across all countries when interest rates are included. We find evidence for an interaction mechanism between household debt and GDP only for the USA and the UK. These findings underscore the importance of nonlinear regime transitions in empirically assessing Minsky's theory.\n"
      ],
      "metadata": {
        "id": "ddSTc-nI0hp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### The Theoretical Premise & Problem Statement\n",
        "The paper addresses a fundamental gap in the empirical validation of Hyman Minsky’s **Financial Instability Hypothesis (FIH)**.\n",
        "*   **The Core Hypothesis:** Minsky argued that \"stability is destabilizing.\" During economic expansions, agents increase leverage, transitioning from \"hedge\" to \"speculative\" and finally \"Ponzi\" financing positions. This endogenously creates financial fragility, leading to a crisis.\n",
        "*   **The Limitation of Previous Work:** Existing empirical literature (e.g., Stockhammer et al., 2019) largely utilizes linear Vector Autoregression (VAR) models. A linear approach assumes constant parameters over time, failing to capture the *episodic* nature of financial fragility. Minskyan cycles are likely not permanent states but specific \"regimes\" that the economy enters and exits.\n",
        "*   **The Objective:** To detect **local** endogenous real-financial cycles by employing a **Markov-Switching Vector Autoregressive (MS-VAR)** model. This allows the system to switch between a \"Minsky Regime\" (endogenous instability) and a \"No Minsky Regime\" (independence between real and financial variables).\n",
        "\n",
        "### The Mathematical Framework\n",
        "\n",
        "\n",
        "The authors construct a bivariate dynamical system linking a real variable ($y_t$, GDP) and a financial variable ($f_t$, Debt or Interest Rate).\n",
        "\n",
        "**2.1 The Linear Condition for Cycles**\n",
        "The foundational dynamics are governed by the transition matrix $A$:\n",
        "$$\n",
        "\\begin{bmatrix} y_t \\\\ f_t \\end{bmatrix} = A \\begin{bmatrix} y_{t-1} \\\\ f_{t-1} \\end{bmatrix}, \\quad \\text{where } A = \\begin{bmatrix} \\alpha_1 & \\alpha_2 \\\\ \\beta_1 & \\beta_2 \\end{bmatrix}\n",
        "$$\n",
        "For endogenous cycles (complex eigenvalues) to exist, the discriminant of the characteristic equation must be negative ($\\Delta < 0$). Furthermore, the **Minsky condition** requires specific signs in the interaction terms:\n",
        "1.  **$\\beta_1 > 0$:** An increase in GDP leads to higher debt/rates (optimism breeds leverage).\n",
        "2.  **$\\alpha_2 < 0$:** An increase in debt/rates drags down GDP (debt service burden).\n",
        "\n",
        "**2.2 The Nonlinear Extension (MS-VAR)**\n",
        "The authors introduce a latent state variable $s_t$ governed by a first-order Markov chain. The system switches between two matrices:\n",
        "*   **Regime 1 (Minsky):** Full interaction matrix allowed.\n",
        "    $$ y_t = A_1(s_t)y_{t-1} + \\epsilon_t $$\n",
        "*   **Regime 2 (No Minsky):** A diagonal matrix where interaction terms are zero ($\\alpha_2 = \\beta_1 = 0$). This implies the real and financial sectors evolve independently (random walks or autoregressive processes without feedback).\n",
        "\n",
        "### Data and Pre-processing\n",
        "*   **Scope:** 1970s–2020.\n",
        "*   **Countries:** USA, UK, France, Germany, Canada, and Australia.\n",
        "*   **Variables:**\n",
        "    *   **Real:** Real GDP ($y$).\n",
        "    *   **Financial:** Non-financial Corporate Debt (NFCD), Household Debt (HD), and Short-Term Interest Rates (STIR).\n",
        "*   **Frequency:** Yearly. (Note: The authors justify low frequency to avoid the noise of high-frequency data and to preserve degrees of freedom in a nonlinear estimation, though this is a constraint on the granularity of the cycle detection).\n",
        "*   **Filtering:** The Hodrick-Prescott (HP) filter is applied to extract the cyclical components, rendering the series stationary before feeding them into the MS-VAR.\n",
        "\n",
        "### Empirical Estimation Results\n",
        "The authors estimate the model separately for the three financial variables.\n",
        "\n",
        "**Corporate Debt (GDP-NFCD Interaction)**\n",
        "*   **Finding:** Evidence of endogenous Minsky cycles is strong in **USA, UK, France, Germany, and Canada**.\n",
        "*   **Coefficients:** In Regime 1, the coefficients $\\alpha_2$ are negative and $\\beta_1$ are positive (statistically significant), satisfying the mathematical condition for limit cycles.\n",
        "*   **The Outlier:** **Australia** does not satisfy the condition; the interaction parameters are insignificant or have the wrong signs.\n",
        "*   **Dynamics:** The \"Minsky Regime\" dominates in the 1970s and periods leading up to the 2001 and 2008 crises.\n",
        "\n",
        "**Household Debt (GDP-HD Interaction)**\n",
        "*   **Finding:** Results are much more heterogeneous.\n",
        "*   **USA & UK:** Strong evidence of Minsky cycles. The interaction between household leverage and GDP is a key driver of instability in these financialized economies.\n",
        "*   **Continental Europe & Australia:** Germany, France, Canada, and Australia show **no evidence** of Minsky cycles driven by household debt. The mathematical conditions for complex eigenvalues are not met.\n",
        "*   **Implication:** The \"house price-wealth effect\" mechanism is institutionally specific (Anglo-Saxon model).\n",
        "\n",
        "**Interest Rates (GDP-STIR Interaction)**\n",
        "*   **Finding:** This interaction is the most robust. **All six countries** (including Australia) exhibit Regime 1 dynamics consistent with Minskyan cycles.\n",
        "*   **Mechanism:** Growth triggers rate hikes (monetary tightening), which subsequently dampen growth. The authors interpret this as validating the central bank's role in generating (or reacting to) endogenous cycles.\n",
        "\n",
        "### Regime Dynamics & Robustness\n",
        "*   **Transition Matrices:**\n",
        "    *   The USA and UK show high persistence in Regime 1 (Minsky), meaning once the economy enters a fragile state, it tends to stay there.\n",
        "    *   France and Canada exhibit more volatility, switching frequently between regimes.\n",
        "*   **Filtered Probabilities:** The plots of $P(s_t=1)$ show that the \"Minsky Regime\" is time-dependent. It is not a permanent state of capitalism but a phase. For example, in the US, the probability of being in the Minsky regime spiked before the 2008 crisis.\n",
        "*   **Robustness:** The authors performed 100 Monte Carlo simulations to validate the small-sample properties of their estimators, confirming that the detected cycles are not artifacts of noise.\n",
        "\n",
        "### Conclusion and Academic Contribution\n",
        "The paper concludes that Minsky’s hypothesis is empirically valid but **conditional**:\n",
        "1.  **Nonlinearity is key:** Linear models mask the reality that financial-real interactions are episodic.\n",
        "2.  **Sector Specificity:** Corporate debt and Interest rates are universal drivers of cycles. Household debt is a driver only in specific institutional contexts (USA/UK).\n",
        "3.  **Policy Implication:** Financial instability is endogenous to the system (Regime 1). Policy requires monitoring the *transition* into this regime, rather than assuming the economy is always stable or always unstable."
      ],
      "metadata": {
        "id": "KFo6sw33Mtqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "HGajsBknEc3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ====================================================================================================#\n",
        "#\n",
        "#  Regime Changes and Real-Financial Cycles: Searching Minsky's Hypothesis in a Nonlinear Setting\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Regime Changes and Real-Financial Cycles:\n",
        "#  Searching Minsky's Hypothesis in a Nonlinear Setting\" by Domenico Delli Gatti,\n",
        "#  Filippo Gusella, and Giorgio Ricchiuti (2025). It delivers a computationally\n",
        "#  tractable system for the dynamic quantification of endogenous systemic fragility,\n",
        "#  enabling robust, regime-dependent analysis of real-financial interactions\n",
        "#  through a nonlinear Markov-Switching Vector Autoregressive (MS-VAR) framework.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Bivariate Markov-Switching Vector Autoregression (MS-VAR) estimation via EM Algorithm\n",
        "#  • Endogenous regime classification: \"Minsky Regime\" (Interaction) vs. \"No Minsky Regime\" (Independence)\n",
        "#  • Spectral decomposition via Hodrick-Prescott (HP) filter for cyclical component extraction\n",
        "#  • Eigenvalue analysis for detecting local endogenous oscillations (complex roots)\n",
        "#  • Parametric bootstrap Monte Carlo simulation for robustness assessment\n",
        "#  • Ljung-Box residual diagnostics for model validation\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Hamilton Forward Filter for recursive state inference\n",
        "#  • Kim Backward Smoother for optimal regime probability estimation\n",
        "#  • Numerical Hessian approximation for standard error computation\n",
        "#  • Sparse matrix algebra for efficient HP filtering\n",
        "#  • Robust numerical optimization with regularization for covariance matrices\n",
        "#  • Comprehensive visualization of regime probabilities and historical crisis markers\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Delli Gatti, D., Gusella, F., & Ricchiuti, G. (2025). Regime Changes and Real-Financial\n",
        "#  Cycles: Searching Minsky's Hypothesis in a Nonlinear Setting. arXiv preprint arXiv:2511.04348.\n",
        "#  https://arxiv.org/abs/2511.04348\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ====================================================================================================#\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from typing import Any, Callable, Dict, List, Optional, Set, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from scipy.sparse.linalg import spsolve\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Configure logging for the cleansing process\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "ZkFLFC6qEjOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "mDDyy4iKEmEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Discussion of Key Callables**\n",
        "\n",
        "### **Task 1: Validate Study Configuration Object Structure**\n",
        "\n",
        "**Callable:** `validate_study_config`\n",
        "\n",
        "*   **Inputs:** `study_config` (Dict[str, Any]): The master configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  Calls `validate_top_level_architecture` to ensure the dictionary contains exactly `input_data_architecture` and `model_physics`.\n",
        "    2.  Calls `validate_input_data_architecture` to verify temporal settings (1970-2020, Annual) and dataset schemas (6 countries, 4 variables).\n",
        "    3.  Calls `validate_model_physics` to enforce mathematical parameters like $\\lambda=100$ for the HP filter and the structural masks for the MS-VAR regimes.\n",
        "*   **Outputs:** `bool`: Returns `True` if valid; raises `ConfigurationError` otherwise.\n",
        "*   **Transformation:** No data transformation; purely a validation gate.\n",
        "*   **Research Role:** Implements the **Model Specification** phase. It ensures the computational environment matches the paper's setup, specifically enforcing the regime structure where Regime 1 allows interaction ($A_1$ full) and Regime 2 enforces independence ($A_2$ diagonal), as defined in Section 2.2:\n",
        "    $$ \\mathbf{y}_t = \\begin{cases} \\mathbf{A}_1 \\mathbf{y}_{t-1} + \\epsilon_t & \\text{if } s_t = 1 \\\\ \\mathbf{A}_2 \\mathbf{y}_{t-1} + \\epsilon_t & \\text{if } s_t = 2 \\end{cases} $$\n",
        "\n",
        "### **Task 2: Validate Raw Datasets Dictionary Structure**\n",
        "\n",
        "**Callable:** `validate_raw_datasets`\n",
        "\n",
        "*   **Inputs:** `datasets` (Dict[str, pd.DataFrame]): Raw country data.\n",
        "*   **Processes:**\n",
        "    1.  Validates container structure and country keys via `validate_datasets_container`.\n",
        "    2.  Validates temporal structure (monotonicity, equidistance) via `validate_datetime_index`.\n",
        "    3.  Validates column presence and domain constraints via `validate_columns_and_domains`.\n",
        "*   **Outputs:** `bool`: Returns `True` if valid; raises `DataValidationError` otherwise.\n",
        "*   **Transformation:** No transformation; validation only.\n",
        "*   **Research Role:** Implements **Data Integrity Verification**. It ensures the input data adheres to the requirements for time series analysis, specifically the equidistant annual frequency required for the HP filter penalty term $\\lambda \\sum [(\\tau_{t+1} - \\tau_t) - (\\tau_t - \\tau_{t-1})]^2$.\n",
        "\n",
        "### **Task 3: Data Cleansing and Temporal Alignment**\n",
        "\n",
        "**Callable:** `cleanse_and_align_datasets`\n",
        "\n",
        "*   **Inputs:** `datasets` (Dict[str, pd.DataFrame]): Raw validated datasets.\n",
        "*   **Processes:**\n",
        "    1.  `handle_missing_and_invalid_values`: Interpolates isolated single-year gaps using linear interpolation: $x_t = x_{t-1} + (x_{t+1} - x_{t-1})/2$.\n",
        "    2.  `enforce_temporal_alignment`: Truncates data to the maximal contiguous block where all variables are valid.\n",
        "    3.  `validate_and_freeze_data`: Computes summary statistics.\n",
        "*   **Outputs:** `Dict[str, Any]`: Contains cleansed DataFrames and summary stats.\n",
        "*   **Transformation:** Raw time series with gaps $\\rightarrow$ Continuous, aligned time series.\n",
        "*   **Research Role:** Implements **Data Preprocessing**. It prepares the raw data for spectral decomposition, ensuring that the vector $\\mathbf{y}_t$ is defined for a contiguous set of time points $t=1, \\dots, T$, a prerequisite for VAR estimation.\n",
        "\n",
        "### **Task 4: Apply Logarithmic Transformation to Real GDP**\n",
        "\n",
        "**Callable:** `apply_log_transform`\n",
        "\n",
        "*   **Inputs:** `datasets` (Dict[str, pd.DataFrame]): Cleansed datasets.\n",
        "*   **Processes:**\n",
        "    1.  `execute_log_transformation`: Applies natural logarithm to Real GDP: $y_t = \\ln(\\text{GDP}_t)$.\n",
        "    2.  `document_transformation_metadata`: Records transformation details.\n",
        "*   **Outputs:** `Tuple[Dict, Dict]`: Transformed datasets and metadata.\n",
        "*   **Transformation:** Level GDP $\\rightarrow$ Log-Level GDP.\n",
        "*   **Research Role:** Implements **Variable Transformation**. As stated in Section 3, \"The real variable $y$ is proxied by... real gross domestic product (GDP)... transformed into logarithmic levels.\" This prepares the real variable for cycle extraction.\n",
        "\n",
        "### **Task 5: Implement Hodrick-Prescott Filter for Cycle Extraction**\n",
        "\n",
        "**Callable:** `extract_hp_cycles`\n",
        "\n",
        "*   **Inputs:** `datasets` (Dict[str, pd.DataFrame]), `study_config` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  Retrieves $\\lambda=100$.\n",
        "    2.  `apply_hp_filter_to_datasets`: Solves the linear system $(I + \\lambda D^T D) \\tau = x$ using sparse matrix algebra (`hp_filter_sparse`) to separate trend $\\tau$ and cycle $c = x - \\tau$.\n",
        "*   **Outputs:** `Dict[str, pd.DataFrame]`: Datasets enriched with cyclical components (e.g., `gdp_cycle`).\n",
        "*   **Transformation:** Log-levels/Ratios $\\rightarrow$ Stationary Cyclical Components.\n",
        "*   **Research Role:** Implements **Spectral Decomposition**. It isolates the business cycle component $c_t$ from the secular trend, as described in Section 3: \"we focus on cyclical phenomena by first extracting cycles from the time series using the Hodrick-Prescott filter.\"\n",
        "\n",
        "### **Task 6: Construct Bivariate Cyclical State Vectors**\n",
        "\n",
        "**Callable:** `construct_bivariate_systems`\n",
        "\n",
        "*   **Inputs:** `datasets` (Dict[str, pd.DataFrame]): Datasets with cycles.\n",
        "*   **Processes:**\n",
        "    1.  `build_bivariate_systems`: Stacks the real cycle and financial cycle into $(T, 2)$ arrays for three specific systems: GDP/NFCD, GDP/HD, GDP/STIR.\n",
        "    2.  `validate_cyclical_properties`: Checks for zero mean and finite values.\n",
        "*   **Outputs:** `Dict[str, Dict[str, Any]]`: Structured dictionary of bivariate systems.\n",
        "*   **Transformation:** Individual Series $\\rightarrow$ Bivariate State Vectors $\\mathbf{y}_t = [y_t, f_t]'$.\n",
        "*   **Research Role:** Implements **Model Setup**. It constructs the state vector $\\mathbf{y}_t$ defined in Eq. (1):\n",
        "    $$ \\begin{bmatrix} y_t \\\\ f_t \\end{bmatrix} $$\n",
        "    This vector is the fundamental input for the MS-VAR estimation.\n",
        "\n",
        "### **Task 7: Augmented Dickey-Fuller (ADF) Test Specification**\n",
        "\n",
        "**Callable:** `run_adf_test`\n",
        "\n",
        "*   **Inputs:** `series` (pd.Series), `study_config` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `execute_adf_test`: Estimates the ADF regression $\\Delta z_t = \\mu + \\gamma z_{t-1} + \\sum \\phi_j \\Delta z_{t-j} + u_t$.\n",
        "    2.  `evaluate_stationarity`: Compares the t-statistic of $\\gamma$ against the critical value $-1.94$.\n",
        "*   **Outputs:** `Dict[str, Any]`: Test results and stationarity decision.\n",
        "*   **Transformation:** Time Series $\\rightarrow$ Statistical Test Metric.\n",
        "*   **Research Role:** Implements **Stationarity Testing**. It verifies the assumption that the cyclical components are $I(0)$, which is required for the validity of the VAR estimation. This corresponds to the results reported in Appendix A.\n",
        "\n",
        "### **Task 8: Execute ADF Tests for All Cyclical Series**\n",
        "\n",
        "**Callable:** `execute_all_adf_tests`\n",
        "\n",
        "*   **Inputs:** `bivariate_systems` (Dict), `study_config` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `test_all_cycles`: Iterates through all unique series in the systems and runs `run_adf_test`.\n",
        "    2.  `validate_stationarity_results`: Aggregates results and checks for global stationarity.\n",
        "*   **Outputs:** `Dict[str, Any]`: Summary of stationarity tests.\n",
        "*   **Transformation:** Collection of Series $\\rightarrow$ Validation Report.\n",
        "*   **Research Role:** Implements **Diagnostic Validation**. It confirms the empirical validity of the inputs used in the paper, ensuring that \"The results in Appendix A confirm the stationarity of the series for all the cases considered.\"\n",
        "\n",
        "### **Task 9: Define the Two-Regime MS-VAR(1) Model**\n",
        "\n",
        "**Callable:** `instantiate_msvar_model`\n",
        "\n",
        "*   **Inputs:** `country` (str), `system_name` (str), `data` (np.ndarray), `study_config` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  Initializes the `MSVARModel` class.\n",
        "    2.  Loads structural masks for $A_1$ (ones) and $A_2$ (diagonal) from config.\n",
        "*   **Outputs:** `MSVARModel`: Initialized model object.\n",
        "*   **Transformation:** Raw Data + Config $\\rightarrow$ Model Object.\n",
        "*   **Research Role:** Implements **Model Definition**. It encapsulates the mathematical structure of the two-regime model defined in Section 2.2, specifically the distinction between the interaction regime ($s_t=1$) and the independence regime ($s_t=2$).\n",
        "\n",
        "### **Task 10: Initialize EM Algorithm Parameters**\n",
        "\n",
        "**Callable:** `initialize_em_parameters`\n",
        "\n",
        "*   **Inputs:** `bivariate_systems` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `estimate_baseline_var`: Estimates a single-regime VAR(1) via OLS: $\\hat{A} = (\\sum y_t y_{t-1}')(\\sum y_{t-1} y_{t-1}')^{-1}$.\n",
        "    2.  `construct_initial_parameters`: Maps $\\hat{A}$ to $A_1^{(0)}$ and $\\text{diag}(\\hat{A})$ to $A_2^{(0)}$. Perturbs $\\Sigma_2$ to break symmetry.\n",
        "*   **Outputs:** `Dict`: Initial parameters for all models.\n",
        "*   **Transformation:** Data $\\rightarrow$ Initial Parameter Guesses $\\Theta^{(0)}$.\n",
        "*   **Research Role:** Implements **Algorithm Initialization**. It provides the starting point for the Expectation-Maximization algorithm, ensuring the optimization begins in a reasonable region of the parameter space.\n",
        "\n",
        "### **Task 11: Implement Hamilton Forward Filter**\n",
        "\n",
        "**Callable:** `run_hamilton_filter`\n",
        "\n",
        "*   **Inputs:** `data` (np.ndarray), `params` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `compute_conditional_log_densities`: Computes $\\log f(y_t | s_t=j)$.\n",
        "    2.  Executes the forward recursion:\n",
        "        *   Prediction: $\\xi_{t|t-1} = \\xi_{t-1|t-1} P$.\n",
        "        *   Update: $\\xi_{t|t} \\propto \\xi_{t|t-1} \\odot \\eta_t$.\n",
        "    3.  Computes log-likelihood via log-sum-exp.\n",
        "*   **Outputs:** `Dict`: Filtered probabilities and log-likelihood.\n",
        "*   **Transformation:** Data + Parameters $\\rightarrow$ Filtered State Probabilities.\n",
        "*   **Research Role:** Implements the **E-Step (Filtering)**. It infers the probability of being in the \"Minsky Regime\" at time $t$ given information up to time $t$, which is central to tracing regime changes over time.\n",
        "\n",
        "### **Task 12: Implement Kim Backward Smoother**\n",
        "\n",
        "**Callable:** `run_kim_smoother`\n",
        "\n",
        "*   **Inputs:** `filter_results` (Dict), `params` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `compute_smoothed_probabilities`: Executes backward recursion $\\xi_{t|T} = \\xi_{t|t} \\odot (P (\\xi_{t+1|T} \\oslash \\xi_{t+1|t}))$.\n",
        "    2.  `compute_joint_smoothed_probabilities`: Computes $P(s_t=i, s_{t+1}=j | \\mathcal{F}_T)$.\n",
        "*   **Outputs:** `Dict`: Smoothed probabilities.\n",
        "*   **Transformation:** Filtered Probabilities $\\rightarrow$ Smoothed Probabilities.\n",
        "*   **Research Role:** Implements the **E-Step (Smoothing)**. It provides the optimal inference of the regime at time $t$ using the *entire* sample, which is used for the final regime classification and parameter updates.\n",
        "\n",
        "### **Task 13: Implement EM M-Step Parameter Updates**\n",
        "\n",
        "**Callable:** `perform_m_step`\n",
        "\n",
        "*   **Inputs:** `data` (np.ndarray), `smoother_results` (Dict), `masks` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `update_transition_matrix`: Updates $P$ using joint smoothed probabilities.\n",
        "    2.  `update_coefficients`: Updates $A_s$ using weighted OLS: $\\hat{A}_s = (\\sum \\xi_t(s) y_t y_{t-1}') (\\sum \\xi_t(s) y_{t-1} y_{t-1}')^{-1}$. Applies structural masks.\n",
        "    3.  `update_covariances`: Updates $\\Sigma_s$ using weighted residuals.\n",
        "*   **Outputs:** `Dict`: Updated parameters $\\Theta^{(k+1)}$.\n",
        "*   **Transformation:** Smoothed Probabilities + Data $\\rightarrow$ New Parameters.\n",
        "*   **Research Role:** Implements the **M-Step (Maximization)**. It computes the parameter values that maximize the expected log-likelihood, refining the estimates of the Minsky cycle coefficients ($\\alpha, \\beta$).\n",
        "\n",
        "### **Task 14: Implement EM Convergence Logic**\n",
        "\n",
        "**Callable:** `finalize_estimation`\n",
        "\n",
        "*   **Inputs:** `current_params`, `current_loglik`, `previous_loglik`, `iteration`, `config`, `data`, `likelihood_evaluator`.\n",
        "*   **Processes:**\n",
        "    1.  `check_convergence`: Checks absolute/relative change in log-likelihood against tolerance ($10^{-6}$).\n",
        "    2.  `compute_standard_errors`: If converged, computes numerical Hessian of log-likelihood and inverts it to get standard errors.\n",
        "*   **Outputs:** `Tuple[bool, Dict]`: Convergence status and final results.\n",
        "*   **Transformation:** Iteration State $\\rightarrow$ Convergence Decision & SEs.\n",
        "*   **Research Role:** Implements **Optimization Control**. It determines when the EM algorithm has found the maximum likelihood estimates and quantifies the uncertainty (standard errors) needed for hypothesis testing.\n",
        "\n",
        "### **Task 15: Estimate MS-VAR for All Country-System Combinations**\n",
        "\n",
        "**Callable:** `estimate_all_msvar_models`\n",
        "\n",
        "*   **Inputs:** `bivariate_systems` (Dict), `initial_params_dict` (Dict), `study_config` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  Iterates over all 18 models.\n",
        "    2.  `estimate_single_model`: Runs the EM loop (Tasks 11-14) for each model.\n",
        "*   **Outputs:** `Dict`: Estimation results for all models.\n",
        "*   **Transformation:** All Data $\\rightarrow$ All Model Estimates.\n",
        "*   **Research Role:** Implements **Model Estimation**. It produces the empirical results presented in Tables 1, 2, and 3 of the paper.\n",
        "\n",
        "### **Task 16: Compute Ljung-Box Residual Diagnostics**\n",
        "\n",
        "**Callable:** `compute_ljung_box_diagnostics`\n",
        "\n",
        "*   **Inputs:** `msvar_estimates` (Dict), `bivariate_systems` (Dict), `study_config` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `extract_model_residuals`: Computes residuals $u_t = y_t - A_{\\hat{s}_t} y_{t-1}$ based on the most probable regime.\n",
        "    2.  `compute_ljung_box_stats`: Calculates Q-statistics for lag $h=3$.\n",
        "*   **Outputs:** `Dict`: Diagnostic results.\n",
        "*   **Transformation:** Estimates $\\rightarrow$ Diagnostic Metrics.\n",
        "*   **Research Role:** Implements **Model Validation**. It checks for serial correlation in the residuals to ensure the MS-VAR(1) specification adequately captures the data dynamics, corresponding to Appendix B.\n",
        "\n",
        "### **Task 17: Compute Eigenvalues and Minsky Oscillation Conditions**\n",
        "\n",
        "**Callable:** `analyze_minsky_conditions`\n",
        "\n",
        "*   **Inputs:** `msvar_estimates` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `evaluate_eigenvalues_and_discriminant`: Computes $\\Delta' = (\\alpha_1 - \\beta_2)^2 + 4 \\alpha_2 \\beta_1$.\n",
        "    2.  `classify_minsky_regime`: Checks if $\\Delta' < 0$, $\\beta_1 > 0$, $\\alpha_2 < 0$, and coefficients are significant.\n",
        "*   **Outputs:** `Dict`: Minsky classification results.\n",
        "*   **Transformation:** Parameters $\\rightarrow$ Economic Interpretation.\n",
        "*   **Research Role:** Implements **Hypothesis Testing**. It empirically tests the mathematical condition for endogenous Minsky cycles derived in Section 2.1:\n",
        "    $$ (\\alpha_1 - \\beta_2)^2 + 4 \\alpha_2 \\beta_1 < 0 $$\n",
        "    and the necessary condition $\\alpha_2 \\beta_1 < 0$.\n",
        "\n",
        "### **Task 18: Extract and Visualize Regime Probabilities**\n",
        "\n",
        "**Callable:** `extract_regime_probabilities`\n",
        "\n",
        "*   **Inputs:** `msvar_estimates` (Dict), `bivariate_systems` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `compute_regime_metrics`: Calculates fraction of time in Regime 1 and expected durations.\n",
        "    2.  `plot_regime_probabilities`: Generates plots of $\\xi_t(1)$ over time.\n",
        "*   **Outputs:** `Dict`: Regime probabilities and metrics.\n",
        "*   **Transformation:** Smoothed Probabilities $\\rightarrow$ Time Series Analysis.\n",
        "*   **Research Role:** Implements **Regime Analysis**. It traces the historical evolution of financial fragility, identifying periods where the \"Minsky Regime\" was dominant (e.g., pre-2008), as shown in Figures 1-13.\n",
        "\n",
        "### **Task 19: Design End-to-End Orchestrator Function**\n",
        "\n",
        "**Callable:** `run_minsky_msvar_pipeline`\n",
        "\n",
        "*   **Inputs:** `study_config` (Dict), `raw_datasets` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  Sequentially calls orchestrators for Validation, Data Engineering, Setup, Estimation, Diagnostics, and Analysis.\n",
        "*   **Outputs:** `Dict`: Comprehensive pipeline results.\n",
        "*   **Transformation:** Raw Inputs $\\rightarrow$ Final Analytical Results.\n",
        "*   **Research Role:** Implements the **Primary Research Workflow**. It automates the generation of the paper's main empirical findings.\n",
        "\n",
        "### **Task 20: Design Monte Carlo Robustness Orchestrator**\n",
        "\n",
        "**Callable:** `run_monte_carlo_robustness`\n",
        "\n",
        "*   **Inputs:** `msvar_estimates` (Dict), `study_config` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  Iterates $N=100$ times per model.\n",
        "    2.  Calls `simulate_msvar_path` (Task 21) to generate synthetic data.\n",
        "    3.  Calls `estimate_single_model` to re-estimate parameters.\n",
        "    4.  Calls `aggregate_mc_results` (Task 22) to compute stats.\n",
        "*   **Outputs:** `Dict`: Monte Carlo statistics.\n",
        "*   **Transformation:** Estimates $\\rightarrow$ Robustness Metrics.\n",
        "*   **Research Role:** Implements **Robustness Analysis**. It validates the stability of the parameter estimates and Minsky classifications against small-sample bias, as discussed in Section 3 and Appendix C.\n",
        "\n",
        "### **Task 21: Implement Monte Carlo Data Generation**\n",
        "\n",
        "**Callable:** `simulate_msvar_path`\n",
        "\n",
        "*   **Inputs:** `T` (int), `params` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `simulate_regime_path`: Simulates Markov chain using $P$.\n",
        "    2.  `generate_var_observables`: Simulates VAR process conditional on regimes.\n",
        "*   **Outputs:** `Tuple[np.ndarray, np.ndarray]`: Simulated data and regimes.\n",
        "*   **Transformation:** Parameters $\\rightarrow$ Synthetic Data.\n",
        "*   **Research Role:** Implements the **Data Generating Process (DGP)** for the parametric bootstrap, creating synthetic histories consistent with the estimated model.\n",
        "\n",
        "### **Task 22: Aggregate Monte Carlo Results**\n",
        "\n",
        "**Callable:** `aggregate_mc_results`\n",
        "\n",
        "*   **Inputs:** `replications` (List[Dict]).\n",
        "*   **Processes:**\n",
        "    1.  `compute_parameter_stats`: Computes mean, std, CI.\n",
        "    2.  `assess_minsky_robustness`: Computes fraction of replications satisfying Minsky conditions.\n",
        "*   **Outputs:** `Dict`: Aggregated stats.\n",
        "*   **Transformation:** List of Estimates $\\rightarrow$ Summary Statistics.\n",
        "*   **Research Role:** Implements **Statistical Inference**. It provides the confidence intervals and robustness checks reported in Appendix C.\n",
        "\n",
        "### **Task 23: Cross-Validate Results Against Paper Tables**\n",
        "\n",
        "**Callable:** `cross_validate_results`\n",
        "\n",
        "*   **Inputs:** `msvar_estimates` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  `load_paper_benchmarks`: Loads hardcoded values from the paper.\n",
        "    2.  `compute_relative_errors`: Calculates deviation between replication and paper.\n",
        "    3.  `generate_validation_report`: Summarizes discrepancies.\n",
        "*   **Outputs:** `Dict`: Validation report.\n",
        "*   **Transformation:** Estimates + Benchmarks $\\rightarrow$ Validation Metrics.\n",
        "*   **Research Role:** Implements **Replication Verification**. It quantifies the fidelity of the replication against the published results.\n",
        "\n",
        "### **Task 24: Final Synthesis and Documentation**\n",
        "\n",
        "**Callable:** `synthesize_final_report`\n",
        "\n",
        "*   **Inputs:** All pipeline outputs.\n",
        "*   **Processes:**\n",
        "    1.  `compile_master_results`: Aggregates everything into one dictionary.\n",
        "    2.  `generate_latex_tables`: Formats results into LaTeX.\n",
        "    3.  `generate_technical_appendix`: Creates documentation.\n",
        "*   **Outputs:** `Dict`: Final report package.\n",
        "*   **Transformation:** Raw Results $\\rightarrow$ Publication Artifacts.\n",
        "*   **Research Role:** Implements **Reporting**. It generates the final tables and figures presented in the paper.\n",
        "\n",
        "### **Top-Level Orchestrator**\n",
        "\n",
        "**Callable:** `execute_minsky_research_project`\n",
        "\n",
        "*   **Inputs:** `study_config` (Dict), `raw_datasets` (Dict).\n",
        "*   **Processes:**\n",
        "    1.  Calls `run_minsky_msvar_pipeline` (Task 19).\n",
        "    2.  Calls `run_monte_carlo_robustness` (Task 20).\n",
        "    3.  Calls `cross_validate_results` (Task 23).\n",
        "    4.  Calls `synthesize_final_report` (Task 24).\n",
        "*   **Outputs:** `Dict`: The complete project output.\n",
        "*   **Transformation:** Raw Data $\\rightarrow$ Complete Research Output.\n",
        "*   **Research Role:** Implements the **Full Project Lifecycle**. It ties together the estimation, robustness checking, validation, and reporting into a single executable workflow.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Usage Example: End-to-End Minsky Cycle Analysis**\n",
        "\n",
        "This example uses synthentic data to demonstrate how to run the `execute_minsky_research_project` orchestrator (correctly) as follows:\n",
        "1.  **Data Generation**: Synthesizing realistic macroeconomic time series for the six target countries.\n",
        "2.  **Configuration Setup**: Reading a `config.yaml` file to simulate a production environment.\n",
        "3.  **Execution**: Running the `execute_minsky_research_project` orchestrator.\n",
        "4.  **Result Inspection**: Accessing the generated artifacts.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Ensure logging is configured to show pipeline progress\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Synthetic Data Generation\n",
        "# ==============================================================================\n",
        "# We generate synthetic data that mimics the statistical properties of the\n",
        "# actual OECD/BIS datasets to ensure the pipeline runs successfully.\n",
        "\n",
        "def generate_synthetic_country_data(seed: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a synthetic macroeconomic dataset for a single country.\n",
        "    \n",
        "    Properties:\n",
        "    - Time: 1970-2020 (Annual)\n",
        "    - GDP: Exponential trend + Cyclical component\n",
        "    - Debt: Linear trend + Cycle\n",
        "    - Rates: Mean reverting + Cycle\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    dates = pd.date_range(start='1970-01-01', end='2020-01-01', freq='AS')\n",
        "    T = len(dates)\n",
        "    t = np.arange(T)\n",
        "    \n",
        "    # 1. Real GDP (Billions)\n",
        "    # Trend: Exponential growth (2% per year)\n",
        "    # Cycle: Sine wave (business cycle) + Noise\n",
        "    trend_gdp = 1000 * np.exp(0.02 * t)\n",
        "    cycle_gdp = 50 * np.sin(2 * np.pi * t / 10) + np.random.normal(0, 20, T)\n",
        "    real_gdp = trend_gdp + cycle_gdp\n",
        "    # Ensure strictly positive\n",
        "    real_gdp = np.maximum(real_gdp, 100.0)\n",
        "\n",
        "    # 2. Non-Financial Corporate Debt (NFCD)\n",
        "    # Modeled as % of GDP approx, then scaled\n",
        "    # Trend: Rising leverage\n",
        "    trend_nfcd = 500 + 10 * t\n",
        "    cycle_nfcd = 30 * np.sin(2 * np.pi * t / 10 + 0.5) + np.random.normal(0, 10, T)\n",
        "    nfcd = trend_nfcd + cycle_nfcd\n",
        "    nfcd = np.maximum(nfcd, 10.0)\n",
        "\n",
        "    # 3. Household Debt (HD)\n",
        "    # Trend: Slower rising leverage\n",
        "    trend_hd = 400 + 8 * t\n",
        "    cycle_hd = 25 * np.sin(2 * np.pi * t / 12) + np.random.normal(0, 10, T)\n",
        "    household_debt = trend_hd + cycle_hd\n",
        "    household_debt = np.maximum(household_debt, 10.0)\n",
        "\n",
        "    # 4. Short-Term Interest Rate (STIR)\n",
        "    # Mean reverting around 5%\n",
        "    stir = 5.0 + 2.0 * np.sin(2 * np.pi * t / 8) + np.random.normal(0, 1.0, T)\n",
        "    # Bound within [-5, 40]\n",
        "    stir = np.clip(stir, 0.0, 20.0)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'real_gdp': real_gdp,\n",
        "        'nfcd': nfcd,\n",
        "        'household_debt': household_debt,\n",
        "        'stir': stir\n",
        "    }, index=dates)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Generate datasets for all 6 countries and store in dictionary\n",
        "countries = [\"USA\", \"UK\", \"France\", \"Germany\", \"Canada\", \"Australia\"]\n",
        "raw_datasets = {\n",
        "    country: generate_synthetic_country_data(seed=i)\n",
        "    for i, country in enumerate(countries)\n",
        "}\n",
        "\n",
        "print(f\"Generated datasets for: {list(raw_datasets.keys())}\")\n",
        "print(f\"USA Data Shape: {raw_datasets['USA'].shape}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Configuration Setup (Write & Read)\n",
        "# ==============================================================================\n",
        "\n",
        "# A. Read the config.yaml file\n",
        "print(\"Reading 'config.yaml'...\")\n",
        "with open(\"config.yaml\", \"r\") as f:\n",
        "    study_config = yaml.safe_load(f)\n",
        "\n",
        "# B. Post-processing: Convert lists to numpy arrays\n",
        "# YAML loads nested lists; our pipeline expects numpy arrays for masks.\n",
        "study_config[\"model_physics\"][\"regime_constraints\"][\"regime_1_mask\"] = np.array(\n",
        "    study_config[\"model_physics\"][\"regime_constraints\"][\"regime_1_mask\"]\n",
        ")\n",
        "study_config[\"model_physics\"][\"regime_constraints\"][\"regime_2_mask\"] = np.array(\n",
        "    study_config[\"model_physics\"][\"regime_constraints\"][\"regime_2_mask\"]\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Execution\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n>>> Executing Minsky Research Project Pipeline...\")\n",
        "try:\n",
        "    # Assumption: All modules and callables are accurately imported into the working environment\n",
        "    # This single call runs Validation -> Data Engineering -> Estimation -> Diagnostics -> Analysis -> Reporting\n",
        "    project_results = execute_minsky_research_project(study_config, raw_datasets)\n",
        "    print(\">>> Execution Successful.\")\n",
        "except Exception as e:\n",
        "    print(f\">>> Execution Failed: {e}\")\n",
        "    # In a real scenario, we would inspect the logs here\n",
        "    raise e\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Result Inspection\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n--- Pipeline Output Summary ---\")\n",
        "print(f\"Keys available: {list(project_results.keys())}\")\n",
        "\n",
        "# A. Inspect Minsky Classification for USA GDP/NFCD\n",
        "minsky_analysis = project_results[\"analysis\"][\"minsky_conditions\"]\n",
        "usa_nfcd_key = (\"USA\", \"GDP_NFCD\")\n",
        "\n",
        "if usa_nfcd_key in minsky_analysis:\n",
        "    res = minsky_analysis[usa_nfcd_key]\n",
        "    print(f\"\\n--- Minsky Analysis: {usa_nfcd_key} ---\")\n",
        "    print(f\"Is Minsky Regime? {res['is_minsky']}\")\n",
        "    print(f\"Oscillatory Dynamics: {res['dynamics']['is_oscillatory']}\")\n",
        "    print(f\"Sign Pattern (beta1>0, alpha2<0): {res['signs']['pattern_valid']}\")\n",
        "    print(f\"Statistically Significant: {res['significance']['valid']}\")\n",
        "\n",
        "# B. Inspect Monte Carlo Robustness\n",
        "mc_results = project_results[\"robustness\"][\"monte_carlo\"]\n",
        "if usa_nfcd_key in mc_results:\n",
        "    mc_stats = mc_results[usa_nfcd_key]\n",
        "    print(f\"\\n--- Monte Carlo Robustness: {usa_nfcd_key} ---\")\n",
        "    print(f\"Fraction satisfying Sign Pattern: {mc_stats['robustness_metrics']['frac_sign']:.2f}\")\n",
        "    print(f\"Fraction satisfying Oscillation: {mc_stats['robustness_metrics']['frac_osc']:.2f}\")\n",
        "\n",
        "# C. View Generated LaTeX Table (Snippet)\n",
        "latex_tables = project_results[\"latex_tables\"]\n",
        "print(\"\\n--- LaTeX Parameter Table (Snippet) ---\")\n",
        "print(latex_tables[\"parameter_estimates\"][:500] + \"...\") # Print first 500 chars\n",
        "\n",
        "# D. View Technical Appendix\n",
        "print(\"\\n--- Technical Appendix ---\")\n",
        "print(project_results[\"technical_appendix\"])\n",
        "\n",
        "# Cleanup\n",
        "os.remove(\"config.yaml\")\n",
        "print(\"\\nRemoved 'config.yaml'.\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MUeUhPd4EpoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Validate Study Configuration Object Structure\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Validate and parse the study configuration dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "class ConfigurationError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for study configuration validation failures.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for errors detected\n",
        "        during the validation of the `study_config` dictionary. It flags structural\n",
        "        inconsistencies, missing keys (e.g., 'max_lags', 'k_regimes'), or invalid\n",
        "        value types (e.g., negative lag lengths) before the computationally\n",
        "        expensive estimation pipeline begins.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        configuration violation.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the initialization\n",
        "        routines to halt immediately if the setup parameters are invalid,\n",
        "        preventing runtime crashes deeper in the pipeline.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `ConfigurationError` instance up the call\n",
        "        stack, signaling that the model inputs are malformed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 1:  Load the `study_parameters` dictionary and verify structural completeness.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_top_level_architecture(study_config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the top-level structure of the study configuration dictionary.\n",
        "\n",
        "    This function enforces that the input is a dictionary and contains exactly\n",
        "    the two required top-level keys: 'input_data_architecture' and 'model_physics'.\n",
        "    This ensures the configuration object adheres to the strict schema required\n",
        "    for the MS-VAR pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    study_config : Dict[str, Any]\n",
        "        The master configuration dictionary containing data schemas and model parameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The function returns nothing if validation passes.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ConfigurationError\n",
        "        If `study_config` is not a dictionary.\n",
        "        If the set of keys does not exactly match the required set.\n",
        "    \"\"\"\n",
        "    # Validate input type\n",
        "    # We ensure the config is a dictionary to allow key-based access.\n",
        "    if not isinstance(study_config, dict):\n",
        "        raise ConfigurationError(\n",
        "            f\"Input 'study_config' must be a dictionary, got {type(study_config)}.\"\n",
        "        )\n",
        "\n",
        "    # Define the strict set of required top-level keys\n",
        "    required_keys: Set[str] = {\"input_data_architecture\", \"model_physics\"}\n",
        "\n",
        "    # Extract actual keys from the input\n",
        "    actual_keys: Set[str] = set(study_config.keys())\n",
        "\n",
        "    # Check for exact set equality\n",
        "    # This prevents missing sections or the injection of unknown parameters.\n",
        "    if actual_keys != required_keys:\n",
        "        missing = required_keys - actual_keys\n",
        "        extra = actual_keys - required_keys\n",
        "        error_msg = \"Top-level configuration keys do not match schema.\"\n",
        "        if missing:\n",
        "            error_msg += f\" Missing: {missing}.\"\n",
        "        if extra:\n",
        "            error_msg += f\" Unexpected: {extra}.\"\n",
        "        raise ConfigurationError(error_msg)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 2: Validate input data architecture schema completeness.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_input_data_architecture(study_config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the 'input_data_architecture' section of the configuration.\n",
        "\n",
        "    This function enforces constraints on the temporal domain (1970-2020, Annual)\n",
        "    and the dataset schemas for the six required countries. It verifies that\n",
        "    each country has the four required features (GDP, NFCD, HD, STIR) and that\n",
        "    the log-transformation flags are set correctly according to the paper's specification.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    study_config : Dict[str, Any]\n",
        "        The master configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ConfigurationError\n",
        "        If temporal domain parameters are incorrect.\n",
        "        If required countries are missing.\n",
        "        If feature schemas are incomplete or incorrect.\n",
        "        If log-transformation flags violate the specification.\n",
        "    \"\"\"\n",
        "    # Access the relevant subsection\n",
        "    data_arch = study_config.get(\"input_data_architecture\")\n",
        "    if not isinstance(data_arch, dict):\n",
        "        raise ConfigurationError(\"'input_data_architecture' must be a dictionary.\")\n",
        "\n",
        "    # 1. Validate Temporal Domain\n",
        "    temporal = data_arch.get(\"temporal_domain\", {})\n",
        "    # Start year must be 1970\n",
        "    if temporal.get(\"start_year\") != 1970:\n",
        "        raise ConfigurationError(f\"Temporal domain start_year must be 1970, got {temporal.get('start_year')}.\")\n",
        "    # End year must be 2020\n",
        "    if temporal.get(\"end_year\") != 2020:\n",
        "        raise ConfigurationError(f\"Temporal domain end_year must be 2020, got {temporal.get('end_year')}.\")\n",
        "    # Frequency must be Annual Start ('AS')\n",
        "    if temporal.get(\"frequency\") != \"AS\":\n",
        "        raise ConfigurationError(f\"Temporal domain frequency must be 'AS', got {temporal.get('frequency')}.\")\n",
        "\n",
        "    # 2. Validate Datasets Keys (Countries)\n",
        "    datasets = data_arch.get(\"datasets\", {})\n",
        "    required_countries: Set[str] = {\"USA\", \"UK\", \"France\", \"Germany\", \"Canada\", \"Australia\"}\n",
        "    actual_countries: Set[str] = set(datasets.keys())\n",
        "\n",
        "    if actual_countries != required_countries:\n",
        "        missing = required_countries - actual_countries\n",
        "        extra = actual_countries - required_countries\n",
        "        raise ConfigurationError(\n",
        "            f\"Dataset country keys mismatch. Missing: {missing}. Unexpected: {extra}.\"\n",
        "        )\n",
        "\n",
        "    # 3. Validate Feature Schemas per Country\n",
        "    required_features: Set[str] = {\"real_gdp\", \"nfcd\", \"household_debt\", \"stir\"}\n",
        "    required_fields: Set[str] = {\"description\", \"unit\", \"source\", \"dtype\", \"transform_log\"}\n",
        "\n",
        "    for country, schema in datasets.items():\n",
        "        features = schema.get(\"features\", {})\n",
        "        actual_features = set(features.keys())\n",
        "\n",
        "        # Check feature presence\n",
        "        if actual_features != required_features:\n",
        "            raise ConfigurationError(\n",
        "                f\"Feature keys for {country} mismatch. \"\n",
        "                f\"Missing: {required_features - actual_features}. \"\n",
        "                f\"Unexpected: {actual_features - required_features}.\"\n",
        "            )\n",
        "\n",
        "        # Check fields within each feature\n",
        "        for feature_name, feature_meta in features.items():\n",
        "            actual_fields = set(feature_meta.keys())\n",
        "            # We check if required fields are a subset of actual fields (allowing extras is okay, but missing is not)\n",
        "            if not required_fields.issubset(actual_fields):\n",
        "                raise ConfigurationError(\n",
        "                    f\"Missing metadata fields in {country}/{feature_name}. \"\n",
        "                    f\"Missing: {required_fields - actual_fields}.\"\n",
        "                )\n",
        "\n",
        "            # Check dtype\n",
        "            if feature_meta[\"dtype\"] != \"float64\":\n",
        "                raise ConfigurationError(\n",
        "                    f\"Feature {country}/{feature_name} must have dtype 'float64'.\"\n",
        "                )\n",
        "\n",
        "            # 4. Validate Log Transformation Flags (Crucial for Model Physics)\n",
        "            # GDP must be logged\n",
        "            if feature_name == \"real_gdp\":\n",
        "                if feature_meta[\"transform_log\"] is not True:\n",
        "                    raise ConfigurationError(\n",
        "                        f\"{country}/real_gdp must have 'transform_log': True.\"\n",
        "                    )\n",
        "            # Others must NOT be logged\n",
        "            else:\n",
        "                if feature_meta[\"transform_log\"] is not False:\n",
        "                    raise ConfigurationError(\n",
        "                        f\"{country}/{feature_name} must have 'transform_log': False.\"\n",
        "                    )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Step 3: Validate model physics parameter domains and types.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_model_physics(study_config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the 'model_physics' section of the configuration.\n",
        "\n",
        "    This function enforces the mathematical and statistical parameters required\n",
        "    for the replication. It checks the HP filter settings, the MS-VAR regime\n",
        "    structure (masks), the EM algorithm settings, and the statistical thresholds\n",
        "    for ADF and Ljung-Box tests.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    study_config : Dict[str, Any]\n",
        "        The master configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ConfigurationError\n",
        "        If HP filter lambda is not 100.0.\n",
        "        If regime masks are incorrect shapes or values.\n",
        "        If EM settings are incorrect.\n",
        "        If statistical thresholds do not match the paper's appendices.\n",
        "    \"\"\"\n",
        "    physics = study_config.get(\"model_physics\")\n",
        "    if not isinstance(physics, dict):\n",
        "        raise ConfigurationError(\"'model_physics' must be a dictionary.\")\n",
        "\n",
        "    # 1. Validate Signal Extraction (HP Filter)\n",
        "    sig_ext = physics.get(\"signal_extraction\", {})\n",
        "    if sig_ext.get(\"algorithm\") != \"Hodrick-Prescott Filter\":\n",
        "        raise ConfigurationError(\"Signal extraction algorithm must be 'Hodrick-Prescott Filter'.\")\n",
        "    # Lambda must be 100.0 for annual data\n",
        "    if sig_ext.get(\"lambda_parameter\") != 100.0:\n",
        "        raise ConfigurationError(f\"HP Filter lambda must be 100.0, got {sig_ext.get('lambda_parameter')}.\")\n",
        "    if sig_ext.get(\"target_component\") != \"cyclical\":\n",
        "        raise ConfigurationError(\"Target component must be 'cyclical'.\")\n",
        "\n",
        "    # 2. Validate Regime Constraints (MS-VAR Structure)\n",
        "    constraints = physics.get(\"regime_constraints\", {})\n",
        "    if constraints.get(\"num_regimes\") != 2:\n",
        "        raise ConfigurationError(\"Number of regimes must be 2.\")\n",
        "    if constraints.get(\"lag_order\") != 1:\n",
        "        raise ConfigurationError(\"Lag order must be 1.\")\n",
        "\n",
        "    # Validate Regime 1 Mask (Full Interaction: 2x2 ones)\n",
        "    mask1 = constraints.get(\"regime_1_mask\")\n",
        "    expected_mask1 = np.array([[1, 1], [1, 1]], dtype=int)\n",
        "    if not isinstance(mask1, np.ndarray) or mask1.shape != (2, 2):\n",
        "        raise ConfigurationError(\"regime_1_mask must be a 2x2 numpy array.\")\n",
        "    if not np.array_equal(mask1, expected_mask1):\n",
        "        raise ConfigurationError(\"regime_1_mask must be all ones (full interaction).\")\n",
        "\n",
        "    # Validate Regime 2 Mask (Diagonal: 2x2 identity-like structure)\n",
        "    mask2 = constraints.get(\"regime_2_mask\")\n",
        "    expected_mask2 = np.array([[1, 0], [0, 1]], dtype=int)\n",
        "    if not isinstance(mask2, np.ndarray) or mask2.shape != (2, 2):\n",
        "        raise ConfigurationError(\"regime_2_mask must be a 2x2 numpy array.\")\n",
        "    if not np.array_equal(mask2, expected_mask2):\n",
        "        raise ConfigurationError(\"regime_2_mask must be diagonal (no interaction).\")\n",
        "\n",
        "    # 3. Validate Estimation Engine\n",
        "    engine = physics.get(\"estimation_engine\", {})\n",
        "    if engine.get(\"algorithm\") != \"Expectation-Maximization (EM)\":\n",
        "        raise ConfigurationError(\"Estimation algorithm must be 'Expectation-Maximization (EM)'.\")\n",
        "\n",
        "    criteria = engine.get(\"convergence_criteria\", {})\n",
        "    if criteria.get(\"tolerance\") != 1e-6:\n",
        "        raise ConfigurationError(\"EM tolerance must be 1e-6.\")\n",
        "    if criteria.get(\"max_iterations\") != 1000:\n",
        "        raise ConfigurationError(\"EM max_iterations must be 1000.\")\n",
        "\n",
        "    # 4. Validate Statistical Thresholds\n",
        "    thresholds = physics.get(\"statistical_thresholds\", {})\n",
        "\n",
        "    # ADF Test (Appendix A)\n",
        "    adf = thresholds.get(\"adf_test\", {})\n",
        "    if adf.get(\"critical_value_5pct\") != -1.94:\n",
        "        raise ConfigurationError(f\"ADF critical value must be -1.94, got {adf.get('critical_value_5pct')}.\")\n",
        "\n",
        "    # Ljung-Box Test (Appendix B)\n",
        "    lb = thresholds.get(\"ljung_box_test\", {})\n",
        "    if lb.get(\"critical_value\") != 6.63:\n",
        "        raise ConfigurationError(f\"Ljung-Box critical value must be 6.63, got {lb.get('critical_value')}.\")\n",
        "    if lb.get(\"lags\") != 3:\n",
        "        raise ConfigurationError(f\"Ljung-Box lags must be 3, got {lb.get('lags')}.\")\n",
        "\n",
        "    # Cycle Conditions (Section 2.1)\n",
        "    cycle = thresholds.get(\"cycle_conditions\", {})\n",
        "    if cycle.get(\"beta1_sign\") != \"positive\":\n",
        "        raise ConfigurationError(\"beta1_sign must be 'positive'.\")\n",
        "    if cycle.get(\"alpha2_sign\") != \"negative\":\n",
        "        raise ConfigurationError(\"alpha2_sign must be 'negative'.\")\n",
        "    if cycle.get(\"discriminant_limit\") != 0.0:\n",
        "        raise ConfigurationError(\"discriminant_limit must be 0.0.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_study_config(study_config: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of the study configuration object.\n",
        "\n",
        "    This function executes the validation pipeline in a granular sequence:\n",
        "    1. Top-level architecture check.\n",
        "    2. Input data schema check.\n",
        "    3. Model physics parameter check.\n",
        "\n",
        "    If any step fails, a ConfigurationError is raised with a descriptive message.\n",
        "    If all steps pass, the function returns True, signaling that the configuration\n",
        "    is valid and safe for downstream processing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    study_config : Dict[str, Any]\n",
        "        The master configuration dictionary to be validated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if the configuration is valid.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ConfigurationError\n",
        "        If any validation step fails.\n",
        "    \"\"\"\n",
        "    # Step 1: Top-level check\n",
        "    validate_top_level_architecture(study_config)\n",
        "\n",
        "    # Step 2: Data architecture check\n",
        "    validate_input_data_architecture(study_config)\n",
        "\n",
        "    # Step 3: Model physics check\n",
        "    validate_model_physics(study_config)\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "2uTYd0-JEsCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Validate Raw Datasets Dictionary Structure\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate Raw Datasets Dictionary Structure\n",
        "# ==============================================================================\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for dataset validation failures.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for errors detected\n",
        "        during the validation of the `datasets` dictionary or its constituent\n",
        "        DataFrames. It is raised when data fails to meet structural (e.g.,\n",
        "        missing columns), temporal (e.g., gaps in index), or domain (e.g.,\n",
        "        non-numeric values) constraints required for the MS-VAR pipeline,\n",
        "        distinguishing data integrity errors from configuration or runtime errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        validation failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the data loading\n",
        "        routines to halt immediately if the input data is compromised.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `DataValidationError` instance up the call\n",
        "        stack, signaling that the input data structure is invalid.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Validate datasets container and country keys.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_datasets_container(datasets: Dict[str, pd.DataFrame]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the top-level structure of the datasets dictionary.\n",
        "\n",
        "    This function enforces that the input `datasets` is a dictionary and that its\n",
        "    keys exactly match the set of six required countries: USA, UK, France, Germany,\n",
        "    Canada, and Australia. This ensures that the pipeline has the necessary\n",
        "    geographical coverage as specified in the study configuration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The dictionary containing raw country data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Returns nothing if validation passes.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    DataValidationError\n",
        "        If `datasets` is not a dictionary.\n",
        "        If the set of keys does not exactly match the required countries.\n",
        "        If any value in the dictionary is not a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # Validate input type\n",
        "    # We ensure the container is a dictionary to allow key-based access.\n",
        "    if not isinstance(datasets, dict):\n",
        "        raise DataValidationError(\n",
        "            f\"Input 'datasets' must be a dictionary, got {type(datasets)}.\"\n",
        "        )\n",
        "\n",
        "    # Define the strict set of required country keys\n",
        "    required_countries: Set[str] = {\"USA\", \"UK\", \"France\", \"Germany\", \"Canada\", \"Australia\"}\n",
        "\n",
        "    # Extract actual keys from the input\n",
        "    actual_countries: Set[str] = set(datasets.keys())\n",
        "\n",
        "    # Check for exact set equality\n",
        "    # This prevents missing countries or the inclusion of unexpected ones.\n",
        "    if actual_countries != required_countries:\n",
        "        missing = required_countries - actual_countries\n",
        "        extra = actual_countries - required_countries\n",
        "        error_msg = \"Dataset country keys do not match schema.\"\n",
        "        if missing:\n",
        "            error_msg += f\" Missing: {missing}.\"\n",
        "        if extra:\n",
        "            error_msg += f\" Unexpected: {extra}.\"\n",
        "        raise DataValidationError(error_msg)\n",
        "\n",
        "    # Validate value types\n",
        "    # Each value must be a pandas DataFrame to support time-series operations.\n",
        "    for country, df in datasets.items():\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            raise DataValidationError(\n",
        "                f\"Data for {country} must be a pandas DataFrame, got {type(df)}.\"\n",
        "            )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Validate DatetimeIndex properties for each country.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_datetime_index(datasets: Dict[str, pd.DataFrame]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the DatetimeIndex properties for each country's DataFrame.\n",
        "\n",
        "    This function enforces strict temporal structure requirements:\n",
        "    1. The index must be a pandas DatetimeIndex.\n",
        "    2. The index must be strictly monotonic increasing (ordered in time).\n",
        "    3. The index must be unique (no duplicate timestamps).\n",
        "    4. The observations must be strictly annual (equidistant intervals of 365 or 366 days).\n",
        "    5. The temporal span must cover at least 30 observations to support MS-VAR estimation.\n",
        "    6. The data must fall within the 1970-2020 domain (though truncation happens later,\n",
        "       we check bounds here).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The dictionary containing raw country data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Returns nothing if validation passes.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    DataValidationError\n",
        "        If any temporal constraint is violated for any country.\n",
        "    \"\"\"\n",
        "    for country, df in datasets.items():\n",
        "        # 1. Check Index Type\n",
        "        if not isinstance(df.index, pd.DatetimeIndex):\n",
        "            raise DataValidationError(\n",
        "                f\"Index for {country} must be a DatetimeIndex, got {type(df.index)}.\"\n",
        "            )\n",
        "\n",
        "        # 2. Check Monotonicity\n",
        "        if not df.index.is_monotonic_increasing:\n",
        "            raise DataValidationError(f\"Index for {country} is not strictly increasing.\")\n",
        "\n",
        "        # 3. Check Uniqueness\n",
        "        if not df.index.is_unique:\n",
        "            raise DataValidationError(f\"Index for {country} contains duplicate timestamps.\")\n",
        "\n",
        "        # 4. Check Equidistance (Annual Frequency)\n",
        "        # We calculate the difference between consecutive dates.\n",
        "        # Since we expect annual data, diffs should be 365 or 366 days.\n",
        "        if len(df) > 1:\n",
        "            diffs = df.index.to_series().diff().dropna()\n",
        "            days = diffs.dt.days\n",
        "            # Allow for leap years (366) and standard years (365)\n",
        "            valid_intervals = days.isin([365, 366])\n",
        "            if not valid_intervals.all():\n",
        "                invalid_diffs = days[~valid_intervals]\n",
        "                raise DataValidationError(\n",
        "                    f\"Index for {country} is not strictly annual. \"\n",
        "                    f\"Found intervals of {invalid_diffs.unique()} days.\"\n",
        "                )\n",
        "\n",
        "        # 5. Check Sample Size\n",
        "        # MS-VAR estimation requires sufficient data points.\n",
        "        if len(df) < 30:\n",
        "            raise DataValidationError(\n",
        "                f\"Insufficient data for {country}. Expected >= 30 observations, got {len(df)}.\"\n",
        "            )\n",
        "\n",
        "        # 6. Check Temporal Bounds\n",
        "        # While cleansing handles truncation, we ensure the raw data isn't completely out of bounds.\n",
        "        start_date = df.index.min()\n",
        "        end_date = df.index.max()\n",
        "\n",
        "        # Bounds from study config (hardcoded here as per task context, but conceptually linked)\n",
        "        min_allowed = pd.Timestamp(\"1970-01-01\")\n",
        "        max_allowed = pd.Timestamp(\"2020-01-01\")\n",
        "\n",
        "        if start_date > max_allowed:\n",
        "            raise DataValidationError(f\"Data for {country} starts after 2020 ({start_date}).\")\n",
        "        if end_date < min_allowed:\n",
        "            raise DataValidationError(f\"Data for {country} ends before 1970 ({end_date}).\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Validate column presence, types, and domain constraints.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_columns_and_domains(datasets: Dict[str, pd.DataFrame]) -> None:\n",
        "    \"\"\"\n",
        "    Validates column presence, data types, and domain constraints for each DataFrame.\n",
        "\n",
        "    This function enforces that:\n",
        "    1. Each DataFrame contains exactly the four required columns: 'real_gdp', 'nfcd',\n",
        "       'household_debt', 'stir'.\n",
        "    2. All columns are of float64 type (or convertible).\n",
        "    3. Domain constraints are met:\n",
        "       - real_gdp > 0 (strictly positive for log transformation).\n",
        "       - nfcd >= 0 (non-negative debt).\n",
        "       - household_debt >= 0 (non-negative debt).\n",
        "       - stir is within a plausible range [-5, 40].\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The dictionary containing raw country data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Returns nothing if validation passes.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    DataValidationError\n",
        "        If columns are missing or extra.\n",
        "        If data types are incorrect.\n",
        "        If domain constraints are violated (though NaNs are allowed here, to be cleaned later).\n",
        "    \"\"\"\n",
        "    required_columns: Set[str] = {\"real_gdp\", \"nfcd\", \"household_debt\", \"stir\"}\n",
        "\n",
        "    for country, df in datasets.items():\n",
        "        # 1. Validate Column Presence\n",
        "        actual_columns = set(df.columns)\n",
        "        if actual_columns != required_columns:\n",
        "            missing = required_columns - actual_columns\n",
        "            extra = actual_columns - required_columns\n",
        "            raise DataValidationError(\n",
        "                f\"Columns for {country} mismatch. \"\n",
        "                f\"Missing: {missing}. Unexpected: {extra}.\"\n",
        "            )\n",
        "\n",
        "        # 2. Validate Data Types\n",
        "        # We check if columns are numeric. We don't strictly enforce float64 here if\n",
        "        # they are int, but we ensure they are numbers.\n",
        "        for col in required_columns:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise DataValidationError(\n",
        "                    f\"Column '{col}' for {country} must be numeric, got {df[col].dtype}.\"\n",
        "                )\n",
        "\n",
        "        # 3. Validate Domain Constraints\n",
        "        # We use boolean indexing to find violations. We ignore NaNs here as they\n",
        "        # will be handled in the cleansing step.\n",
        "\n",
        "        # Real GDP > 0\n",
        "        gdp_violations = df[\"real_gdp\"] <= 0\n",
        "        if gdp_violations.any():\n",
        "            # We raise an error for non-positive GDP because log(<=0) is undefined.\n",
        "            # While cleansing could fix this, raw data with <=0 GDP is likely erroneous.\n",
        "            bad_dates = df.index[gdp_violations].strftime('%Y-%m-%d').tolist()\n",
        "            raise DataValidationError(\n",
        "                f\"Found non-positive Real GDP for {country} at {bad_dates}. \"\n",
        "                \"GDP must be strictly positive for log transformation.\"\n",
        "            )\n",
        "\n",
        "        # NFCD >= 0\n",
        "        nfcd_violations = (df[\"nfcd\"] < 0) & df[\"nfcd\"].notna()\n",
        "        if nfcd_violations.any():\n",
        "            bad_dates = df.index[nfcd_violations].strftime('%Y-%m-%d').tolist()\n",
        "            raise DataValidationError(\n",
        "                f\"Found negative NFCD for {country} at {bad_dates}.\"\n",
        "            )\n",
        "\n",
        "        # Household Debt >= 0\n",
        "        hd_violations = (df[\"household_debt\"] < 0) & df[\"household_debt\"].notna()\n",
        "        if hd_violations.any():\n",
        "            bad_dates = df.index[hd_violations].strftime('%Y-%m-%d').tolist()\n",
        "            raise DataValidationError(\n",
        "                f\"Found negative Household Debt for {country} at {bad_dates}.\"\n",
        "            )\n",
        "\n",
        "        # STIR within [-5, 40]\n",
        "        # Interest rates can be negative, but usually not below -5% or above 40% in developed economies.\n",
        "        stir_violations = ((df[\"stir\"] < -5) | (df[\"stir\"] > 40)) & df[\"stir\"].notna()\n",
        "        if stir_violations.any():\n",
        "            bad_dates = df.index[stir_violations].strftime('%Y-%m-%d').tolist()\n",
        "            # We log a warning or raise an error. Given strict fidelity, we raise.\n",
        "            raise DataValidationError(\n",
        "                f\"Found implausible STIR for {country} at {bad_dates}. \"\n",
        "                \"Expected range [-5, 40].\"\n",
        "            )\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_raw_datasets(datasets: Dict[str, pd.DataFrame]) -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete validation of the raw datasets.\n",
        "\n",
        "    This function executes the validation pipeline for the input data:\n",
        "    1. Validates the container structure and country keys.\n",
        "    2. Validates the temporal structure (DatetimeIndex) of each DataFrame.\n",
        "    3. Validates the column schema, data types, and value domains.\n",
        "\n",
        "    If any step fails, a DataValidationError is raised. If all pass, returns True.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The dictionary containing raw country data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if the datasets are valid.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    DataValidationError\n",
        "        If any validation step fails.\n",
        "    \"\"\"\n",
        "    # Step 1: Container and Keys Check\n",
        "    validate_datasets_container(datasets)\n",
        "\n",
        "    # Step 2: Temporal Structure Check\n",
        "    validate_datetime_index(datasets)\n",
        "\n",
        "    # Step 3: Column and Domain Check\n",
        "    validate_columns_and_domains(datasets)\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "xBTesi90f24m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Data Cleansing and Temporal Alignment\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Data Cleansing and Temporal Alignment\n",
        "# ==============================================================================\n",
        "\n",
        "class DataCleansingError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during data cleansing and alignment.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specialized exception identifier used during the\n",
        "        data preprocessing stage. It distinguishes errors related to data integrity\n",
        "        (e.g., null values, duplicate indices, mismatched timestamps) from\n",
        "        generic runtime or configuration errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input required beyond standard Exception arguments (typically\n",
        "        an error message string).\n",
        "\n",
        "    Processes:\n",
        "        Inherits the standard behavior of the Python `Exception` class. It does\n",
        "        not implement additional logic but provides a semantic distinction for\n",
        "        error handling strategies (try/except blocks).\n",
        "\n",
        "    Outputs:\n",
        "        Raises a `DataCleansingError` instance, halting execution or triggering\n",
        "        an exception handler.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax without adding methods or attributes.\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Handle missing and invalid values.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def handle_missing_and_invalid_values(datasets: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Identifies invalid values, interpolates isolated single-year gaps, and prepares\n",
        "    data for truncation.\n",
        "\n",
        "    This function iterates through each country's dataset and performs the following:\n",
        "    1. Identifies invalid entries: NaNs, Infs, and domain violations (e.g., negative debt).\n",
        "    2. Masks these invalid entries as NaN.\n",
        "    3. Detects gaps in the data.\n",
        "    4. Applies linear interpolation ONLY for single-year gaps (isolated missing values).\n",
        "       Equation: x_t = x_{t-1} + (x_{t+1} - x_{t-1}) / 2\n",
        "    5. Leaves multi-year gaps as NaN, to be handled by truncation in the next step.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The raw validated datasets.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.DataFrame]\n",
        "        A dictionary of partially cleansed DataFrames (interpolated but not yet truncated).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    DataCleansingError\n",
        "        If interpolation fails or data structure is corrupted.\n",
        "    \"\"\"\n",
        "    cleansed_datasets = {}\n",
        "\n",
        "    for country, df in datasets.items():\n",
        "        # Create a copy to avoid mutating the original\n",
        "        df_clean = df.copy()\n",
        "\n",
        "        # 1. Mask Domain Violations as NaN\n",
        "        # Real GDP <= 0\n",
        "        df_clean.loc[df_clean[\"real_gdp\"] <= 0, \"real_gdp\"] = np.nan\n",
        "        # NFCD < 0\n",
        "        df_clean.loc[df_clean[\"nfcd\"] < 0, \"nfcd\"] = np.nan\n",
        "        # Household Debt < 0\n",
        "        df_clean.loc[df_clean[\"household_debt\"] < 0, \"household_debt\"] = np.nan\n",
        "        # STIR outside [-5, 40]\n",
        "        df_clean.loc[(df_clean[\"stir\"] < -5) | (df_clean[\"stir\"] > 40), \"stir\"] = np.nan\n",
        "\n",
        "        # Replace Inf with NaN\n",
        "        df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "        # 2. Interpolate Single-Year Gaps\n",
        "        # We iterate column by column to handle gaps specific to each variable\n",
        "        for col in df_clean.columns:\n",
        "            series = df_clean[col]\n",
        "\n",
        "            # Find indices where data is missing\n",
        "            missing_mask = series.isna()\n",
        "            if not missing_mask.any():\n",
        "                continue\n",
        "\n",
        "            # Get integer locations of NaNs\n",
        "            nan_locs = np.where(missing_mask)[0]\n",
        "\n",
        "            # Identify isolated NaNs\n",
        "            isolated_nans = []\n",
        "            for loc in nan_locs:\n",
        "                # Check bounds\n",
        "                if loc == 0 or loc == len(series) - 1:\n",
        "                    continue # Cannot interpolate boundaries\n",
        "\n",
        "                # Check neighbors\n",
        "                if not missing_mask.iloc[loc - 1] and not missing_mask.iloc[loc + 1]:\n",
        "                    isolated_nans.append(series.index[loc])\n",
        "\n",
        "            # Apply interpolation only to isolated NaNs\n",
        "            if isolated_nans:\n",
        "                for date_idx in isolated_nans:\n",
        "                    # Get previous and next valid values\n",
        "                    # We know they exist and are at loc-1 and loc+1 because of the check above\n",
        "                    loc = df_clean.index.get_loc(date_idx)\n",
        "                    prev_val = series.iloc[loc - 1]\n",
        "                    next_val = series.iloc[loc + 1]\n",
        "\n",
        "                    # Linear interpolation formula\n",
        "                    interpolated_val = prev_val + (next_val - prev_val) / 2.0\n",
        "\n",
        "                    # Update DataFrame\n",
        "                    df_clean.at[date_idx, col] = interpolated_val\n",
        "\n",
        "                    logger.info(f\"Interpolated isolated missing value for {country} - {col} at {date_idx.date()}\")\n",
        "\n",
        "        cleansed_datasets[country] = df_clean\n",
        "\n",
        "    return cleansed_datasets\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Enforce temporal alignment within each country.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def enforce_temporal_alignment(datasets: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Truncates DataFrames to the maximal contiguous temporal intersection of all variables.\n",
        "\n",
        "    This function ensures that for each country, all four variables (GDP, NFCD, HD, STIR)\n",
        "    are present and valid for the exact same time range. It handles multi-year gaps\n",
        "    by selecting the longest contiguous block of valid data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The datasets with isolated gaps interpolated (from Step 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.DataFrame]\n",
        "        The aligned and truncated DataFrames.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    DataCleansingError\n",
        "        If the resulting aligned dataset has fewer than 20 observations.\n",
        "    \"\"\"\n",
        "    aligned_datasets = {}\n",
        "\n",
        "    for country, df in datasets.items():\n",
        "        # 1. Identify rows where ALL columns are valid (no NaNs)\n",
        "        # After Step 1, any remaining NaNs are part of multi-year gaps or boundaries.\n",
        "        valid_rows_mask = df.notna().all(axis=1)\n",
        "\n",
        "        if not valid_rows_mask.any():\n",
        "            logger.warning(f\"No valid overlapping data found for {country}. Excluding from analysis.\")\n",
        "            continue\n",
        "\n",
        "        # 2. Find contiguous blocks of valid data\n",
        "        # We group by the difference between the index (integer location) and a counter.\n",
        "        # Consecutive valid rows will have a constant difference.\n",
        "        # Reset index to get integer index for calculation\n",
        "        df_reset = df.reset_index()\n",
        "        valid_indices = df_reset.index[valid_rows_mask.values]\n",
        "\n",
        "        if len(valid_indices) == 0:\n",
        "             logger.warning(f\"No valid overlapping data found for {country}. Excluding from analysis.\")\n",
        "             continue\n",
        "\n",
        "        # Group consecutive indices\n",
        "        # Series(valid_indices) - Series(range(len)) is constant for contiguous blocks\n",
        "        blocks = valid_indices.to_series().groupby(valid_indices - np.arange(len(valid_indices)))\n",
        "\n",
        "        # 3. Select the longest block\n",
        "        longest_block_indices = []\n",
        "        max_len = 0\n",
        "\n",
        "        for _, block in blocks:\n",
        "            if len(block) > max_len:\n",
        "                max_len = len(block)\n",
        "                longest_block_indices = block.values\n",
        "            # Tie-breaking: prefer earlier block? The prompt says \"earliest chronologically\"\n",
        "            elif len(block) == max_len:\n",
        "                # If current max_len is same, we keep the existing one\n",
        "                pass\n",
        "\n",
        "        # 4. Truncate DataFrame\n",
        "        # Use the indices to slice the original DataFrame\n",
        "        # We need to map integer indices back to the DatetimeIndex\n",
        "        start_idx = longest_block_indices[0]\n",
        "        end_idx = longest_block_indices[-1]\n",
        "\n",
        "        # Slicing by integer position (iloc) is safer here\n",
        "        df_aligned = df.iloc[start_idx : end_idx + 1].copy()\n",
        "\n",
        "        # 5. Validate Sample Size\n",
        "        if len(df_aligned) < 20:\n",
        "            logger.warning(\n",
        "                f\"Aligned data for {country} has {len(df_aligned)} observations (minimum 20 required). \"\n",
        "                \"Excluding from analysis.\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # 6. Final Equidistance Check\n",
        "        # Ensure no internal rows were dropped (which shouldn't happen with contiguous block logic)\n",
        "        diffs = df_aligned.index.to_series().diff().dropna().dt.days\n",
        "        if not diffs.isin([365, 366]).all():\n",
        "             raise DataCleansingError(f\"Temporal alignment failed for {country}: Resulting index is not equidistant.\")\n",
        "\n",
        "        aligned_datasets[country] = df_aligned\n",
        "\n",
        "        logger.info(\n",
        "            f\"Aligned {country}: {df_aligned.index.min().date()} to {df_aligned.index.max().date()} \"\n",
        "            f\"(T={len(df_aligned)})\"\n",
        "        )\n",
        "\n",
        "    return aligned_datasets\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Step 3: Validate cleansed data and freeze canonical input.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_and_freeze_data(datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs final validation on cleansed datasets, computes summary statistics,\n",
        "    and prepares the canonical data structure.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The aligned and truncated datasets.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing:\n",
        "        - 'cleansed_datasets': The final DataFrames.\n",
        "        - 'summary_statistics': Nested dict of stats per country/variable.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    DataCleansingError\n",
        "        If any final validation check fails.\n",
        "    \"\"\"\n",
        "    summary_stats = {}\n",
        "\n",
        "    for country, df in datasets.items():\n",
        "        # 1. Re-run Validations\n",
        "        # Check for NaNs\n",
        "        if df.isna().any().any():\n",
        "            raise DataCleansingError(f\"Final validation failed for {country}: NaNs present.\")\n",
        "\n",
        "        # Check for Infs\n",
        "        if np.isinf(df.values).any():\n",
        "             raise DataCleansingError(f\"Final validation failed for {country}: Infs present.\")\n",
        "\n",
        "        # Check Domains\n",
        "        if (df[\"real_gdp\"] <= 0).any():\n",
        "             raise DataCleansingError(f\"Final validation failed for {country}: Non-positive GDP.\")\n",
        "        if (df[\"nfcd\"] < 0).any():\n",
        "             raise DataCleansingError(f\"Final validation failed for {country}: Negative NFCD.\")\n",
        "        if (df[\"household_debt\"] < 0).any():\n",
        "             raise DataCleansingError(f\"Final validation failed for {country}: Negative Household Debt.\")\n",
        "\n",
        "        # 2. Compute Summary Statistics\n",
        "        stats = df.describe().T\n",
        "        stats[\"skew\"] = df.skew()\n",
        "        stats[\"kurtosis\"] = df.kurt()\n",
        "        stats[\"T\"] = len(df)\n",
        "\n",
        "        summary_stats[country] = stats.to_dict(orient=\"index\")\n",
        "\n",
        "    # 3. Prepare Result Structure\n",
        "    result = {\n",
        "        \"cleansed_datasets\": datasets,\n",
        "        \"summary_statistics\": summary_stats\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_and_align_datasets(datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the data cleansing and temporal alignment pipeline.\n",
        "\n",
        "    Sequence:\n",
        "    1. Handle missing/invalid values (Interpolation).\n",
        "    2. Enforce temporal alignment (Truncation).\n",
        "    3. Validate and freeze canonical input (Stats & Final Check).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The raw validated datasets.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing the cleansed datasets and summary statistics.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting data cleansing pipeline...\")\n",
        "\n",
        "    # Step 1: Interpolation\n",
        "    interpolated_data = handle_missing_and_invalid_values(datasets)\n",
        "\n",
        "    # Step 2: Alignment/Truncation\n",
        "    aligned_data = enforce_temporal_alignment(interpolated_data)\n",
        "\n",
        "    # Step 3: Final Validation & Stats\n",
        "    final_output = validate_and_freeze_data(aligned_data)\n",
        "\n",
        "    logger.info(\"Data cleansing pipeline completed successfully.\")\n",
        "    return final_output\n",
        "\n"
      ],
      "metadata": {
        "id": "kMTHltUNhqBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Apply Logarithmic Transformation to Real GDP\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Apply Logarithmic Transformation to Real GDP\n",
        "# ==============================================================================\n",
        "\n",
        "class TransformationError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during data transformation.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specialized exception identifier used during the\n",
        "        feature engineering or mathematical transformation stage. It flags issues\n",
        "        such as failures in log-return calculations, division by zero during\n",
        "        standardization, or errors in stationarity conversion.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input required beyond standard Exception arguments (typically\n",
        "        an error message string).\n",
        "\n",
        "    Processes:\n",
        "        Inherits the standard behavior of the Python `Exception` class. It does\n",
        "        not implement additional logic but provides a semantic distinction for\n",
        "        upstream error handling.\n",
        "\n",
        "    Outputs:\n",
        "        Raises a `TransformationError` instance, halting execution or triggering\n",
        "        an exception handler.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax without adding methods or attributes.\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Define log transformation rule.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def define_log_transformation_rule(df: pd.DataFrame, country: str) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Defines and executes the logarithmic transformation rule for Real GDP.\n",
        "\n",
        "    Equation:\n",
        "        y_t^(log, c) = ln(gdp_t^c)\n",
        "\n",
        "    This function takes a country's DataFrame, extracts the 'real_gdp' column,\n",
        "    validates that all values are strictly positive, and applies the natural\n",
        "    logarithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The cleansed DataFrame for a specific country.\n",
        "    country : str\n",
        "        The name of the country (for error messaging).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        The log-transformed Real GDP series.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    TransformationError\n",
        "        If 'real_gdp' contains non-positive values.\n",
        "    \"\"\"\n",
        "    # Extract the series\n",
        "    gdp_series = df[\"real_gdp\"]\n",
        "\n",
        "    # Validate strictly positive domain\n",
        "    # Although Task 3 cleansed this, we verify defensively before the math operation.\n",
        "    if (gdp_series <= 0).any():\n",
        "        bad_indices = gdp_series[gdp_series <= 0].index.tolist()\n",
        "        raise TransformationError(\n",
        "            f\"Cannot apply log transform for {country}: Found non-positive GDP at {bad_indices}.\"\n",
        "        )\n",
        "\n",
        "    # Apply natural logarithm\n",
        "    # np.log is the natural logarithm (base e)\n",
        "    log_gdp = np.log(gdp_series)\n",
        "\n",
        "    return log_gdp\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Execute log transformation and validation.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_log_transformation(datasets: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Executes the log transformation for all countries and validates the results.\n",
        "\n",
        "    This function iterates through the datasets, applies the log transform to\n",
        "    'real_gdp', stores the result in a new column 'log_real_gdp', and verifies\n",
        "    that the output contains finite, numeric values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The dictionary of cleansed country DataFrames.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.DataFrame]\n",
        "        The datasets with the added 'log_real_gdp' column.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    TransformationError\n",
        "        If the transformation results in NaNs or Infs.\n",
        "    \"\"\"\n",
        "    transformed_datasets = {}\n",
        "\n",
        "    for country, df in datasets.items():\n",
        "        # Create a copy to avoid mutating the input in place immediately\n",
        "        df_transformed = df.copy()\n",
        "\n",
        "        # Apply the transformation rule\n",
        "        try:\n",
        "            log_gdp_series = define_log_transformation_rule(df_transformed, country)\n",
        "        except TransformationError as e:\n",
        "            raise e\n",
        "\n",
        "        # Validate the result\n",
        "        if not np.isfinite(log_gdp_series).all():\n",
        "            raise TransformationError(\n",
        "                f\"Log transformation for {country} resulted in non-finite values (NaN or Inf).\"\n",
        "            )\n",
        "\n",
        "        # Store the result\n",
        "        # We use a specific column name to distinguish from the raw level\n",
        "        df_transformed[\"log_real_gdp\"] = log_gdp_series\n",
        "\n",
        "        # Verify index alignment\n",
        "        if not df_transformed.index.equals(log_gdp_series.index):\n",
        "             raise TransformationError(f\"Index misalignment after log transform for {country}.\")\n",
        "\n",
        "        transformed_datasets[country] = df_transformed\n",
        "\n",
        "        logger.info(f\"Applied log transform to Real GDP for {country}.\")\n",
        "\n",
        "    return transformed_datasets\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Document transformation metadata.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def document_transformation_metadata(datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Captures and documents metadata regarding the log transformation.\n",
        "\n",
        "    This function records the min/max statistics of the original and transformed\n",
        "    GDP series, the timestamp of the operation, and confirms that other variables\n",
        "    remain in their original levels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The datasets containing both 'real_gdp' and 'log_real_gdp'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing metadata for each country.\n",
        "    \"\"\"\n",
        "    metadata = {}\n",
        "    timestamp = datetime.now().isoformat()\n",
        "\n",
        "    for country, df in datasets.items():\n",
        "        # Calculate statistics\n",
        "        orig_min = df[\"real_gdp\"].min()\n",
        "        orig_max = df[\"real_gdp\"].max()\n",
        "        log_min = df[\"log_real_gdp\"].min()\n",
        "        log_max = df[\"log_real_gdp\"].max()\n",
        "\n",
        "        # Verify other variables are untouched (sanity check)\n",
        "        country_meta = {\n",
        "            \"transformation_date\": timestamp,\n",
        "            \"variable_transformed\": \"real_gdp\",\n",
        "            \"transformation_type\": \"natural_log\",\n",
        "            \"original_range\": {\"min\": float(orig_min), \"max\": float(orig_max)},\n",
        "            \"log_range\": {\"min\": float(log_min), \"max\": float(log_max)},\n",
        "            \"other_variables_status\": \"levels (untouched)\"\n",
        "        }\n",
        "\n",
        "        metadata[country] = country_meta\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_log_transform(datasets: Dict[str, pd.DataFrame]) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the application of the logarithmic transformation to Real GDP.\n",
        "\n",
        "    Sequence:\n",
        "    1. Define and execute the log transformation for all countries.\n",
        "    2. Validate the results (finite, aligned).\n",
        "    3. Document metadata for reproducibility.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The cleansed and aligned datasets.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]\n",
        "        - The datasets with 'log_real_gdp' added.\n",
        "        - The transformation metadata.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting log transformation pipeline...\")\n",
        "\n",
        "    # Steps 1 & 2: Execute and Validate\n",
        "    transformed_data = execute_log_transformation(datasets)\n",
        "\n",
        "    # Step 3: Document Metadata\n",
        "    metadata = document_transformation_metadata(transformed_data)\n",
        "\n",
        "    logger.info(\"Log transformation pipeline completed successfully.\")\n",
        "    return transformed_data, metadata\n"
      ],
      "metadata": {
        "id": "niRvYat5jqO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Implement Hodrick-Prescott Filter for Cycle Extraction\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Implement Hodrick-Prescott Filter for Cycle Extraction\n",
        "# ==============================================================================\n",
        "\n",
        "class HPFilterError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during HP filtering.\n",
        "\n",
        "    Purpose:\n",
        "        This class acts as a distinct identifier for runtime errors encountered\n",
        "        specifically while applying the Hodrick-Prescott filter. This allows\n",
        "        calling functions to differentiate between general data processing\n",
        "        errors and specific mathematical failures within the detrending\n",
        "        algorithm (e.g., invalid lambda parameters or data dimensionality issues).\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts standard arguments passed to the base `Exception` class,\n",
        "        typically an error message string describing the failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a specific type in the exception hierarchy for targeted `except` blocks.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates an `HPFilterError` instance up the call\n",
        "        stack, signaling a specific failure in the trend-cycle decomposition\n",
        "        process.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to ensure syntactical correctness while defining an empty class body\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 1 & 2: Define HP filter optimization problem and solve linear system.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def hp_filter_sparse(series: pd.Series, lamb: float = 100.0) -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Implements the Hodrick-Prescott (HP) filter using sparse matrix algebra.\n",
        "\n",
        "    The HP filter decomposes a time series x_t into a trend component tau_t\n",
        "    and a cyclical component c_t by solving the minimization problem:\n",
        "\n",
        "    min_tau sum((x_t - tau_t)^2) + lambda * sum(((tau_{t+1} - tau_t) - (tau_t - tau_{t-1}))^2)\n",
        "\n",
        "    The first-order condition yields the linear system:\n",
        "    (I + lambda * D^T D) * tau = x\n",
        "\n",
        "    where D is the second-difference matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    series : pd.Series\n",
        "        The input time series (e.g., log GDP).\n",
        "    lamb : float, optional\n",
        "        The smoothing parameter lambda. Default is 100.0 for annual data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.Series, pd.Series]\n",
        "        - The trend component (tau).\n",
        "        - The cyclical component (cycle = x - tau).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    HPFilterError\n",
        "        If the linear system cannot be solved.\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if series.empty:\n",
        "        raise HPFilterError(\"Input series is empty.\")\n",
        "\n",
        "    # Convert to numpy array for calculation\n",
        "    x = series.values\n",
        "    n = len(x)\n",
        "\n",
        "    if n < 3:\n",
        "        raise HPFilterError(f\"Series length {n} is too short for HP filter (min 3).\")\n",
        "\n",
        "    # Construct the second-difference matrix D\n",
        "    data = np.array([np.ones(n-2), -2*np.ones(n-2), np.ones(n-2)])\n",
        "    offsets = [0, 1, 2]\n",
        "    # D shape is (n-2, n)\n",
        "    D = sparse.diags(data, offsets, shape=(n-2, n))\n",
        "\n",
        "    # Construct the system matrix A = I + lambda * D^T D\n",
        "    # I is n x n identity\n",
        "    I = sparse.eye(n)\n",
        "\n",
        "    # D.T @ D results in a pentadiagonal matrix\n",
        "    # A is symmetric positive definite\n",
        "    A = I + lamb * (D.T @ D)\n",
        "\n",
        "    # Solve the linear system A * tau = x\n",
        "    try:\n",
        "        # spsolve is efficient for sparse CSR/CSC matrices\n",
        "        tau = spsolve(A.tocsc(), x)\n",
        "    except Exception as e:\n",
        "        raise HPFilterError(f\"Failed to solve HP filter linear system: {e}\")\n",
        "\n",
        "    # Compute cycle\n",
        "    cycle = x - tau\n",
        "\n",
        "    # Convert back to pandas Series with original index\n",
        "    trend_series = pd.Series(tau, index=series.index, name=f\"{series.name}_trend\")\n",
        "    cycle_series = pd.Series(cycle, index=series.index, name=f\"{series.name}_cycle\")\n",
        "\n",
        "    return trend_series, cycle_series\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Apply HP filter to all series and countries.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def apply_hp_filter_to_datasets(datasets: Dict[str, pd.DataFrame], lamb: float = 100.0) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Applies the HP filter to all relevant variables for each country.\n",
        "\n",
        "    Variables processed:\n",
        "    1. 'log_real_gdp' -> 'gdp_cycle' (Real Variable y_t)\n",
        "    2. 'nfcd' -> 'nfcd_cycle' (Financial Variable f_t)\n",
        "    3. 'household_debt' -> 'hd_cycle' (Financial Variable f_t)\n",
        "    4. 'stir' -> 'stir_cycle' (Financial Variable f_t)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The datasets containing log-transformed GDP and raw financial variables.\n",
        "    lamb : float\n",
        "        The smoothing parameter (default 100.0).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.DataFrame]\n",
        "        The datasets enriched with cyclical component columns.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    HPFilterError\n",
        "        If filtering fails for any series.\n",
        "    \"\"\"\n",
        "    processed_datasets = {}\n",
        "\n",
        "    # Define mapping from input column to output cycle name\n",
        "    # Note: 'log_real_gdp' is the input for GDP cycle, not 'real_gdp'\n",
        "    variable_map = {\n",
        "        \"log_real_gdp\": \"gdp_cycle\",\n",
        "        \"nfcd\": \"nfcd_cycle\",\n",
        "        \"household_debt\": \"hd_cycle\",\n",
        "        \"stir\": \"stir_cycle\"\n",
        "    }\n",
        "\n",
        "    for country, df in datasets.items():\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        for input_col, output_col in variable_map.items():\n",
        "            if input_col not in df_processed.columns:\n",
        "                raise HPFilterError(f\"Required column '{input_col}' missing for {country}.\")\n",
        "\n",
        "            series = df_processed[input_col]\n",
        "\n",
        "            # Apply HP Filter\n",
        "            try:\n",
        "                _, cycle = hp_filter_sparse(series, lamb=lamb)\n",
        "            except HPFilterError as e:\n",
        "                raise HPFilterError(f\"HP Filter failed for {country} - {input_col}: {e}\")\n",
        "\n",
        "            # Store cycle\n",
        "            df_processed[output_col] = cycle\n",
        "\n",
        "            # Validation: Check for NaNs in result (shouldn't happen if input was clean)\n",
        "            if cycle.isna().any():\n",
        "                 raise HPFilterError(f\"HP Filter produced NaNs for {country} - {input_col}.\")\n",
        "\n",
        "        processed_datasets[country] = df_processed\n",
        "        logger.info(f\"Extracted cyclical components for {country} (lambda={lamb}).\")\n",
        "\n",
        "    return processed_datasets\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_hp_cycles(datasets: Dict[str, pd.DataFrame], study_config: Dict[str, Any]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the extraction of cyclical components using the HP filter.\n",
        "\n",
        "    Sequence:\n",
        "    1. Retrieve lambda parameter from configuration.\n",
        "    2. Apply HP filter to all required series.\n",
        "    3. Return enriched datasets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The datasets with log-transformed GDP.\n",
        "    study_config : Dict[str, Any]\n",
        "        The study configuration containing model physics parameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, pd.DataFrame]\n",
        "        The datasets with added cyclical components.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting HP filter cycle extraction...\")\n",
        "\n",
        "    # 1. Retrieve Lambda\n",
        "    try:\n",
        "        lamb = study_config[\"model_physics\"][\"signal_extraction\"][\"lambda_parameter\"]\n",
        "    except KeyError:\n",
        "        raise HPFilterError(\"Configuration missing 'lambda_parameter' in 'model_physics/signal_extraction'.\")\n",
        "\n",
        "    # 2. Apply Filter\n",
        "    datasets_with_cycles = apply_hp_filter_to_datasets(datasets, lamb=lamb)\n",
        "\n",
        "    logger.info(\"Cycle extraction completed successfully.\")\n",
        "    return datasets_with_cycles\n"
      ],
      "metadata": {
        "id": "2A4dUqZFsC2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Construct Bivariate Cyclical State Vectors\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Construct Bivariate Cyclical State Vectors\n",
        "# ==============================================================================\n",
        "\n",
        "class StateVectorError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during state vector construction.\n",
        "\n",
        "    Purpose:\n",
        "        This class acts as a specific error identifier used when aggregating\n",
        "        individual time series (e.g., Carbon, Clean Energy, Geopolitical Risk)\n",
        "        into the unified state vector $Y_t$ required for the MS-VAR model.\n",
        "        It allows the system to segregate errors related to vector dimensionality\n",
        "        and component alignment from general preprocessing exceptions.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically an error message string describing the specific construction failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy to facilitate targeted error\n",
        "        trapping during the matrix formation steps.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `StateVectorError` instance up the call\n",
        "        stack, signaling that the formation of the endogenous variable vector\n",
        "        has failed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to ensure valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 1 & 2: Define bivariate systems and organize into containers.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def build_bivariate_systems(datasets: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Constructs the three bivariate cyclical state vectors for each country.\n",
        "\n",
        "    For each country c, creates:\n",
        "    1. System A (GDP/NFCD): y_t = [gdp_cycle, nfcd_cycle]'\n",
        "    2. System B (GDP/HD):   y_t = [gdp_cycle, hd_cycle]'\n",
        "    3. System C (GDP/STIR): y_t = [gdp_cycle, stir_cycle]'\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The datasets containing cyclical components (output of Task 5).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Dict[str, Any]]\n",
        "        A nested dictionary structure:\n",
        "        {\n",
        "            country: {\n",
        "                \"GDP_NFCD\": {\n",
        "                    \"data\": np.ndarray (T x 2),\n",
        "                    \"index\": pd.DatetimeIndex,\n",
        "                    \"T\": int\n",
        "                },\n",
        "                \"GDP_HD\": { ... },\n",
        "                \"GDP_STIR\": { ... }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    StateVectorError\n",
        "        If required cyclical columns are missing.\n",
        "    \"\"\"\n",
        "    bivariate_systems = {}\n",
        "\n",
        "    # Define the systems and their constituent columns\n",
        "    # Order is CRITICAL: [Real Variable, Financial Variable]\n",
        "    # This aligns with the MS-VAR coefficient matrix structure:\n",
        "    # [ alpha1 alpha2 ]\n",
        "    # [ beta1  beta2  ]\n",
        "    systems_map = {\n",
        "        \"GDP_NFCD\": (\"gdp_cycle\", \"nfcd_cycle\"),\n",
        "        \"GDP_HD\":   (\"gdp_cycle\", \"hd_cycle\"),\n",
        "        \"GDP_STIR\": (\"gdp_cycle\", \"stir_cycle\")\n",
        "    }\n",
        "\n",
        "    for country, df in datasets.items():\n",
        "        country_systems = {}\n",
        "\n",
        "        for system_name, (real_col, fin_col) in systems_map.items():\n",
        "            # Validate column presence\n",
        "            if real_col not in df.columns or fin_col not in df.columns:\n",
        "                raise StateVectorError(\n",
        "                    f\"Missing columns for {country} system {system_name}. \"\n",
        "                    f\"Required: {real_col}, {fin_col}.\"\n",
        "                )\n",
        "\n",
        "            # Extract series\n",
        "            real_series = df[real_col].values\n",
        "            fin_series = df[fin_col].values\n",
        "\n",
        "            # Stack into (T, 2) array\n",
        "            # column 0 = real, column 1 = financial\n",
        "            data_matrix = np.column_stack((real_series, fin_series))\n",
        "\n",
        "            # Store with metadata\n",
        "            country_systems[system_name] = {\n",
        "                \"data\": data_matrix,\n",
        "                \"index\": df.index,\n",
        "                \"T\": len(df),\n",
        "                \"columns\": [real_col, fin_col] # Record for traceability\n",
        "            }\n",
        "\n",
        "        bivariate_systems[country] = country_systems\n",
        "        logger.info(f\"Constructed 3 bivariate systems for {country}.\")\n",
        "\n",
        "    return bivariate_systems\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Step 3: Validate cyclical component properties.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_cyclical_properties(bivariate_systems: Dict[str, Dict[str, Any]]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the statistical properties of the constructed cyclical state vectors.\n",
        "\n",
        "    Checks:\n",
        "    1. Zero Mean: HP filtered cycles should have a mean close to zero.\n",
        "    2. Finite Values: No NaNs or Infs.\n",
        "    3. Non-trivial Variance: Standard deviation should be positive.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bivariate_systems : Dict[str, Dict[str, Any]]\n",
        "        The constructed systems from Step 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    StateVectorError\n",
        "        If validation fails (e.g., NaNs present).\n",
        "    \"\"\"\n",
        "    for country, systems in bivariate_systems.items():\n",
        "        for system_name, sys_data in systems.items():\n",
        "            data = sys_data[\"data\"]\n",
        "\n",
        "            # 1. Check for NaNs/Infs\n",
        "            if not np.isfinite(data).all():\n",
        "                raise StateVectorError(f\"Non-finite values found in {country} - {system_name}.\")\n",
        "\n",
        "            # 2. Check Mean (should be approx 0)\n",
        "            means = np.mean(data, axis=0)\n",
        "            # We use a relatively loose tolerance because for short samples (T~50),\n",
        "            # the HP filter mean isn't exactly zero, but should be small.\n",
        "            if np.any(np.abs(means) > 1e-1):\n",
        "                logger.warning(\n",
        "                    f\"Large mean detected for {country} - {system_name}: {means}. \"\n",
        "                    \"Check HP filter parameters.\"\n",
        "                )\n",
        "\n",
        "            # 3. Check Variance (should be > 0)\n",
        "            stds = np.std(data, axis=0)\n",
        "            if np.any(stds == 0):\n",
        "                raise StateVectorError(f\"Zero variance detected for {country} - {system_name}.\")\n",
        "\n",
        "            # Log stats for audit\n",
        "            logger.debug(f\"{country} {system_name} stats: Mean={means}, Std={stds}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_bivariate_systems(datasets: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the construction and validation of bivariate cyclical state vectors.\n",
        "\n",
        "    Sequence:\n",
        "    1. Build systems (stacking arrays).\n",
        "    2. Validate statistical properties.\n",
        "    3. Return structured dictionary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    datasets : Dict[str, pd.DataFrame]\n",
        "        The datasets containing cyclical components.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Dict[str, Any]]\n",
        "        The validated bivariate systems ready for MS-VAR estimation.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting bivariate system construction...\")\n",
        "\n",
        "    # Step 1 & 2: Build\n",
        "    systems = build_bivariate_systems(datasets)\n",
        "\n",
        "    # Step 3: Validate\n",
        "    validate_cyclical_properties(systems)\n",
        "\n",
        "    logger.info(\"Bivariate system construction completed successfully.\")\n",
        "    return systems\n"
      ],
      "metadata": {
        "id": "8GgbtFVYtt6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Augmented Dickey–Fuller (ADF) Test Specification\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Augmented Dickey-Fuller (ADF) Test Specification\n",
        "# ==============================================================================\n",
        "\n",
        "class ADFTestError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during ADF testing.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specialized exception identifier for errors\n",
        "        encountered during the assessment of time series stationarity using the\n",
        "        Augmented Dickey-Fuller (ADF) test. It distinguishes statistical or\n",
        "        computational failures within the stationarity testing module (e.g.,\n",
        "        convergence issues, insufficient observations for lag selection) from\n",
        "        general runtime errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically an error message string detailing the cause of the ADF failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy to allow for granular error\n",
        "        handling strategies during the pre-estimation phase.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates an `ADFTestError` instance up the call\n",
        "        stack, signaling that the stationarity verification for a specific\n",
        "        time series has failed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to ensure valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 1 & 2: Define ADF regression, lag selection, and estimation.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_adf_test(series: pd.Series, max_lag: Optional[int] = None, autolag: str = 'AIC') -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the Augmented Dickey-Fuller (ADF) test on a single time series.\n",
        "\n",
        "    Regression Specification:\n",
        "        Delta z_t = mu + gamma * z_{t-1} + sum_{j=1}^p phi_j * Delta z_{t-j} + u_t\n",
        "\n",
        "    This specification includes a constant (intercept) but no deterministic trend,\n",
        "    appropriate for cyclical components centered around zero.\n",
        "\n",
        "    Lag Selection:\n",
        "        If max_lag is None, it is calculated as: p_max = floor(12 * (T / 100)^0.25).\n",
        "        The optimal lag p is selected by minimizing the specified information criterion (AIC or BIC).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    series : pd.Series\n",
        "        The time series to test (e.g., a cyclical component).\n",
        "    max_lag : int, optional\n",
        "        Maximum lag order. If None, calculated based on sample size.\n",
        "    autolag : str\n",
        "        Method to select lag length {'AIC', 'BIC', 't-stat', None}. Default 'AIC'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing:\n",
        "        - 'adf_stat': The test statistic (t-stat of gamma).\n",
        "        - 'p_value': MacKinnon's approximate p-value.\n",
        "        - 'used_lag': The lag order p selected.\n",
        "        - 'n_obs': Number of observations used in regression.\n",
        "        - 'critical_values': Dictionary of critical values from standard tables.\n",
        "        - 'icbest': The value of the information criterion at the optimal lag.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ADFTestError\n",
        "        If the test fails (e.g., input too short, constant series).\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if series.empty:\n",
        "        raise ADFTestError(\"Input series is empty.\")\n",
        "\n",
        "    # Drop NaNs (ADF cannot handle missing values)\n",
        "    clean_series = series.dropna()\n",
        "    n = len(clean_series)\n",
        "\n",
        "    if n < 5: # Minimal length for lag=1 + regression\n",
        "        raise ADFTestError(f\"Series length {n} is too short for ADF test.\")\n",
        "\n",
        "    # Calculate max_lag if not provided\n",
        "    if max_lag is None:\n",
        "        # Schwert's rule of thumb\n",
        "        max_lag = int(12 * (n / 100)**0.25)\n",
        "\n",
        "    # Ensure max_lag is valid\n",
        "    max_lag = min(max_lag, n - 2)\n",
        "    if max_lag < 0:\n",
        "        max_lag = 0\n",
        "\n",
        "    try:\n",
        "        # Execute ADF test using statsmodels\n",
        "        # regression='c' implies constant only (intercept)\n",
        "        result = adfuller(clean_series, maxlag=max_lag, regression='c', autolag=autolag)\n",
        "\n",
        "        adf_stat, p_value, used_lag, n_obs, critical_values, icbest = result\n",
        "\n",
        "        return {\n",
        "            \"adf_stat\": float(adf_stat),\n",
        "            \"p_value\": float(p_value),\n",
        "            \"used_lag\": int(used_lag),\n",
        "            \"n_obs\": int(n_obs),\n",
        "            \"critical_values\": critical_values,\n",
        "            \"icbest\": float(icbest) if icbest is not None else None\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise ADFTestError(f\"ADF test failed: {e}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Define decision rule and critical values.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def evaluate_stationarity(adf_result: Dict[str, Any], critical_value: float = -1.94) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluates the stationarity hypothesis based on the ADF statistic and a custom critical value.\n",
        "\n",
        "    Null Hypothesis (H0): The series has a unit root (gamma = 0).\n",
        "    Alternative Hypothesis (H1): The series is stationary (gamma < 0).\n",
        "\n",
        "    Decision Rule:\n",
        "        If adf_stat < critical_value, Reject H0 (Stationary).\n",
        "        Else, Fail to Reject H0 (Non-Stationary).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adf_result : Dict[str, Any]\n",
        "        The output from `execute_adf_test`.\n",
        "    critical_value : float\n",
        "        The critical value for the test (default -1.94 from paper).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The input dictionary enriched with:\n",
        "        - 'custom_critical_value': The threshold used.\n",
        "        - 'is_stationary': Boolean indicating if H0 was rejected.\n",
        "        - 'decision': String description (\"Stationary\" or \"Unit Root\").\n",
        "    \"\"\"\n",
        "    stat = adf_result[\"adf_stat\"]\n",
        "\n",
        "    # Decision logic\n",
        "    is_stationary = stat < critical_value\n",
        "\n",
        "    result = adf_result.copy()\n",
        "    result[\"custom_critical_value\"] = critical_value\n",
        "    result[\"is_stationary\"] = is_stationary\n",
        "    result[\"decision\"] = \"Stationary\" if is_stationary else \"Unit Root\"\n",
        "\n",
        "    return result\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_adf_test(series: pd.Series, study_config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the ADF test for a single series using study configuration parameters.\n",
        "\n",
        "    Sequence:\n",
        "    1. Retrieve critical value from config.\n",
        "    2. Execute ADF test.\n",
        "    3. Evaluate stationarity against the specific critical value.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    series : pd.Series\n",
        "        The time series to test.\n",
        "    study_config : Dict[str, Any]\n",
        "        The study configuration containing statistical thresholds.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The complete ADF test results and decision.\n",
        "    \"\"\"\n",
        "    # 1. Retrieve Parameters\n",
        "    try:\n",
        "        crit_val = study_config[\"model_physics\"][\"statistical_thresholds\"][\"adf_test\"][\"critical_value_5pct\"]\n",
        "    except KeyError:\n",
        "        # Fallback or raise error? Raising error ensures fidelity.\n",
        "        raise ADFTestError(\"Configuration missing 'critical_value_5pct' in 'statistical_thresholds/adf_test'.\")\n",
        "\n",
        "    # 2. Execute Test\n",
        "    # We use AIC for lag selection as it's a common standard if not specified otherwise\n",
        "    raw_result = execute_adf_test(series, autolag='AIC')\n",
        "\n",
        "    # 3. Evaluate\n",
        "    final_result = evaluate_stationarity(raw_result, critical_value=crit_val)\n",
        "\n",
        "    return final_result\n"
      ],
      "metadata": {
        "id": "6k4vdnwdut7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Execute ADF Tests for All Cyclical Series\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Execute ADF Tests for All Cyclical Series\n",
        "# ==============================================================================\n",
        "\n",
        "class StationarityWarning(Warning):\n",
        "    \"\"\"\n",
        "    Warning for series that fail the stationarity test.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific non-fatal alert used during the data\n",
        "        preprocessing phase. It is issued when a time series fails to reject the\n",
        "        null hypothesis of a unit root (e.g., via the ADF test) after applied\n",
        "        transformations (log-returns, differencing). It flags potential\n",
        "        violations of the asymptotic assumptions required for stable VAR parameter\n",
        "        estimation without halting the entire pipeline.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts standard arguments passed to the base `Warning` class,\n",
        "        typically a message string indicating which series failed and its test\n",
        "        statistics.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior of the Python `Warning` class.\n",
        "        It allows for the implementation of distinct filtering policies (e.g.,\n",
        "        `warnings.simplefilter`) to treat stationarity issues separately from\n",
        "        deprecation or runtime warnings.\n",
        "\n",
        "    Outputs:\n",
        "        When issued, it prints a warning message to the standard error stream\n",
        "        or logging system, alerting the analyst to potential model instability\n",
        "        due to non-stationary inputs.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 1 & 2: Apply ADF test to real and financial cycles.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def test_all_cycles(bivariate_systems: Dict[str, Dict[str, Any]], study_config: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes the ADF test for every unique cyclical series in the bivariate systems.\n",
        "\n",
        "    Iterates through each country and system:\n",
        "    1. Tests the GDP cycle (Real Variable) once per country.\n",
        "    2. Tests the Financial Variable cycle for each system.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bivariate_systems : Dict[str, Dict[str, Any]]\n",
        "        The constructed bivariate systems containing data arrays.\n",
        "    study_config : Dict[str, Any]\n",
        "        The study configuration containing statistical thresholds.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Dict[str, Any]]\n",
        "        A list of result dictionaries, each containing:\n",
        "        - 'country': Country name.\n",
        "        - 'variable': Variable name (e.g., 'GDP_cycle', 'NFCD_cycle').\n",
        "        - 'adf_stat': Test statistic.\n",
        "        - 'p_value': Approximate p-value.\n",
        "        - 'is_stationary': Boolean decision.\n",
        "        - 'decision': String description.\n",
        "        - 'used_lag': Lag order used.\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    for country, systems in bivariate_systems.items():\n",
        "        # Track if GDP has been tested for this country to avoid redundancy\n",
        "        gdp_tested = False\n",
        "\n",
        "        for system_name, sys_data in systems.items():\n",
        "            # Extract data and metadata\n",
        "            data = sys_data[\"data\"]\n",
        "            index = sys_data[\"index\"]\n",
        "            columns = sys_data[\"columns\"] # e.g., ['gdp_cycle', 'nfcd_cycle']\n",
        "\n",
        "            # 1. Test GDP Cycle (Column 0)\n",
        "            if not gdp_tested:\n",
        "                gdp_series = pd.Series(data[:, 0], index=index, name=columns[0])\n",
        "                try:\n",
        "                    # run_adf_test is defined in Task 7 context (assumed available)\n",
        "                    # We call the orchestrator from Task 7\n",
        "                    gdp_result = run_adf_test(gdp_series, study_config)\n",
        "\n",
        "                    # Enrich result with metadata\n",
        "                    gdp_result.update({\n",
        "                        \"country\": country,\n",
        "                        \"variable\": columns[0], # 'gdp_cycle'\n",
        "                        \"system_context\": \"All\"\n",
        "                    })\n",
        "                    all_results.append(gdp_result)\n",
        "                    gdp_tested = True\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"ADF test failed for {country} GDP: {e}\")\n",
        "\n",
        "            # 2. Test Financial Cycle (Column 1)\n",
        "            fin_series = pd.Series(data[:, 1], index=index, name=columns[1])\n",
        "            try:\n",
        "                fin_result = run_adf_test(fin_series, study_config)\n",
        "\n",
        "                fin_result.update({\n",
        "                    \"country\": country,\n",
        "                    \"variable\": columns[1], # e.g., 'nfcd_cycle'\n",
        "                    \"system_context\": system_name\n",
        "                })\n",
        "                all_results.append(fin_result)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"ADF test failed for {country} {columns[1]}: {e}\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Step 3: Validate stationarity assumption and document exceptions.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def validate_stationarity_results(adf_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates that all tested series are stationary and documents any exceptions.\n",
        "\n",
        "    This function iterates through the results of the Augmented Dickey-Fuller (ADF)\n",
        "    tests performed on the cyclical components. It checks the 'is_stationary' flag\n",
        "    for each series. If any series fails to reject the null hypothesis of a unit root\n",
        "    (i.e., is non-stationary), it is flagged and logged as a warning.\n",
        "\n",
        "    The function produces a summary report indicating whether the stationarity\n",
        "    assumption holds globally across the dataset, which is a prerequisite for\n",
        "    stable MS-VAR estimation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adf_results : List[Dict[str, Any]]\n",
        "        The list of ADF test results, where each dictionary contains the test\n",
        "        statistics, critical values, and decision for a specific country-variable pair.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A summary dictionary containing:\n",
        "        - 'all_stationary': Boolean indicating if all series passed the stationarity test.\n",
        "        - 'failed_series': A list of dictionaries detailing the series that failed the test.\n",
        "        - 'full_report': The complete input list of results for archival purposes.\n",
        "    \"\"\"\n",
        "    failed_series = []\n",
        "\n",
        "    # Iterate through each result to check the stationarity decision\n",
        "    for res in adf_results:\n",
        "        if not res[\"is_stationary\"]:\n",
        "            # Record details of the failure for the summary report\n",
        "            failed_series.append({\n",
        "                \"country\": res[\"country\"],\n",
        "                \"variable\": res[\"variable\"],\n",
        "                \"adf_stat\": res[\"adf_stat\"],\n",
        "                \"critical_value\": res[\"custom_critical_value\"]\n",
        "            })\n",
        "\n",
        "            # Log a warning with specific statistical details\n",
        "            logger.warning(\n",
        "                f\"Unit root not rejected for {res['country']} - {res['variable']} \"\n",
        "                f\"(Stat: {res['adf_stat']:.3f} > Crit: {res['custom_critical_value']})\"\n",
        "            )\n",
        "\n",
        "    # Determine global success\n",
        "    all_passed = len(failed_series) == 0\n",
        "\n",
        "    if all_passed:\n",
        "        logger.info(\"All cyclical series confirmed stationary (I(0)).\")\n",
        "    else:\n",
        "        logger.warning(f\"{len(failed_series)} series failed stationarity test.\")\n",
        "\n",
        "    return {\n",
        "        \"all_stationary\": all_passed,\n",
        "        \"failed_series\": failed_series,\n",
        "        \"full_report\": adf_results\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def execute_all_adf_tests(bivariate_systems: Dict[str, Dict[str, Any]], study_config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution and validation of ADF tests for all cyclical series.\n",
        "\n",
        "    Sequence:\n",
        "    1. Run ADF tests on all unique series.\n",
        "    2. Validate results against the critical value.\n",
        "    3. Return comprehensive report.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bivariate_systems : Dict[str, Dict[str, Any]]\n",
        "        The constructed bivariate systems.\n",
        "    study_config : Dict[str, Any]\n",
        "        The study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        The validation summary and full results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting ADF stationarity testing...\")\n",
        "\n",
        "    # Step 1 & 2: Execute Tests\n",
        "    raw_results = test_all_cycles(bivariate_systems, study_config)\n",
        "\n",
        "    # Step 3: Validate\n",
        "    validation_summary = validate_stationarity_results(raw_results)\n",
        "\n",
        "    logger.info(\"ADF testing completed.\")\n",
        "    return validation_summary\n"
      ],
      "metadata": {
        "id": "UKb2ulaRxzJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Define the Two-Regime MS-VAR(1) Model\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Define the Two-Regime MS-VAR(1) Model\n",
        "# ==============================================================================\n",
        "\n",
        "class MSVARModel:\n",
        "    \"\"\"\n",
        "    Represents a Two-Regime Markov-Switching Vector Autoregression (MS-VAR) model of order 1.\n",
        "\n",
        "    This class encapsulates the data, parameters, and structural constraints for a\n",
        "    bivariate MS-VAR(1) model. It supports regime-dependent coefficient matrices\n",
        "    and covariance matrices, governed by a first-order Markov chain.\n",
        "\n",
        "    Model Specification:\n",
        "        y_t = A(s_t) * y_{t-1} + epsilon_t\n",
        "        epsilon_t ~ N(0, Sigma(s_t))\n",
        "        s_t follows a first-order Markov chain with transition matrix P.\n",
        "\n",
        "    Regimes:\n",
        "        Regime 1 (Index 0): Interaction Regime (Minsky). Full A matrix allowing cross-variable feedback.\n",
        "        Regime 2 (Index 1): Independence Regime (No Minsky). Diagonal A matrix enforcing decoupling.\n",
        "\n",
        "    Attributes:\n",
        "        country (str): The country name associated with the data.\n",
        "        system_name (str): The system identifier (e.g., 'GDP_NFCD').\n",
        "        data (np.ndarray): The bivariate time series data of shape (T, 2).\n",
        "        T (int): Number of observations.\n",
        "        params (Dict[str, Any]): Dictionary holding current parameter estimates:\n",
        "            - 'A': (2, 2, 2) array, A[s, :, :] representing coefficient matrices.\n",
        "            - 'Sigma': (2, 2, 2) array, Sigma[s, :, :] representing covariance matrices.\n",
        "            - 'P': (2, 2) array, transition matrix P[i, j] = Prob(s_t=j | s_{t-1}=i).\n",
        "        masks (Dict[int, np.ndarray]): Structural masks for A matrices to enforce regime constraints.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, country: str, system_name: str, data: np.ndarray, study_config: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Initializes the MS-VAR model structure with data and configuration constraints.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        country : str\n",
        "            Name of the country.\n",
        "        system_name : str\n",
        "            Name of the bivariate system (e.g., 'GDP_NFCD').\n",
        "        data : np.ndarray\n",
        "            The bivariate cyclical data array of shape (T, 2).\n",
        "        study_config : Dict[str, Any]\n",
        "            The study configuration dictionary containing regime constraints and masks.\n",
        "        \"\"\"\n",
        "        # Initialize key variables\n",
        "        self.country = country\n",
        "        self.system_name = system_name\n",
        "        self.data = data\n",
        "        self.T = data.shape[0]\n",
        "\n",
        "        # Initialize parameter container with zeros\n",
        "        # Shapes correspond to 2 regimes, 2 variables\n",
        "        self.params = {\n",
        "            \"A\": np.zeros((2, 2, 2)),      # Stacked A matrices: [regime, row, col]\n",
        "            \"Sigma\": np.zeros((2, 2, 2)),  # Stacked Sigma matrices: [regime, row, col]\n",
        "            \"P\": np.zeros((2, 2))          # Transition matrix: [from_state, to_state]\n",
        "        }\n",
        "\n",
        "        # Load structural masks from config to enforce regime-specific dynamics\n",
        "        # Note: Config uses 1-based indexing in names, we map to 0-based indices internally\n",
        "        # Regime 1 (Index 0) -> Full Interaction (Mask of ones)\n",
        "        # Regime 2 (Index 1) -> Diagonal (Identity-like mask)\n",
        "        try:\n",
        "            mask1 = study_config[\"model_physics\"][\"regime_constraints\"][\"regime_1_mask\"]\n",
        "            mask2 = study_config[\"model_physics\"][\"regime_constraints\"][\"regime_2_mask\"]\n",
        "\n",
        "            self.masks = {\n",
        "                0: np.array(mask1, dtype=int),\n",
        "                1: np.array(mask2, dtype=int)\n",
        "            }\n",
        "        except KeyError:\n",
        "            # Fallback to default if config is malformed (though validation should prevent this)\n",
        "            # This ensures the class remains robust even if config is partial during testing\n",
        "            logging.warning(\"Regime masks not found in config. Using defaults.\")\n",
        "            self.masks = {\n",
        "                0: np.ones((2, 2), dtype=int),\n",
        "                1: np.eye(2, dtype=int)\n",
        "            }\n",
        "\n",
        "    def set_parameters(self, A: np.ndarray, Sigma: np.ndarray, P: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Sets the model parameters, strictly enforcing structural constraints on A.\n",
        "\n",
        "        This method updates the internal parameter state. It applies the regime-specific\n",
        "        masks to the coefficient matrices A to ensure that Regime 2 remains diagonal\n",
        "        (decoupled) regardless of the input values.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : np.ndarray\n",
        "            Coefficient matrices of shape (2, 2, 2).\n",
        "        Sigma : np.ndarray\n",
        "            Covariance matrices of shape (2, 2, 2).\n",
        "        P : np.ndarray\n",
        "            Transition matrix of shape (2, 2).\n",
        "        \"\"\"\n",
        "        # Enforce masks on A by element-wise multiplication\n",
        "        # This zeroes out off-diagonal elements in Regime 2 (Index 1)\n",
        "        A_constrained = A.copy()\n",
        "        A_constrained[0] = A[0] * self.masks[0]\n",
        "        A_constrained[1] = A[1] * self.masks[1]\n",
        "\n",
        "        self.params[\"A\"] = A_constrained\n",
        "        self.params[\"Sigma\"] = Sigma\n",
        "        self.params[\"P\"] = P\n",
        "\n",
        "    def validate_parameters(self) -> bool:\n",
        "        \"\"\"\n",
        "        Validates the current parameters for mathematical and structural consistency.\n",
        "\n",
        "        Checks performed:\n",
        "        1. Transition matrix P: Row sums must equal 1 (stochastic matrix).\n",
        "        2. Covariance matrices Sigma: Must be positive definite for both regimes.\n",
        "        3. Coefficient matrices A: Must strictly obey the structural masks (zeros where required).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        bool\n",
        "            True if all parameters are valid.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If any parameter constraint is violated.\n",
        "        \"\"\"\n",
        "        P = self.params[\"P\"]\n",
        "        # Check row sums of transition matrix\n",
        "        if not np.allclose(P.sum(axis=1), 1.0):\n",
        "            raise ValueError(f\"Transition matrix rows do not sum to 1: {P.sum(axis=1)}\")\n",
        "\n",
        "        for s in range(2):\n",
        "            # Check Sigma positive definiteness via eigenvalues\n",
        "            eigvals = np.linalg.eigvalsh(self.params[\"Sigma\"][s])\n",
        "            if np.any(eigvals <= 0):\n",
        "                raise ValueError(f\"Regime {s} covariance matrix is not positive definite.\")\n",
        "\n",
        "            # Check A mask compliance\n",
        "            # Elements that should be zero (mask=0) must be zero\n",
        "            masked_elements = self.params[\"A\"][s] * (1 - self.masks[s])\n",
        "            if not np.allclose(masked_elements, 0.0):\n",
        "                raise ValueError(f\"Regime {s} coefficient matrix violates structural mask.\")\n",
        "\n",
        "        return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def instantiate_msvar_model(country: str, system_name: str, data: np.ndarray, study_config: Dict[str, Any]) -> MSVARModel:\n",
        "    \"\"\"\n",
        "    Factory function to create and initialize an MSVARModel instance.\n",
        "\n",
        "    This orchestrator handles the instantiation of the MSVARModel class, ensuring\n",
        "    that the correct data and configuration are passed. It serves as the entry point\n",
        "    for creating model objects in the estimation pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    country : str\n",
        "        Name of the country.\n",
        "    system_name : str\n",
        "        Name of the bivariate system (e.g., 'GDP_NFCD').\n",
        "    data : np.ndarray\n",
        "        The bivariate cyclical data array of shape (T, 2).\n",
        "    study_config : Dict[str, Any]\n",
        "        The study configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    MSVARModel\n",
        "        The initialized and configured MS-VAR model object.\n",
        "    \"\"\"\n",
        "    # Fit model\n",
        "    model = MSVARModel(country, system_name, data, study_config)\n",
        "\n",
        "    logging.debug(f\"Instantiated MS-VAR model for {country} - {system_name}.\")\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "9Q69rhi7zdAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Initialize EM Algorithm Parameters\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Initialize EM Algorithm Parameters\n",
        "# ==============================================================================\n",
        "\n",
        "class InitializationError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during EM initialization.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for runtime errors\n",
        "        encountered during the initialization of the Markov-Switching VAR model parameters.\n",
        "        It flags critical failures in the startup phase of the Expectation-Maximization (EM)\n",
        "        algorithm—such as convergence failures in K-Means clustering used for regime\n",
        "        guessing or singularity issues when estimating initial covariance matrices—before\n",
        "        the iterative estimation begins.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the initialization failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy to facilitate targeted error\n",
        "        handling during the model instantiation and parameter seeding steps.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates an `InitializationError` instance up the call\n",
        "        stack, signaling that the iterative estimation process cannot commence\n",
        "        due to invalid starting conditions.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 1: Estimate baseline single-regime VAR(1).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_baseline_var(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Estimates a zero-intercept VAR(1) model using OLS.\n",
        "\n",
        "    Model: y_t = A * y_{t-1} + u_t\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        The bivariate time series data of shape (T, 2).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[np.ndarray, np.ndarray]\n",
        "        - A_hat: Estimated coefficient matrix (2, 2).\n",
        "        - Sigma_hat: Estimated residual covariance matrix (2, 2).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    InitializationError\n",
        "        If OLS estimation fails (e.g., singular matrix).\n",
        "    \"\"\"\n",
        "    T, K = data.shape\n",
        "\n",
        "    # Prepare regressors (Y_{t-1}) and targets (Y_t)\n",
        "    # X = Y_{t-1} (rows 0 to T-2)\n",
        "    # Y = Y_t     (rows 1 to T-1)\n",
        "    X = data[:-1]\n",
        "    Y = data[1:]\n",
        "\n",
        "    # OLS Estimation: Y = X * A' + U\n",
        "    # We want A such that y_t = A y_{t-1}.\n",
        "    # In matrix form Y ~ X A.T\n",
        "    # Solution: A.T = (X'X)^(-1) X'Y  => A = Y'X (X'X)^(-1)\n",
        "    try:\n",
        "        # Use lstsq for numerical stability\n",
        "        # lstsq solves X * B = Y -> B = A.T\n",
        "        A_transpose, residuals, rank, s = np.linalg.lstsq(X, Y, rcond=None)\n",
        "        A_hat = A_transpose.T\n",
        "\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        raise InitializationError(f\"OLS estimation failed: {e}\")\n",
        "\n",
        "    # Compute Residuals\n",
        "    # U = Y - X * A'\n",
        "    U_hat = Y - X @ A_hat.T\n",
        "\n",
        "    # Compute Covariance\n",
        "    # Sigma = (U'U) / (T_eff - K*p)\n",
        "    # Here p=1 (lag), K=2 (vars). T_eff = T-1.\n",
        "    # Degrees of freedom correction: T-1 - 2*1?\n",
        "    # Standard VAR estimators often use T-1-Kp or just T.\n",
        "    # We use T-1-2 for unbiasedness in small samples.\n",
        "    df = (T - 1) - 2\n",
        "    if df <= 0:\n",
        "        raise InitializationError(\"Insufficient degrees of freedom for covariance estimation.\")\n",
        "\n",
        "    Sigma_hat = (U_hat.T @ U_hat) / df\n",
        "\n",
        "    return A_hat, Sigma_hat\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Step 2 & 3: Map to regime-specific values and initialize transition matrix.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def construct_initial_parameters(A_hat: np.ndarray, Sigma_hat: np.ndarray) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Constructs the initial parameter set for the MS-VAR EM algorithm.\n",
        "\n",
        "    Mappings:\n",
        "    - Regime 1 (Minsky): A1 = A_hat (Full interaction)\n",
        "    - Regime 2 (Independence): A2 = diag(A_hat) (No interaction)\n",
        "    - Covariances: Sigma1 = Sigma_hat, Sigma2 = Sigma_hat + perturbation\n",
        "    - Transition Matrix: P = [[0.8, 0.2], [0.2, 0.8]]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    A_hat : np.ndarray\n",
        "        Baseline VAR coefficient matrix.\n",
        "    Sigma_hat : np.ndarray\n",
        "        Baseline VAR covariance matrix.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, np.ndarray]\n",
        "        Dictionary containing 'A', 'Sigma', 'P' with correct shapes for MS-VAR.\n",
        "    \"\"\"\n",
        "    # 1. Coefficient Matrices\n",
        "    # Shape (2, 2, 2) -> [regime, row, col]\n",
        "    A_init = np.zeros((2, 2, 2))\n",
        "\n",
        "    # Regime 1: Full A\n",
        "    A_init[0] = A_hat\n",
        "\n",
        "    # Regime 2: Diagonal A\n",
        "    A_init[1] = np.diag(np.diag(A_hat))\n",
        "\n",
        "    # 2. Covariance Matrices\n",
        "    # Shape (2, 2, 2)\n",
        "    Sigma_init = np.zeros((2, 2, 2))\n",
        "\n",
        "    # Regime 1\n",
        "    Sigma_init[0] = Sigma_hat\n",
        "\n",
        "    # Regime 2: Perturb slightly to break symmetry and aid identification\n",
        "    # We add a small scalar to the diagonal\n",
        "    perturbation = 1e-4 * np.eye(2)\n",
        "    Sigma_init[1] = Sigma_hat + perturbation\n",
        "\n",
        "    # Verify positive definiteness\n",
        "    for s in range(2):\n",
        "        eigvals = np.linalg.eigvalsh(Sigma_init[s])\n",
        "        if np.any(eigvals <= 0):\n",
        "            # Regularize if not PD\n",
        "            min_eig = np.min(eigvals)\n",
        "            fix = (-min_eig + 1e-6) * np.eye(2)\n",
        "            Sigma_init[s] += fix\n",
        "            logger.warning(f\"Regularized initial Sigma for regime {s}.\")\n",
        "\n",
        "    # 3. Transition Matrix\n",
        "    # Symmetric persistence\n",
        "    P_init = np.array([\n",
        "        [0.8, 0.2],\n",
        "        [0.2, 0.8]\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        \"A\": A_init,\n",
        "        \"Sigma\": Sigma_init,\n",
        "        \"P\": P_init\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 10, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def initialize_em_parameters(bivariate_systems: Dict[str, Dict[str, Any]]) -> Dict[Tuple[str, str], Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the initialization of EM parameters for all models.\n",
        "\n",
        "    Sequence:\n",
        "    1. Estimate baseline VAR for each system.\n",
        "    2. Construct initial MS-VAR parameters.\n",
        "    3. Store in a dictionary keyed by (country, system).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bivariate_systems : Dict[str, Dict[str, Any]]\n",
        "        The constructed bivariate systems.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[Tuple[str, str], Dict[str, np.ndarray]]\n",
        "        A dictionary mapping (country, system_name) to the initial parameter dict.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting EM parameter initialization...\")\n",
        "\n",
        "    initial_params = {}\n",
        "\n",
        "    # Iterate through countries and systems\n",
        "    for country, systems in bivariate_systems.items():\n",
        "        for system_name, sys_data in systems.items():\n",
        "            data = sys_data[\"data\"]\n",
        "\n",
        "            try:\n",
        "                # Step 1: Baseline VAR\n",
        "                A_hat, Sigma_hat = estimate_baseline_var(data)\n",
        "\n",
        "                # Step 2 & 3: Construct MS-VAR Init\n",
        "                params = construct_initial_parameters(A_hat, Sigma_hat)\n",
        "\n",
        "                initial_params[(country, system_name)] = params\n",
        "\n",
        "            except InitializationError as e:\n",
        "                logger.error(f\"Initialization failed for {country} - {system_name}: {e}\")\n",
        "                # We might choose to skip this model or halt.\n",
        "                # For now, we log and skip, but the main loop should handle missing keys.\n",
        "\n",
        "    logger.info(f\"Initialized parameters for {len(initial_params)} models.\")\n",
        "\n",
        "    return initial_params\n"
      ],
      "metadata": {
        "id": "luxddaBe5EGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Implement Hamilton Forward Filter\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Implement Hamilton Forward Filter\n",
        "# ==============================================================================\n",
        ".\n",
        "class FilterError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during Hamilton filtering.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for numerical or logical\n",
        "        errors encountered during the forward filtering recursion (Hamilton Filter).\n",
        "        It distinguishes critical failures in calculating filtered regime probabilities\n",
        "        ($\\\\xi_{t|t}$)—such as likelihood underflow, non-positive definite covariance\n",
        "        matrices during density evaluation, or invalid transition probabilities—from\n",
        "        general runtime errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific filtering\n",
        "        failure (e.g., \"Likelihood collapse at t=50\").\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the calling estimation\n",
        "        loop to catch filtering errors specifically, potentially triggering\n",
        "        optimization restarts or alternative numerical strategies.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `FilterError` instance up the call stack,\n",
        "        signaling that the recursive inference of the latent state vector has\n",
        "        failed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 2: Compute regime-conditional densities (Helper Function).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_conditional_log_densities(data: np.ndarray, params: Dict[str, np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the log-density of the observations conditional on each regime.\n",
        "\n",
        "    For each time t and regime s, computes:\n",
        "        log f(y_t | s_t=s, y_{t-1}) = log N(y_t; A_s y_{t-1}, Sigma_s)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        The bivariate data array of shape (T, 2).\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Dictionary containing 'A' (2, 2, 2) and 'Sigma' (2, 2, 2).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        Array of log-densities with shape (T, 2).\n",
        "        Entry [t, s] is the log-likelihood of observation t given regime s.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FilterError\n",
        "        If covariance matrices are singular.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    T, K = data.shape\n",
        "    A = params[\"A\"]         # Shape (2, 2, 2) -> [regime, row, col]\n",
        "    Sigma = params[\"Sigma\"] # Shape (2, 2, 2)\n",
        "\n",
        "    log_densities = np.zeros((T, 2))\n",
        "\n",
        "    # Precompute inverse covariances and log determinants\n",
        "    inv_Sigma = np.zeros_like(Sigma)\n",
        "    log_det_Sigma = np.zeros(2)\n",
        "    const_term = -0.5 * K * np.log(2 * np.pi)\n",
        "\n",
        "    for s in range(2):\n",
        "        try:\n",
        "            # Cholesky decomposition for stability: Sigma = L L^T\n",
        "            L = np.linalg.cholesky(Sigma[s])\n",
        "            inv_Sigma[s] = np.linalg.inv(Sigma[s])\n",
        "            log_det_Sigma[s] = 2 * np.sum(np.log(np.diag(L)))\n",
        "        except np.linalg.LinAlgError:\n",
        "            raise FilterError(f\"Singular covariance matrix in regime {s}.\")\n",
        "\n",
        "    # Loop over time (vectorized over regimes would be possible but loop is clear)\n",
        "    # We start from t=1 because we need y_{t-1}.\n",
        "    # For t=0, we cannot compute density conditional on y_{-1} unless we assume it.\n",
        "    # Standard practice: Conditional likelihood starts at t=1.\n",
        "    # We fill t=0 with zeros or handle it in the filter loop.\n",
        "    for t in range(1, T):\n",
        "        y_t = data[t]\n",
        "        y_tm1 = data[t-1]\n",
        "\n",
        "        for s in range(2):\n",
        "            # Mean: mu_t = A_s * y_{t-1}\n",
        "            mu = A[s] @ y_tm1\n",
        "\n",
        "            # Residual: e_t = y_t - mu_t\n",
        "            resid = y_t - mu\n",
        "\n",
        "            # Quadratic form: e_t' * inv_Sigma * e_t\n",
        "            quad_form = resid @ inv_Sigma[s] @ resid\n",
        "\n",
        "            # Log density\n",
        "            log_densities[t, s] = const_term - 0.5 * log_det_Sigma[s] - 0.5 * quad_form\n",
        "\n",
        "    return log_densities\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 11, Step 1 & 3: Implement Forward Filter Recursion.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_hamilton_filter(data: np.ndarray, params: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the Hamilton forward filter to infer regime probabilities.\n",
        "\n",
        "    Recursion:\n",
        "    1. Prediction: P(s_t=j | Y_{t-1}) = sum_i p_ij * P(s_{t-1}=i | Y_{t-1})\n",
        "    2. Update: P(s_t=j | Y_t) propto f(y_t | s_t=j) * P(s_t=j | Y_{t-1})\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        The bivariate data array (T, 2).\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Dictionary containing 'A', 'Sigma', 'P'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        - 'filtered_probs': (T, 2) array of P(s_t | Y_t).\n",
        "        - 'predicted_probs': (T, 2) array of P(s_t | Y_{t-1}).\n",
        "        - 'log_likelihood': Scalar total log-likelihood.\n",
        "    \"\"\"\n",
        "    T = data.shape[0]\n",
        "    P_trans = params[\"P\"] # P[i, j] = Prob(i -> j)\n",
        "\n",
        "    # 1. Compute Conditional Log-Densities\n",
        "    log_eta = compute_conditional_log_densities(data, params)\n",
        "\n",
        "    # Initialize containers\n",
        "    filtered_probs = np.zeros((T, 2))\n",
        "    predicted_probs = np.zeros((T, 2))\n",
        "    log_likelihoods = np.zeros(T)\n",
        "\n",
        "    # Initialization at t=0\n",
        "    # We use the stationary distribution of P as the initial prior\n",
        "    # Solve pi = pi * P => (I - P^T) * pi = 0, sum(pi) = 1\n",
        "    eigvals, eigvecs = np.linalg.eig(P_trans.T)\n",
        "    # Find eigenvector for eigenvalue 1\n",
        "    idx = np.argmin(np.abs(eigvals - 1.0))\n",
        "    pi_stat = np.real(eigvecs[:, idx])\n",
        "    pi_stat /= np.sum(pi_stat)\n",
        "\n",
        "    # For t=0, we don't have y_{-1}, so we can't compute a likelihood update based on dynamics.\n",
        "    # We set filtered_probs[0] = stationary distribution (prior).\n",
        "    # Or we could treat y_0 as fixed.\n",
        "    # Standard approach: Start recursion at t=1.\n",
        "    filtered_probs[0] = pi_stat\n",
        "    predicted_probs[0] = pi_stat # Not strictly used but good for completeness\n",
        "\n",
        "    total_log_lik = 0.0\n",
        "\n",
        "    # Forward Loop\n",
        "    for t in range(1, T):\n",
        "        # 1. Prediction Step: xi_{t|t-1} = P^T * xi_{t-1|t-1}\n",
        "        # P[i, j] is prob i -> j.\n",
        "        # Prob(s_t=j) = sum_i Prob(s_{t-1}=i) * P[i, j]\n",
        "        # Vectorized: pred = filtered[t-1] @ P\n",
        "        pred = filtered_probs[t-1] @ P_trans\n",
        "        predicted_probs[t] = pred\n",
        "\n",
        "        # 2. Update Step\n",
        "        # We work with log-densities to avoid underflow\n",
        "        # log_unnorm_prob = log(pred) + log_eta[t]\n",
        "        # We use the log-sum-exp trick to compute the normalization constant (likelihood contribution)\n",
        "\n",
        "        log_pred = np.log(pred + 1e-20) # Safety floor\n",
        "        log_joint = log_pred + log_eta[t]\n",
        "\n",
        "        # Compute max for stability\n",
        "        max_log = np.max(log_joint)\n",
        "\n",
        "        # Likelihood contribution f(y_t | Y_{t-1}) = sum_j exp(log_joint_j)\n",
        "        # log_lik_t = max_log + log(sum(exp(log_joint - max_log)))\n",
        "        term = np.sum(np.exp(log_joint - max_log))\n",
        "        log_lik_t = max_log + np.log(term)\n",
        "\n",
        "        total_log_lik += log_lik_t\n",
        "        log_likelihoods[t] = log_lik_t\n",
        "\n",
        "        # Filtered probabilities: exp(log_joint - log_lik_t)\n",
        "        # This is equivalent to exp(log_joint) / sum(exp(log_joint))\n",
        "        filt = np.exp(log_joint - log_lik_t)\n",
        "\n",
        "        # Ensure normalization (handle numerical noise)\n",
        "        filt /= np.sum(filt)\n",
        "        filtered_probs[t] = filt\n",
        "\n",
        "    return {\n",
        "        \"filtered_probs\": filtered_probs,\n",
        "        \"predicted_probs\": predicted_probs,\n",
        "        \"log_likelihood\": total_log_lik,\n",
        "        \"log_densities\": log_eta # Needed for smoother\n",
        "    }\n"
      ],
      "metadata": {
        "id": "SKNirHAq9Yd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Implement Kim Backward Smoother\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Implement Kim Backward Smoother\n",
        "# ==============================================================================\n",
        "\n",
        "class SmootherError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during Kim smoothing.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for numerical or logical\n",
        "        errors encountered during the backward smoothing recursion (Kim's Smoother).\n",
        "        It distinguishes critical failures in calculating smoothed regime probabilities\n",
        "        ($\\\\xi_{t|T}$)—such as division by zero when the predicted probability\n",
        "        $\\\\xi_{t+1|t}$ is close to zero, or numerical instability in the recursive\n",
        "        update equation—from general runtime or forward filtering errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific smoothing\n",
        "        failure (e.g., \"Smoothing recursion failed at t=100 due to singularity\").\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the calling estimation\n",
        "        routine to differentiate between filtering failures (forward pass) and\n",
        "        smoothing failures (backward pass).\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `SmootherError` instance up the call stack,\n",
        "        signaling that the retrospective inference of the latent state vector has\n",
        "        failed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 1 & 2: Backward recursion for smoothed single-state probabilities.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_smoothed_probabilities(\n",
        "    filtered_probs: np.ndarray,\n",
        "    predicted_probs: np.ndarray,\n",
        "    params: Dict[str, np.ndarray]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes smoothed probabilities P(s_t | Y_T) using the Kim backward recursion.\n",
        "\n",
        "    Recursion:\n",
        "        xi_{t|T} = xi_{t|t} * [ (P * (xi_{t+1|T} / xi_{t+1|t})) ]\n",
        "        where division and multiplication are element-wise with broadcasting.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filtered_probs : np.ndarray\n",
        "        (T, 2) array of filtered probabilities P(s_t | Y_t).\n",
        "    predicted_probs : np.ndarray\n",
        "        (T, 2) array of predicted probabilities P(s_t | Y_{t-1}).\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Dictionary containing 'P' (transition matrix).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        (T, 2) array of smoothed probabilities.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    SmootherError\n",
        "        If numerical instability occurs (e.g., division by zero).\n",
        "    \"\"\"\n",
        "    # Initialize key variables\n",
        "    T, K = filtered_probs.shape\n",
        "    P_trans = params[\"P\"] # P[i, j] = Prob(i -> j)\n",
        "\n",
        "    smoothed_probs = np.zeros((T, K))\n",
        "\n",
        "    # Step 1: Initialization at t=T\n",
        "    smoothed_probs[T-1] = filtered_probs[T-1]\n",
        "\n",
        "    # Step 2: Backward Recursion\n",
        "    for t in range(T-2, -1, -1):\n",
        "        # Ratio: xi_{t+1|T}(j) / xi_{t+1|t}(j)\n",
        "        # Add epsilon to denominator to prevent division by zero\n",
        "        ratio = smoothed_probs[t+1] / (predicted_probs[t+1] + 1e-20)\n",
        "\n",
        "        # Weight: sum_j p_{ij} * ratio_j\n",
        "        # P is (2, 2), ratio is (2,)\n",
        "        # We want for each i: sum_j P[i, j] * ratio[j]\n",
        "        # This is matrix-vector product: P @ ratio\n",
        "        weight = P_trans @ ratio\n",
        "\n",
        "        # Update: xi_{t|T}(i) = xi_{t|t}(i) * weight_i\n",
        "        smoothed_probs[t] = filtered_probs[t] * weight\n",
        "\n",
        "        # Normalize to ensure sum to 1 (corrects minor numerical drift)\n",
        "        smoothed_probs[t] /= np.sum(smoothed_probs[t])\n",
        "\n",
        "    return smoothed_probs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Step 3: Compute joint smoothed probabilities for transitions.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_joint_smoothed_probabilities(\n",
        "    smoothed_probs: np.ndarray,\n",
        "    filtered_probs: np.ndarray,\n",
        "    predicted_probs: np.ndarray,\n",
        "    params: Dict[str, np.ndarray]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes joint smoothed probabilities P(s_t=i, s_{t+1}=j | Y_T).\n",
        "\n",
        "    Formula:\n",
        "        P(s_t=i, s_{t+1}=j | Y_T) = P(s_{t+1}=j | Y_T) * [ (P(s_t=i | Y_t) * p_{ij}) / P(s_{t+1}=j | Y_t) ]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    smoothed_probs : np.ndarray\n",
        "        (T, 2) array of smoothed probabilities.\n",
        "    filtered_probs : np.ndarray\n",
        "        (T, 2) array of filtered probabilities.\n",
        "    predicted_probs : np.ndarray\n",
        "        (T, 2) array of predicted probabilities.\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Dictionary containing 'P'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        (T-1, 2, 2) array of joint probabilities.\n",
        "        Index [t, i, j] corresponds to transition from t to t+1.\n",
        "    \"\"\"\n",
        "    # Initialize key variables\n",
        "    T, K = smoothed_probs.shape\n",
        "    P_trans = params[\"P\"]\n",
        "\n",
        "    joint_probs = np.zeros((T-1, K, K))\n",
        "\n",
        "    for t in range(T-1):\n",
        "        # Denominator: P(s_{t+1}=j | Y_t)\n",
        "        denom = predicted_probs[t+1] + 1e-20\n",
        "\n",
        "        # Term 1: P(s_{t+1}=j | Y_T) / P(s_{t+1}=j | Y_t)\n",
        "        term1 = smoothed_probs[t+1] / denom\n",
        "\n",
        "        # Term 2: P(s_t=i | Y_t) * p_{ij}\n",
        "        # We want a (2, 2) matrix where element (i, j) is filtered[t, i] * P[i, j]\n",
        "        # filtered[t] is (2,), P is (2, 2)\n",
        "        # Use broadcasting: (2, 1) * (2, 2)\n",
        "        term2 = filtered_probs[t][:, np.newaxis] * P_trans\n",
        "\n",
        "        # Joint: term2 * term1 (broadcast over i)\n",
        "        # term1 is (2,) corresponding to j\n",
        "        # term2 is (i, j)\n",
        "        # Result is (i, j)\n",
        "        joint_t = term2 * term1[np.newaxis, :]\n",
        "\n",
        "        # Normalize so sum over i,j is 1\n",
        "        joint_t /= np.sum(joint_t)\n",
        "\n",
        "        joint_probs[t] = joint_t\n",
        "\n",
        "    return joint_probs\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_kim_smoother(\n",
        "    filter_results: Dict[str, Any],\n",
        "    params: Dict[str, np.ndarray]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the Kim backward smoothing algorithm.\n",
        "\n",
        "    Sequence:\n",
        "    1. Compute smoothed single-state probabilities.\n",
        "    2. Compute joint smoothed probabilities.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filter_results : Dict[str, Any]\n",
        "        Output from `run_hamilton_filter`.\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Model parameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, np.ndarray]\n",
        "        - 'smoothed_probs': (T, 2) array.\n",
        "        - 'joint_smoothed_probs': (T-1, 2, 2) array.\n",
        "    \"\"\"\n",
        "    # Extract filtered and predicted results\n",
        "    filtered = filter_results[\"filtered_probs\"]\n",
        "    predicted = filter_results[\"predicted_probs\"]\n",
        "\n",
        "    # Step 1: Single-state smoothing\n",
        "    smoothed = compute_smoothed_probabilities(filtered, predicted, params)\n",
        "\n",
        "    # Step 2: Joint smoothing\n",
        "    joint = compute_joint_smoothed_probabilities(smoothed, filtered, predicted, params)\n",
        "\n",
        "    return {\n",
        "        \"smoothed_probs\": smoothed,\n",
        "        \"joint_smoothed_probs\": joint\n",
        "    }\n"
      ],
      "metadata": {
        "id": "IQvVj4bx9zry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Implement EM M-Step Parameter Updates\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Implement EM M-Step Parameter Updates\n",
        "# ==============================================================================\n",
        "\n",
        "class MStepError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during M-step updates.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for numerical or logical\n",
        "        errors encountered during the parameter update phase (M-step) of the\n",
        "        Expectation-Maximization algorithm. It distinguishes failures in calculating\n",
        "        the Maximum Likelihood Estimates (MLE) for the model parameters—such as\n",
        "        singular matrices during VAR coefficient regression,\n",
        "        non-positive definite covariance updates, or invalid transition probability\n",
        "        normalizations—from filtering or smoothing errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        optimization failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the estimation\n",
        "        algorithm to abort the current iteration or attempt numerical regularization\n",
        "        strategies specifically when parameter updates fail.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates an `MStepError` instance up the call stack,\n",
        "        signaling that the optimization of the objective function with respect\n",
        "        to the model parameters has failed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 1: Update transition probabilities.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def update_transition_matrix(\n",
        "    joint_smoothed_probs: np.ndarray,\n",
        "    smoothed_probs: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Updates the Markov transition matrix P.\n",
        "\n",
        "    Formula:\n",
        "        p_{ij} = sum_t P(s_t=i, s_{t+1}=j | Y_T) / sum_t P(s_t=i | Y_T)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    joint_smoothed_probs : np.ndarray\n",
        "        (T-1, 2, 2) array of joint probabilities.\n",
        "    smoothed_probs : np.ndarray\n",
        "        (T, 2) array of smoothed probabilities.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        (2, 2) updated transition matrix.\n",
        "    \"\"\"\n",
        "    # Numerator: Sum over time t=1 to T-1\n",
        "    # joint_smoothed_probs has shape (T-1, 2, 2)\n",
        "    num = np.sum(joint_smoothed_probs, axis=0)\n",
        "\n",
        "    # Denominator: Sum over time t=1 to T-1 (exclude T)\n",
        "    # smoothed_probs has shape (T, 2)\n",
        "    # We need sum_{t=1}^{T-1} P(s_t=i)\n",
        "    # Note: smoothed_probs indices are 0 to T-1. We sum 0 to T-2.\n",
        "    denom = np.sum(smoothed_probs[:-1], axis=0)\n",
        "\n",
        "    # Broadcasting division: (2, 2) / (2,) -> divide each row i by denom[i]\n",
        "    P_new = num / (denom[:, np.newaxis] + 1e-20)\n",
        "\n",
        "    # Normalize rows to ensure sum to 1\n",
        "    P_new /= np.sum(P_new, axis=1, keepdims=True)\n",
        "\n",
        "    return P_new\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Update VAR coefficient matrices for both regimes.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def update_coefficients(\n",
        "    data: np.ndarray,\n",
        "    smoothed_probs: np.ndarray,\n",
        "    masks: Dict[int, np.ndarray]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Updates the VAR coefficient matrices A_s for each regime.\n",
        "\n",
        "    Formula (Weighted OLS):\n",
        "        A_s = (sum_t xi_t(s) y_t y_{t-1}') * (sum_t xi_t(s) y_{t-1} y_{t-1}')^-1\n",
        "\n",
        "    Constraints:\n",
        "        Applies structural masks to zero out restricted elements (e.g., off-diagonals in Regime 2).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        (T, 2) data array.\n",
        "    smoothed_probs : np.ndarray\n",
        "        (T, 2) smoothed probabilities.\n",
        "    masks : Dict[int, np.ndarray]\n",
        "        Structural masks for each regime.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        (2, 2, 2) updated coefficient matrices.\n",
        "    \"\"\"\n",
        "    T, K = data.shape\n",
        "    A_new = np.zeros((2, K, K))\n",
        "\n",
        "    # Prepare data matrices\n",
        "    # Y = y_t (t=2..T) -> indices 1..T-1\n",
        "    # X = y_{t-1} (t=2..T) -> indices 0..T-2\n",
        "    Y = data[1:]\n",
        "    X = data[:-1]\n",
        "\n",
        "    # Smoothed probs aligned with t=2..T (indices 1..T-1)\n",
        "    weights = smoothed_probs[1:]\n",
        "\n",
        "    for s in range(2):\n",
        "        # Weighted Moment Matrices\n",
        "        # S_xx = sum_t w_t(s) x_t x_t'\n",
        "        # S_yx = sum_t w_t(s) y_t x_t'\n",
        "\n",
        "        # Weight vector for regime s\n",
        "        w = weights[:, s]\n",
        "\n",
        "        # Efficient weighted sum using broadcasting\n",
        "        # X.T @ (w[:, None] * X)\n",
        "        S_xx = X.T @ (w[:, np.newaxis] * X)\n",
        "        S_yx = Y.T @ (w[:, np.newaxis] * X)\n",
        "\n",
        "        # Solve S_xx * A_s^T = S_yx^T  => A_s * S_xx = S_yx\n",
        "        # A_s = S_yx @ inv(S_xx)\n",
        "\n",
        "        try:\n",
        "            # Add ridge for stability\n",
        "            S_xx_reg = S_xx + 1e-6 * np.eye(K)\n",
        "            A_s = S_yx @ np.linalg.inv(S_xx_reg)\n",
        "        except np.linalg.LinAlgError:\n",
        "            raise MStepError(f\"Singular matrix in coefficient update for regime {s}.\")\n",
        "\n",
        "        # Apply Mask\n",
        "        # This is a projection step. For strict MLE with constraints, one should solve\n",
        "        # the constrained GLS problem. However, zeroing out elements is the standard\n",
        "        # EM approximation when regressors are identical (VAR).\n",
        "        # For Regime 2 (Diagonal), this is equivalent to equation-by-equation OLS.\n",
        "        A_s_constrained = A_s * masks[s]\n",
        "\n",
        "        A_new[s] = A_s_constrained\n",
        "\n",
        "    return A_new\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Step 3: Update covariance matrices for both regimes.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def update_covariances(\n",
        "    data: np.ndarray,\n",
        "    smoothed_probs: np.ndarray,\n",
        "    A_new: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Updates the covariance matrices Sigma_s.\n",
        "\n",
        "    Formula:\n",
        "        Sigma_s = (sum_t xi_t(s) u_t u_t') / (sum_t xi_t(s))\n",
        "        where u_t = y_t - A_s y_{t-1}\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        (T, 2) data array.\n",
        "    smoothed_probs : np.ndarray\n",
        "        (T, 2) smoothed probabilities.\n",
        "    A_new : np.ndarray\n",
        "        (2, 2, 2) updated coefficient matrices.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        (2, 2, 2) updated covariance matrices.\n",
        "    \"\"\"\n",
        "    # Initialize key variables\n",
        "    T, K = data.shape\n",
        "    Sigma_new = np.zeros((2, K, K))\n",
        "\n",
        "    Y = data[1:]\n",
        "    X = data[:-1]\n",
        "    weights = smoothed_probs[1:]\n",
        "\n",
        "    for s in range(2):\n",
        "        # Compute residuals\n",
        "        # U = Y - X @ A_s.T\n",
        "        U = Y - X @ A_new[s].T\n",
        "\n",
        "        # Weighted covariance\n",
        "        w = weights[:, s]\n",
        "        sum_w = np.sum(w) + 1e-20\n",
        "\n",
        "        # Sigma = (U.T @ diag(w) @ U) / sum_w\n",
        "        Sigma_s = (U.T @ (w[:, np.newaxis] * U)) / sum_w\n",
        "\n",
        "        # Regularize\n",
        "        eigvals = np.linalg.eigvalsh(Sigma_s)\n",
        "        if np.min(eigvals) < 1e-6:\n",
        "            Sigma_s += 1e-6 * np.eye(K)\n",
        "\n",
        "        Sigma_new[s] = Sigma_s\n",
        "\n",
        "    return Sigma_new\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def perform_m_step(\n",
        "    data: np.ndarray,\n",
        "    smoother_results: Dict[str, np.ndarray],\n",
        "    masks: Dict[int, np.ndarray]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the M-step parameter updates.\n",
        "\n",
        "    Sequence:\n",
        "    1. Update P.\n",
        "    2. Update A (with masks).\n",
        "    3. Update Sigma.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        Bivariate data.\n",
        "    smoother_results : Dict[str, np.ndarray]\n",
        "        Output from Kim smoother.\n",
        "    masks : Dict[int, np.ndarray]\n",
        "        Structural masks.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, np.ndarray]\n",
        "        Updated parameters 'A', 'Sigma', 'P'.\n",
        "    \"\"\"\n",
        "    smoothed = smoother_results[\"smoothed_probs\"]\n",
        "    joint = smoother_results[\"joint_smoothed_probs\"]\n",
        "\n",
        "    # 1. Update P\n",
        "    P_new = update_transition_matrix(joint, smoothed)\n",
        "\n",
        "    # 2. Update A\n",
        "    A_new = update_coefficients(data, smoothed, masks)\n",
        "\n",
        "    # 3. Update Sigma\n",
        "    Sigma_new = update_covariances(data, smoothed, A_new)\n",
        "\n",
        "    return {\n",
        "        \"A\": A_new,\n",
        "        \"Sigma\": Sigma_new,\n",
        "        \"P\": P_new\n",
        "    }\n"
      ],
      "metadata": {
        "id": "fJGZ0k65_usa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Implement EM Convergence Logic\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Implement EM Convergence Logic\n",
        "# ==============================================================================\n",
        "\n",
        "class ConvergenceError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for EM convergence failures.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for situations where the\n",
        "        Expectation-Maximization (EM) algorithm fails to satisfy its convergence criteria\n",
        "        (e.g., log-likelihood improvement drops below a tolerance threshold, or the\n",
        "        maximum number of iterations is reached without stability). It allows the\n",
        "        estimation pipeline to distinguish between mathematical errors (like singularities)\n",
        "        and algorithmic non-convergence.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the convergence statistics\n",
        "        (e.g., \"EM did not converge after 1000 iterations\").\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This enables the calling code\n",
        "        to implement fallback strategies, such as restarting with different initial\n",
        "        parameters or relaxing convergence tolerances.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `ConvergenceError` instance up the call stack,\n",
        "        halting the estimation process or triggering a specific retry mechanism.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 1 & 2: Check convergence criteria.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def check_convergence(\n",
        "    current_loglik: float,\n",
        "    previous_loglik: float,\n",
        "    iteration: int,\n",
        "    config: Dict[str, Any]\n",
        ") -> Tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Evaluates EM convergence criteria based on log-likelihood changes.\n",
        "\n",
        "    Criteria:\n",
        "    1. Absolute change < tolerance\n",
        "    2. Relative change < tolerance\n",
        "    3. Max iterations reached\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    current_loglik : float\n",
        "        Log-likelihood at current iteration k.\n",
        "    previous_loglik : float\n",
        "        Log-likelihood at previous iteration k-1.\n",
        "    iteration : int\n",
        "        Current iteration number.\n",
        "    config : Dict[str, Any]\n",
        "        Configuration dictionary containing 'convergence_criteria'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[bool, str]\n",
        "        - converged: True if converged, False otherwise.\n",
        "        - reason: Description of the convergence status.\n",
        "    \"\"\"\n",
        "    criteria = config[\"model_physics\"][\"estimation_engine\"][\"convergence_criteria\"]\n",
        "    tol = criteria[\"tolerance\"]\n",
        "    max_iters = criteria[\"max_iterations\"]\n",
        "\n",
        "    # Check monotonicity (EM property)\n",
        "    # Allow small numerical noise, but warn on large drops\n",
        "    diff = current_loglik - previous_loglik\n",
        "    if diff < -1e-4:\n",
        "        logger.warning(f\"Log-likelihood decreased at iteration {iteration}: {diff:.6f}\")\n",
        "\n",
        "    abs_diff = abs(diff)\n",
        "    rel_diff = abs_diff / (abs(previous_loglik) + 1e-10)\n",
        "\n",
        "    if abs_diff < tol:\n",
        "        return True, \"Absolute tolerance reached\"\n",
        "\n",
        "    if rel_diff < tol:\n",
        "        return True, \"Relative tolerance reached\"\n",
        "\n",
        "    if iteration >= max_iters:\n",
        "        return True, \"Maximum iterations reached\"\n",
        "\n",
        "    return False, \"Continuing\"\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Step 3: Compute Standard Errors (Numerical Hessian).\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def flatten_parameters(params: Dict[str, np.ndarray]) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Flattens the structured parameter dictionary into a single vector.\n",
        "\n",
        "    This function converts the dictionary of model parameters (A, Sigma, P) into\n",
        "    a 1D numpy array, which is required for numerical optimization and Hessian\n",
        "    computation. It also returns metadata to reconstruct the original structure.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Dictionary containing 'A', 'Sigma', 'P'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[np.ndarray, Dict[str, Any]]\n",
        "        - theta: Flattened parameter vector.\n",
        "        - shapes: Metadata dictionary containing shapes and lengths to reconstruct the dictionary.\n",
        "    \"\"\"\n",
        "    # Extract and flatten objects\n",
        "    A_flat = params[\"A\"].flatten()\n",
        "    Sigma_flat = params[\"Sigma\"].flatten()\n",
        "    P_flat = params[\"P\"].flatten()\n",
        "\n",
        "    # Compute theta\n",
        "    theta = np.concatenate([A_flat, Sigma_flat, P_flat])\n",
        "\n",
        "    shapes = {\n",
        "        \"A_shape\": params[\"A\"].shape,\n",
        "        \"Sigma_shape\": params[\"Sigma\"].shape,\n",
        "        \"P_shape\": params[\"P\"].shape,\n",
        "        \"A_len\": len(A_flat),\n",
        "        \"Sigma_len\": len(Sigma_flat),\n",
        "        \"P_len\": len(P_flat)\n",
        "    }\n",
        "    return theta, shapes\n",
        "\n",
        "def unflatten_parameters(theta: np.ndarray, shapes: Dict[str, Any]) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Reconstructs the parameter dictionary from a flattened vector.\n",
        "\n",
        "    This function reverses the flattening process, using the metadata provided by\n",
        "    `flatten_parameters` to reshape the 1D vector back into the structured\n",
        "    dictionary of model parameters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    theta : np.ndarray\n",
        "        Flattened parameter vector.\n",
        "    shapes : Dict[str, Any]\n",
        "        Metadata dictionary from `flatten_parameters`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, np.ndarray]\n",
        "        Reconstructed parameter dictionary containing 'A', 'Sigma', 'P'.\n",
        "    \"\"\"\n",
        "    # Extract objects\n",
        "    A_end = shapes[\"A_len\"]\n",
        "    Sigma_end = A_end + shapes[\"Sigma_len\"]\n",
        "\n",
        "    A_flat = theta[:A_end]\n",
        "    Sigma_flat = theta[A_end:Sigma_end]\n",
        "    P_flat = theta[Sigma_end:]\n",
        "\n",
        "    return {\n",
        "        \"A\": A_flat.reshape(shapes[\"A_shape\"]),\n",
        "        \"Sigma\": Sigma_flat.reshape(shapes[\"Sigma_shape\"]),\n",
        "        \"P\": P_flat.reshape(shapes[\"P_shape\"])\n",
        "    }\n",
        "\n",
        "def compute_numerical_hessian(\n",
        "    theta: np.ndarray,\n",
        "    objective_func: Callable[[np.ndarray], float],\n",
        "    epsilon: float = 1e-5\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the Hessian matrix of a scalar function using central finite differences.\n",
        "\n",
        "    The Hessian matrix H contains the second-order partial derivatives of the function:\n",
        "    H_ij = d^2 f / (dx_i dx_j). This implementation uses a central difference\n",
        "    approximation for both diagonal and off-diagonal elements to ensure accuracy.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    theta : np.ndarray\n",
        "        Parameter vector at the optimum.\n",
        "    objective_func : Callable\n",
        "        Function mapping theta -> log-likelihood.\n",
        "    epsilon : float\n",
        "        Step size for finite differences.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        Hessian matrix of shape (N, N).\n",
        "    \"\"\"\n",
        "    n = len(theta)\n",
        "    hessian = np.zeros((n, n))\n",
        "\n",
        "    # Evaluate function at optimum\n",
        "    f_0 = objective_func(theta)\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i, n):\n",
        "            if i == j:\n",
        "                # Diagonal element: d^2 f / dx_i^2\n",
        "                theta_plus = theta.copy()\n",
        "                theta_plus[i] += epsilon\n",
        "                f_plus = objective_func(theta_plus)\n",
        "\n",
        "                theta_minus = theta.copy()\n",
        "                theta_minus[i] -= epsilon\n",
        "                f_minus = objective_func(theta_minus)\n",
        "\n",
        "                hessian[i, i] = (f_plus - 2*f_0 + f_minus) / (epsilon**2)\n",
        "            else:\n",
        "                # Off-diagonal element: d^2 f / (dx_i dx_j)\n",
        "                # f(x + ei + ej)\n",
        "                theta_pp = theta.copy()\n",
        "                theta_pp[i] += epsilon\n",
        "                theta_pp[j] += epsilon\n",
        "                f_pp = objective_func(theta_pp)\n",
        "\n",
        "                # f(x + ei - ej)\n",
        "                theta_pm = theta.copy()\n",
        "                theta_pm[i] += epsilon\n",
        "                theta_pm[j] -= epsilon\n",
        "                f_pm = objective_func(theta_pm)\n",
        "\n",
        "                # f(x - ei + ej)\n",
        "                theta_mp = theta.copy()\n",
        "                theta_mp[i] -= epsilon\n",
        "                theta_mp[j] += epsilon\n",
        "                f_mp = objective_func(theta_mp)\n",
        "\n",
        "                # f(x - ei - ej)\n",
        "                theta_mm = theta.copy()\n",
        "                theta_mm[i] -= epsilon\n",
        "                theta_mm[j] -= epsilon\n",
        "                f_mm = objective_func(theta_mm)\n",
        "\n",
        "                val = (f_pp - f_pm - f_mp + f_mm) / (4 * epsilon**2)\n",
        "                hessian[i, j] = val\n",
        "                hessian[j, i] = val\n",
        "\n",
        "    return hessian\n",
        "\n",
        "def compute_standard_errors(\n",
        "    model_data: np.ndarray,\n",
        "    final_params: Dict[str, np.ndarray],\n",
        "    likelihood_evaluator: Callable[[np.ndarray, Dict[str, np.ndarray]], float]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes standard errors for MS-VAR parameters using numerical Hessian approximation.\n",
        "\n",
        "    This function calculates the asymptotic standard errors of the maximum likelihood\n",
        "    estimates. It approximates the Observed Information Matrix by computing the\n",
        "    negative Hessian of the log-likelihood function at the optimum. The standard\n",
        "    errors are the square roots of the diagonal elements of the inverse information matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_data : np.ndarray\n",
        "        The bivariate data array of shape (T, 2).\n",
        "    final_params : Dict[str, np.ndarray]\n",
        "        The optimized parameter dictionary.\n",
        "    likelihood_evaluator : Callable\n",
        "        A function that takes (data, params) and returns a dictionary containing\n",
        "        the 'log_likelihood'. This is typically the `run_hamilton_filter` function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, np.ndarray]\n",
        "        Dictionary of standard errors with the same structure as `final_params`.\n",
        "        Contains NaNs if the Hessian inversion fails.\n",
        "    \"\"\"\n",
        "    # 1. Flatten parameters\n",
        "    theta_opt, shapes = flatten_parameters(final_params)\n",
        "\n",
        "    # 2. Define objective wrapper\n",
        "    def objective(theta: np.ndarray) -> float:\n",
        "        try:\n",
        "            # Reconstruct params\n",
        "            params = unflatten_parameters(theta, shapes)\n",
        "\n",
        "            # Run filter to get log-likelihood\n",
        "            result = likelihood_evaluator(model_data, params)\n",
        "            return result[\"log_likelihood\"]\n",
        "        except Exception:\n",
        "            # Return a large negative value if parameters are invalid (e.g., non-PD Sigma)\n",
        "            return -1e20\n",
        "\n",
        "    # 3. Compute Hessian of the log-likelihood\n",
        "    H = compute_numerical_hessian(theta_opt, objective)\n",
        "\n",
        "    # 4. Invert Negative Hessian to get Covariance Matrix\n",
        "    try:\n",
        "        # Observed Information Matrix J = -H\n",
        "        # Covariance = J^-1 = (-H)^-1\n",
        "        # Add small ridge for numerical stability if H is near-singular\n",
        "        H_reg = H - 1e-6 * np.eye(len(theta_opt))\n",
        "        cov_matrix = np.linalg.inv(-H_reg)\n",
        "\n",
        "        # 5. Extract SEs (square root of diagonal elements)\n",
        "        variances = np.diag(cov_matrix)\n",
        "        # Handle potential negative variances due to numerical noise\n",
        "        variances = np.maximum(variances, 0)\n",
        "        std_errors_flat = np.sqrt(variances)\n",
        "\n",
        "    except np.linalg.LinAlgError:\n",
        "        logging.warning(\"Hessian inversion failed. Returning NaN standard errors.\")\n",
        "        std_errors_flat = np.full_like(theta_opt, np.nan)\n",
        "\n",
        "    # 6. Unflatten standard errors back to dictionary structure\n",
        "    return unflatten_parameters(std_errors_flat, shapes)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def finalize_estimation(\n",
        "    current_params: Dict[str, np.ndarray],\n",
        "    current_loglik: float,\n",
        "    previous_loglik: float,\n",
        "    iteration: int,\n",
        "    config: Dict[str, Any],\n",
        "    data: np.ndarray,\n",
        "    likelihood_evaluator: Callable\n",
        ") -> Tuple[bool, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates convergence checking and finalization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    current_params : Dict[str, np.ndarray]\n",
        "        Parameters at iteration k.\n",
        "    current_loglik : float\n",
        "        Log-likelihood at iteration k.\n",
        "    previous_loglik : float\n",
        "        Log-likelihood at iteration k-1.\n",
        "    iteration : int\n",
        "        Current iteration.\n",
        "    config : Dict[str, Any]\n",
        "        Configuration.\n",
        "    data : np.ndarray\n",
        "        Data for SE computation.\n",
        "    likelihood_evaluator : Callable\n",
        "        Function to compute log-likelihood.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[bool, Dict[str, Any]]\n",
        "        - converged: Boolean.\n",
        "        - results: Dictionary containing final params, SEs, loglik, etc. (if converged).\n",
        "    \"\"\"\n",
        "    converged, reason = check_convergence(current_loglik, previous_loglik, iteration, config)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    if converged:\n",
        "        logger.info(f\"EM converged at iteration {iteration}: {reason}\")\n",
        "\n",
        "        # Compute SEs using the rigorous numerical Hessian approach\n",
        "        std_errors = compute_standard_errors(data, current_params, likelihood_evaluator)\n",
        "\n",
        "        results = {\n",
        "            \"params\": current_params,\n",
        "            \"std_errors\": std_errors,\n",
        "            \"final_loglik\": current_loglik,\n",
        "            \"iterations\": iteration,\n",
        "            \"convergence_reason\": reason\n",
        "        }\n",
        "\n",
        "    return converged, results\n"
      ],
      "metadata": {
        "id": "tVDQr26DCVDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Estimate MS-VAR for All Country-System Combinations\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Estimate MS-VAR for All Country-System Combinations\n",
        "# ==============================================================================\n",
        "\n",
        "class EstimationError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during MS-VAR estimation.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a high-level exception identifier for errors encountered\n",
        "        during the execution of the `fit` method or the broader estimation pipeline\n",
        "        of the MS-VAR model. It acts as a wrapper or a distinct category for\n",
        "        failures that prevent the model from producing valid parameter estimates,\n",
        "        often aggregating more specific underlying errors (like InitializationError\n",
        "        or ConvergenceError) or handling structural data mismatches during estimation.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        cause of the estimation failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows users of the\n",
        "        library to catch all model estimation failures with a single `except`\n",
        "        block without catching unrelated system errors.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates an `EstimationError` instance up the call\n",
        "        stack, signaling that the model estimation has failed and no valid\n",
        "        model attributes have been set.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 2: Execute EM estimation for a single model.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_single_model(\n",
        "    data: np.ndarray,\n",
        "    initial_params: Dict[str, np.ndarray],\n",
        "    masks: Dict[int, np.ndarray],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the EM algorithm for a single MS-VAR model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        (T, 2) data array.\n",
        "    initial_params : Dict[str, np.ndarray]\n",
        "        Initial parameters 'A', 'Sigma', 'P'.\n",
        "    masks : Dict[int, np.ndarray]\n",
        "        Structural masks for A.\n",
        "    config : Dict[str, Any]\n",
        "        Configuration dictionary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Estimation results including final parameters, log-likelihood, and regime probabilities.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    EstimationError\n",
        "        If estimation fails (divergence, numerical error).\n",
        "    \"\"\"\n",
        "    current_params = initial_params\n",
        "    current_loglik = -np.inf\n",
        "\n",
        "    # Retrieve max iterations\n",
        "    max_iters = config[\"model_physics\"][\"estimation_engine\"][\"convergence_criteria\"][\"max_iterations\"]\n",
        "\n",
        "    # Storage for history\n",
        "    loglik_history = []\n",
        "\n",
        "    for k in range(max_iters):\n",
        "        try:\n",
        "            # 1. E-Step: Hamilton Filter\n",
        "            # run_hamilton_filter is defined in Task 11 context\n",
        "            filter_res = run_hamilton_filter(data, current_params)\n",
        "            new_loglik = filter_res[\"log_likelihood\"]\n",
        "            loglik_history.append(new_loglik)\n",
        "\n",
        "            # 2. Check Convergence\n",
        "            # finalize_estimation is defined in Task 14 context\n",
        "            # We pass run_hamilton_filter as the likelihood evaluator for SE computation\n",
        "            converged, final_res = finalize_estimation(\n",
        "                current_params, new_loglik, current_loglik, k, config, data, run_hamilton_filter\n",
        "            )\n",
        "\n",
        "            if converged:\n",
        "                # Compute smoothed probabilities for final output\n",
        "                # run_kim_smoother is defined in Task 12 context\n",
        "                smoother_res = run_kim_smoother(filter_res, current_params)\n",
        "\n",
        "                final_res[\"smoothed_probs\"] = smoother_res[\"smoothed_probs\"]\n",
        "                final_res[\"filtered_probs\"] = filter_res[\"filtered_probs\"]\n",
        "                final_res[\"loglik_history\"] = loglik_history\n",
        "                return final_res\n",
        "\n",
        "            # 3. E-Step Continued: Kim Smoother\n",
        "            smoother_res = run_kim_smoother(filter_res, current_params)\n",
        "\n",
        "            # 4. M-Step: Parameter Updates\n",
        "            # perform_m_step is defined in Task 13 context\n",
        "            current_params = perform_m_step(data, smoother_res, masks)\n",
        "\n",
        "            # Update loglik for next iteration check\n",
        "            current_loglik = new_loglik\n",
        "\n",
        "        except Exception as e:\n",
        "            raise EstimationError(f\"EM failed at iteration {k}: {e}\")\n",
        "\n",
        "    # If loop finishes without convergence\n",
        "    raise EstimationError(f\"EM did not converge within {max_iters} iterations.\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 15, Step 1 & 3: Construct grid, execute estimation, and organize results.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_all_msvar_models(\n",
        "    bivariate_systems: Dict[str, Dict[str, Any]],\n",
        "    initial_params_dict: Dict[Tuple[str, str], Dict[str, np.ndarray]],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[Tuple[str, str], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the estimation of all MS-VAR models defined in the study.\n",
        "\n",
        "    Iterates through every country and system, retrieves initial parameters,\n",
        "    and runs the EM algorithm. Collects and validates results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bivariate_systems : Dict[str, Dict[str, Any]]\n",
        "        The constructed bivariate systems.\n",
        "    initial_params_dict : Dict[Tuple[str, str], Dict[str, np.ndarray]]\n",
        "        Dictionary of initial parameters keyed by (country, system).\n",
        "    study_config : Dict[str, Any]\n",
        "        The study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[Tuple[str, str], Dict[str, Any]]\n",
        "        A dictionary mapping (country, system) to estimation results.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting batch MS-VAR estimation...\")\n",
        "\n",
        "    msvar_estimates = {}\n",
        "\n",
        "    # Load masks once\n",
        "    try:\n",
        "        mask1 = np.array(study_config[\"model_physics\"][\"regime_constraints\"][\"regime_1_mask\"], dtype=int)\n",
        "        mask2 = np.array(study_config[\"model_physics\"][\"regime_constraints\"][\"regime_2_mask\"], dtype=int)\n",
        "        masks = {0: mask1, 1: mask2}\n",
        "    except KeyError:\n",
        "        logger.error(\"Regime masks missing in config. Aborting estimation.\")\n",
        "        return {}\n",
        "\n",
        "    # Iterate over the grid\n",
        "    success_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for country, systems in bivariate_systems.items():\n",
        "        for system_name, sys_data in systems.items():\n",
        "            total_count += 1\n",
        "            model_key = (country, system_name)\n",
        "\n",
        "            # Check if initialization exists\n",
        "            if model_key not in initial_params_dict:\n",
        "                logger.warning(f\"Skipping {model_key}: No initial parameters found.\")\n",
        "                continue\n",
        "\n",
        "            data = sys_data[\"data\"]\n",
        "            init_params = initial_params_dict[model_key]\n",
        "\n",
        "            logger.info(f\"Estimating model: {country} - {system_name} (T={len(data)})\")\n",
        "\n",
        "            try:\n",
        "                # Execute Estimation\n",
        "                result = estimate_single_model(data, init_params, masks, study_config)\n",
        "\n",
        "                # Validate Result Structure\n",
        "                if \"params\" not in result or \"smoothed_probs\" not in result:\n",
        "                    raise EstimationError(\"Estimation returned incomplete results.\")\n",
        "\n",
        "                # Store Result\n",
        "                msvar_estimates[model_key] = result\n",
        "                success_count += 1\n",
        "                logger.info(f\"Successfully estimated {country} - {system_name}. LogLik: {result['final_loglik']:.2f}\")\n",
        "\n",
        "            except EstimationError as e:\n",
        "                logger.error(f\"Estimation failed for {country} - {system_name}: {e}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Unexpected error for {country} - {system_name}: {e}\")\n",
        "\n",
        "    logger.info(f\"Batch estimation completed. Success: {success_count}/{total_count}.\")\n",
        "    return msvar_estimates\n"
      ],
      "metadata": {
        "id": "43UJmlkEF2Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Compute Ljung–Box Residual Diagnostics\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Compute Ljung–Box Residual Diagnostics\n",
        "# ==============================================================================\n",
        "\n",
        "class DiagnosticError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during residual diagnostics.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for runtime errors\n",
        "        encountered during the validation of model adequacy. It flags failures\n",
        "        in calculating diagnostic statistics—such as the Ljung-Box test for\n",
        "        autocorrelation, the Jarque-Bera test for normality, or ARCH-LM tests\n",
        "        for conditional heteroskedasticity—applied to the estimated residuals\n",
        "        ($\\\\hat{u}_t$). This distinguishes computational failures in the diagnostic\n",
        "        module from core estimation errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        test failure (e.g., \"NaN values encountered in residuals during Ljung-Box test\").\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the pipeline to\n",
        "        isolate issues related to model validation steps, ensuring that a fitted\n",
        "        model is not discarded solely due to a secondary diagnostic calculation error.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `DiagnosticError` instance up the call stack,\n",
        "        signaling that the quality control checks on the model residuals could\n",
        "        not be completed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 1: Extract regime-weighted residuals.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_model_residuals(\n",
        "    data: np.ndarray,\n",
        "    params: Dict[str, np.ndarray],\n",
        "    smoothed_probs: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the residuals of the MS-VAR model based on the most probable regime.\n",
        "\n",
        "    For each time t, the residual is defined as:\n",
        "        u_t = y_t - A_{s_t} * y_{t-1}\n",
        "    where s_t = argmax_i P(s_t=i | Y_T).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        (T, 2) data array.\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Estimated parameters 'A'.\n",
        "    smoothed_probs : np.ndarray\n",
        "        (T, 2) smoothed probabilities.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        (T-1, 2) array of residuals (starting from t=1).\n",
        "    \"\"\"\n",
        "    T, K = data.shape\n",
        "    A = params[\"A\"]\n",
        "\n",
        "    # Determine most probable regime for each time t\n",
        "    # shape (T,)\n",
        "    most_probable_regime = np.argmax(smoothed_probs, axis=1)\n",
        "\n",
        "    # Prepare data\n",
        "    # Y = y_t (t=1..T-1)\n",
        "    # X = y_{t-1} (t=0..T-2)\n",
        "    Y = data[1:]\n",
        "    X = data[:-1]\n",
        "\n",
        "    # Regimes corresponding to Y (t=1..T-1)\n",
        "    regimes = most_probable_regime[1:]\n",
        "\n",
        "    residuals = np.zeros_like(Y)\n",
        "\n",
        "    # Compute residuals\n",
        "    # We can vectorize this by using fancy indexing if A was (T, K, K)\n",
        "    # Construct A_t sequence\n",
        "    A_sequence = A[regimes] # shape (T-1, K, K)\n",
        "\n",
        "    # Compute A_t @ x_t\n",
        "    # (T-1, K, K) @ (T-1, K, 1) -> (T-1, K, 1)\n",
        "    # We need to reshape X for matmul\n",
        "    X_reshaped = X[:, :, np.newaxis]\n",
        "    fitted = A_sequence @ X_reshaped\n",
        "    fitted = fitted.squeeze(-1)\n",
        "\n",
        "    residuals = Y - fitted\n",
        "\n",
        "    return residuals\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Step 2 & 3: Compute Ljung–Box Q statistic and evaluate.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_ljung_box_stats(\n",
        "    residuals: np.ndarray,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes Ljung-Box Q-statistics for the residuals of both equations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    residuals : np.ndarray\n",
        "        (T_res, 2) array of residuals.\n",
        "    config : Dict[str, Any]\n",
        "        Configuration containing 'ljung_box_test' thresholds.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary containing Q-stats, p-values, and decisions for 'GDP' and 'Financial'.\n",
        "    \"\"\"\n",
        "    # Retrieve settings\n",
        "    lb_config = config[\"model_physics\"][\"statistical_thresholds\"][\"ljung_box_test\"]\n",
        "    lags = lb_config[\"lags\"]\n",
        "    crit_val = lb_config[\"critical_value\"]\n",
        "\n",
        "    results = {}\n",
        "    equation_names = [\"GDP\", \"Financial\"]\n",
        "\n",
        "    for i, name in enumerate(equation_names):\n",
        "        series = residuals[:, i]\n",
        "\n",
        "        # Use statsmodels for robust calculation\n",
        "        # We want the stat at exactly 'lags'\n",
        "        try:\n",
        "            lb_df = acorr_ljungbox(series, lags=[lags], return_df=True)\n",
        "            q_stat = lb_df.iloc[0][\"lb_stat\"]\n",
        "            p_val = lb_df.iloc[0][\"lb_pvalue\"]\n",
        "\n",
        "            # Compute autocorrelations manually for reporting\n",
        "            # acf(k) = cov(x_t, x_{t-k}) / var(x_t)\n",
        "            mean = np.mean(series)\n",
        "            var = np.var(series)\n",
        "            n = len(series)\n",
        "            autocorr = []\n",
        "            for k in range(1, lags + 1):\n",
        "                cov = np.sum((series[k:] - mean) * (series[:-k] - mean)) / n\n",
        "                autocorr.append(cov / var)\n",
        "\n",
        "            decision = \"No Autocorrelation\" if q_stat < crit_val else \"Autocorrelation Detected\"\n",
        "\n",
        "            results[name] = {\n",
        "                \"q_stat\": float(q_stat),\n",
        "                \"p_value\": float(p_val),\n",
        "                \"critical_value\": crit_val,\n",
        "                \"decision\": decision,\n",
        "                \"autocorrelations\": autocorr,\n",
        "                \"is_clean\": bool(q_stat < crit_val)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            raise DiagnosticError(f\"Ljung-Box test failed for {name} equation: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_ljung_box_diagnostics(\n",
        "    msvar_estimates: Dict[Tuple[str, str], Dict[str, Any]],\n",
        "    bivariate_systems: Dict[str, Dict[str, Any]],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[Tuple[str, str], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates residual diagnostics for all estimated models.\n",
        "\n",
        "    Sequence:\n",
        "    1. Extract residuals based on most probable regime.\n",
        "    2. Compute Ljung-Box statistics.\n",
        "    3. Evaluate against critical values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    msvar_estimates : Dict\n",
        "        Output from Task 15.\n",
        "    bivariate_systems : Dict\n",
        "        Data dictionary.\n",
        "    study_config : Dict\n",
        "        Configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "        Diagnostic results keyed by (country, system).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Ljung-Box residual diagnostics...\")\n",
        "\n",
        "    diagnostic_results = {}\n",
        "\n",
        "    for key, estimate in msvar_estimates.items():\n",
        "        country, system_name = key\n",
        "\n",
        "        # Retrieve data\n",
        "        # We need the original data to compute residuals\n",
        "        # msvar_estimates doesn't store data, so we look it up\n",
        "        try:\n",
        "            data = bivariate_systems[country][system_name][\"data\"]\n",
        "        except KeyError:\n",
        "            logger.error(f\"Data not found for {key} during diagnostics.\")\n",
        "            continue\n",
        "\n",
        "        params = estimate[\"params\"]\n",
        "        smoothed_probs = estimate[\"smoothed_probs\"]\n",
        "\n",
        "        try:\n",
        "            # Step 1: Extract Residuals\n",
        "            residuals = extract_model_residuals(data, params, smoothed_probs)\n",
        "\n",
        "            # Step 2 & 3: Compute Stats\n",
        "            lb_stats = compute_ljung_box_stats(residuals, study_config)\n",
        "\n",
        "            diagnostic_results[key] = lb_stats\n",
        "\n",
        "            # Log summary\n",
        "            gdp_clean = lb_stats[\"GDP\"][\"is_clean\"]\n",
        "            fin_clean = lb_stats[\"Financial\"][\"is_clean\"]\n",
        "            status = \"Pass\" if (gdp_clean and fin_clean) else \"Fail\"\n",
        "            logger.info(f\"Diagnostics for {country} - {system_name}: {status}\")\n",
        "\n",
        "        except DiagnosticError as e:\n",
        "            logger.error(f\"Diagnostics failed for {key}: {e}\")\n",
        "\n",
        "    logger.info(\"Residual diagnostics completed.\")\n",
        "    return diagnostic_results\n"
      ],
      "metadata": {
        "id": "31IdSel0PKvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Compute Eigenvalues and Minsky Oscillation Conditions\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Compute Eigenvalues and Minsky Oscillation Conditions\n",
        "# ==============================================================================\n",
        "\n",
        "class MinskyAnalysisError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during Minsky condition analysis.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for logical or numerical\n",
        "        failures encountered when testing the Minsky instability hypothesis.\n",
        "        It flags issues such as the inability to calculate correlation regimes\n",
        "        between returns and volatility, or insufficient data points within a\n",
        "        specific regime to validate the transition from hedge/speculative finance\n",
        "        to Ponzi finance strategies (as defined in the context of the MS-VAR\n",
        "        analysis of financial stability).\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        analysis failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the analytical\n",
        "        modules to distinguish between core estimation errors and failures in\n",
        "        post-estimation economic interpretation steps.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `MinskyAnalysisError` instance up the call\n",
        "        stack, signaling that the specific economic hypothesis testing could\n",
        "        not be completed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax.\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 1 & 2: Compute eigenvalues and evaluate oscillation conditions.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def evaluate_eigenvalues_and_discriminant(A1: np.ndarray) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes eigenvalues and evaluates the oscillation discriminant for Regime 1.\n",
        "\n",
        "    Formulas:\n",
        "        Tr(A1) = alpha1 + beta2\n",
        "        Det(A1) = alpha1*beta2 - alpha2*beta1\n",
        "        Discriminant Delta' = (alpha1 - beta2)^2 + 4*alpha2*beta1\n",
        "        Eigenvalues = (Tr +/- sqrt(Tr^2 - 4*Det)) / 2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    A1 : np.ndarray\n",
        "        (2, 2) coefficient matrix for Regime 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Dictionary containing eigenvalues, discriminant, and boolean flags.\n",
        "    \"\"\"\n",
        "    alpha1 = A1[0, 0]\n",
        "    alpha2 = A1[0, 1]\n",
        "    beta1 = A1[1, 0]\n",
        "    beta2 = A1[1, 1]\n",
        "\n",
        "    # 1. Eigenvalues\n",
        "    # Use numpy for robust calculation handling complex numbers\n",
        "    eigenvalues = np.linalg.eigvals(A1)\n",
        "\n",
        "    # 2. Discriminant Delta'\n",
        "    # Condition for complex eigenvalues in the specific 2x2 form\n",
        "    # Note: The characteristic equation discriminant is Tr^2 - 4Det.\n",
        "    # The paper's Delta' = (alpha1 - beta2)^2 + 4*alpha2*beta1 is algebraically equivalent.\n",
        "    delta_prime = (alpha1 - beta2)**2 + 4 * alpha2 * beta1\n",
        "\n",
        "    # 3. Necessary Sign Condition\n",
        "    # alpha2 * beta1 < 0\n",
        "    cross_product = alpha2 * beta1\n",
        "\n",
        "    return {\n",
        "        \"eigenvalues\": eigenvalues,\n",
        "        \"delta_prime\": delta_prime,\n",
        "        \"cross_product\": cross_product,\n",
        "        \"is_oscillatory\": delta_prime < 0,\n",
        "        \"has_necessary_sign\": cross_product < 0\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Step 3: Classify Minsky regime status.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def classify_minsky_regime(\n",
        "    params: Dict[str, np.ndarray],\n",
        "    std_errors: Dict[str, np.ndarray]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Classifies whether Regime 1 qualifies as a 'Minsky Regime'.\n",
        "\n",
        "    Criteria:\n",
        "    1. Oscillatory dynamics (Delta' < 0).\n",
        "    2. Minsky sign pattern: beta1 > 0 (financial pro-cyclical), alpha2 < 0 (financial drag).\n",
        "    3. Statistical significance of beta1 and alpha2 (one-sided t-test, 5% level).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Estimated parameters.\n",
        "    std_errors : Dict[str, np.ndarray]\n",
        "        Standard errors.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        Classification results and details.\n",
        "    \"\"\"\n",
        "    A1 = params[\"A\"][0] # Regime 1\n",
        "    SE_A1 = std_errors[\"A\"][0]\n",
        "\n",
        "    # 1. Dynamics\n",
        "    dynamics = evaluate_eigenvalues_and_discriminant(A1)\n",
        "\n",
        "    # 2. Sign Pattern\n",
        "    beta1 = A1[1, 0]\n",
        "    alpha2 = A1[0, 1]\n",
        "\n",
        "    sign_pattern_valid = (beta1 > 0) and (alpha2 < 0)\n",
        "\n",
        "    # 3. Significance\n",
        "    # Critical value for one-sided 5% test (normal approx)\n",
        "    t_crit = 1.645\n",
        "\n",
        "    se_beta1 = SE_A1[1, 0]\n",
        "    se_alpha2 = SE_A1[0, 1]\n",
        "\n",
        "    # Handle missing SEs (NaN)\n",
        "    if np.isnan(se_beta1) or np.isnan(se_alpha2):\n",
        "        beta1_sig = False\n",
        "        alpha2_sig = False\n",
        "        significance_valid = False\n",
        "        logger.warning(\"Standard errors missing; assuming coefficients not significant.\")\n",
        "    else:\n",
        "        t_beta1 = beta1 / se_beta1\n",
        "        t_alpha2 = alpha2 / se_alpha2\n",
        "\n",
        "        # H1: beta1 > 0 => t > 1.645\n",
        "        beta1_sig = t_beta1 > t_crit\n",
        "        # H1: alpha2 < 0 => t < -1.645\n",
        "        alpha2_sig = t_alpha2 < -t_crit\n",
        "\n",
        "        significance_valid = beta1_sig and alpha2_sig\n",
        "\n",
        "    # Final Classification\n",
        "    is_minsky = (\n",
        "        dynamics[\"is_oscillatory\"] and\n",
        "        sign_pattern_valid and\n",
        "        significance_valid\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"is_minsky\": is_minsky,\n",
        "        \"dynamics\": dynamics,\n",
        "        \"signs\": {\n",
        "            \"beta1\": beta1,\n",
        "            \"alpha2\": alpha2,\n",
        "            \"pattern_valid\": sign_pattern_valid\n",
        "        },\n",
        "        \"significance\": {\n",
        "            \"beta1_sig\": beta1_sig,\n",
        "            \"alpha2_sig\": alpha2_sig,\n",
        "            \"valid\": significance_valid\n",
        "        }\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def analyze_minsky_conditions(\n",
        "    msvar_estimates: Dict[Tuple[str, str], Dict[str, Any]]\n",
        ") -> Dict[Tuple[str, str], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the Minsky condition analysis for all estimated models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    msvar_estimates : Dict\n",
        "        Output from Task 15.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "        Analysis results keyed by (country, system).\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Minsky condition analysis...\")\n",
        "\n",
        "    analysis_results = {}\n",
        "\n",
        "    for key, estimate in msvar_estimates.items():\n",
        "        country, system_name = key\n",
        "\n",
        "        try:\n",
        "            params = estimate[\"params\"]\n",
        "            std_errors = estimate[\"std_errors\"]\n",
        "\n",
        "            # Execute Classification\n",
        "            result = classify_minsky_regime(params, std_errors)\n",
        "\n",
        "            analysis_results[key] = result\n",
        "\n",
        "            # Log outcome\n",
        "            status = \"YES\" if result[\"is_minsky\"] else \"NO\"\n",
        "            logger.info(f\"Minsky Regime identified for {country} - {system_name}: {status}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed for {key}: {e}\")\n",
        "\n",
        "    logger.info(\"Minsky analysis completed.\")\n",
        "    return analysis_results\n"
      ],
      "metadata": {
        "id": "tQl1SDSrSt3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Extract and Visualize Regime Probabilities\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Extract and Visualize Regime Probabilities\n",
        "# ==============================================================================\n",
        "\n",
        "class VisualizationError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during visualization.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for runtime errors\n",
        "        encountered during the generation of graphical outputs (e.g., impulse\n",
        "        response functions, regime probability plots, or dynamic spillover networks).\n",
        "        It distinguishes failures in the presentation layer—such as dimensionality\n",
        "        mismatches in `matplotlib` calls, invalid file paths for saving figures,\n",
        "        or data formatting issues specific to plotting libraries—from core\n",
        "        analytical or estimation errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        plotting failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the execution\n",
        "        pipeline to potentially skip a failed plot and continue with subsequent\n",
        "        visualizations or reporting, rather than crashing the entire analysis\n",
        "        due to a display-related issue.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `VisualizationError` instance up the call\n",
        "        stack, signaling that a specific visual artifact could not be rendered\n",
        "        or saved.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 1 & 2: Retrieve probabilities and compute metrics.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_regime_metrics(\n",
        "    smoothed_probs: np.ndarray,\n",
        "    transition_matrix: np.ndarray,\n",
        "    index: pd.DatetimeIndex\n",
        ") -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Constructs a time series of regime probabilities and computes dominance metrics.\n",
        "\n",
        "    Metrics:\n",
        "    - Fraction of time in Regime 1 (Minsky).\n",
        "    - Expected duration of each regime: D_i = 1 / (1 - p_ii).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    smoothed_probs : np.ndarray\n",
        "        (T, 2) array of smoothed probabilities.\n",
        "    transition_matrix : np.ndarray\n",
        "        (2, 2) transition matrix P.\n",
        "    index : pd.DatetimeIndex\n",
        "        Time index for the data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[pd.DataFrame, Dict[str, float]]\n",
        "        - DataFrame with columns 'prob_regime1', 'prob_regime2', 'dominant_regime'.\n",
        "        - Dictionary of summary metrics.\n",
        "    \"\"\"\n",
        "    # 1. Construct DataFrame\n",
        "    df_probs = pd.DataFrame(\n",
        "        smoothed_probs,\n",
        "        index=index,\n",
        "        columns=[\"prob_regime1\", \"prob_regime2\"]\n",
        "    )\n",
        "\n",
        "    # Determine dominant regime (1-based index for reporting)\n",
        "    # argmax returns 0 or 1; we add 1 to get Regime 1 or 2\n",
        "    df_probs[\"dominant_regime\"] = np.argmax(smoothed_probs, axis=1) + 1\n",
        "\n",
        "    # 2. Compute Metrics\n",
        "    T = len(df_probs)\n",
        "    fraction_regime1 = (df_probs[\"dominant_regime\"] == 1).sum() / T\n",
        "\n",
        "    # Expected durations\n",
        "    p11 = transition_matrix[0, 0]\n",
        "    p22 = transition_matrix[1, 1]\n",
        "\n",
        "    # Handle absorbing states (p_ii = 1) to avoid division by zero\n",
        "    dur1 = 1.0 / (1.0 - p11) if p11 < 1.0 else np.inf\n",
        "    dur2 = 1.0 / (1.0 - p22) if p22 < 1.0 else np.inf\n",
        "\n",
        "    metrics = {\n",
        "        \"fraction_regime1\": fraction_regime1,\n",
        "        \"expected_duration_regime1\": dur1,\n",
        "        \"expected_duration_regime2\": dur2,\n",
        "        \"persistence_p11\": p11,\n",
        "        \"persistence_p22\": p22\n",
        "    }\n",
        "\n",
        "    return df_probs, metrics\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Step 3: Define visualization specifications.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def plot_regime_probabilities(\n",
        "    df_probs: pd.DataFrame,\n",
        "    country: str,\n",
        "    system_name: str,\n",
        "    save_path: str = None\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generates and saves a plot of regime probabilities.\n",
        "\n",
        "    Visuals:\n",
        "    - Solid line: Regime 1 (Minsky).\n",
        "    - Dashed line: Regime 2 (No Minsky).\n",
        "    - Vertical lines: 1973 (Oil), 2001 (Dot-com), 2008 (GFC).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_probs : pd.DataFrame\n",
        "        DataFrame containing probabilities.\n",
        "    country : str\n",
        "        Country name.\n",
        "    system_name : str\n",
        "        System identifier.\n",
        "    save_path : str, optional\n",
        "        Path to save the figure. If None, plot is not saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        # Plot probabilities\n",
        "        ax.plot(df_probs.index, df_probs[\"prob_regime1\"], label=\"Minsky Regime (Regime 1)\", color=\"black\", linestyle=\"-\")\n",
        "        ax.plot(df_probs.index, df_probs[\"prob_regime2\"], label=\"No Minsky Regime (Regime 2)\", color=\"gray\", linestyle=\"--\")\n",
        "\n",
        "        # Add crisis markers\n",
        "        crisis_years = [1973, 2001, 2008]\n",
        "        for year in crisis_years:\n",
        "            # Convert year to timestamp approx\n",
        "            date = pd.Timestamp(f\"{year}-01-01\")\n",
        "            if date >= df_probs.index.min() and date <= df_probs.index.max():\n",
        "                ax.axvline(x=date, color=\"red\", linestyle=\":\", alpha=0.6, label=f\"Crisis {year}\" if year == 1973 else \"\")\n",
        "\n",
        "        # Formatting\n",
        "        ax.set_title(f\"Regime Probabilities: {country} - {system_name}\")\n",
        "        ax.set_ylabel(\"Probability\")\n",
        "        ax.set_xlabel(\"Year\")\n",
        "        ax.set_ylim(-0.05, 1.05)\n",
        "        ax.legend(loc=\"best\")\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "            logger.info(f\"Saved plot to {save_path}\")\n",
        "\n",
        "        plt.close(fig)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise VisualizationError(f\"Plotting failed for {country} - {system_name}: {e}\")\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def extract_regime_probabilities(\n",
        "    msvar_estimates: Dict[Tuple[str, str], Dict[str, Any]],\n",
        "    bivariate_systems: Dict[str, Dict[str, Any]]\n",
        ") -> Dict[Tuple[str, str], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the extraction, analysis, and visualization of regime probabilities.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    msvar_estimates : Dict\n",
        "        Output from Task 15.\n",
        "    bivariate_systems : Dict\n",
        "        Data dictionary (for indices).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "        Results keyed by (country, system) containing the probability DataFrame and metrics.\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting regime probability analysis...\")\n",
        "\n",
        "    regime_results = {}\n",
        "\n",
        "    for key, estimate in msvar_estimates.items():\n",
        "        country, system_name = key\n",
        "\n",
        "        try:\n",
        "            # Retrieve data index\n",
        "            index = bivariate_systems[country][system_name][\"index\"]\n",
        "\n",
        "            # Retrieve estimates\n",
        "            smoothed_probs = estimate[\"smoothed_probs\"]\n",
        "            P = estimate[\"params\"][\"P\"]\n",
        "\n",
        "            # Step 1 & 2: Compute Metrics\n",
        "            df_probs, metrics = compute_regime_metrics(smoothed_probs, P, index)\n",
        "\n",
        "            # Step 3: Visualize (Optional: set save_path to actually save)\n",
        "            # We define the function but don't call it to avoid filesystem I/O in this environment\n",
        "            # plot_regime_probabilities(df_probs, country, system_name)\n",
        "\n",
        "            regime_results[key] = {\n",
        "                \"probabilities\": df_probs,\n",
        "                \"metrics\": metrics\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Analyzed regimes for {country} - {system_name}. Regime 1 Fraction: {metrics['fraction_regime1']:.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Regime analysis failed for {key}: {e}\")\n",
        "\n",
        "    logger.info(\"Regime analysis completed.\")\n",
        "    return regime_results\n"
      ],
      "metadata": {
        "id": "0QHPsqrZiMs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Design End-to-End Orchestrator Function\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Design End-to-End Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "class PipelineError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for critical pipeline failures.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as the terminal exception identifier for the MS-VAR\n",
        "        analytical pipeline. It is raised when an unrecoverable error occurs that\n",
        "        cannot be isolated to a specific module (like filtering or optimization)\n",
        "        but rather indicates a structural breakdown of the workflow (e.g.,\n",
        "        dependency failures, input/output corruption, or critical configuration\n",
        "        inconsistencies).\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the catastrophic\n",
        "        failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy to signal the need for an\n",
        "        immediate, hard stop of the execution script.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `PipelineError` instance up the call\n",
        "        stack, triggering the final error logging and cleanup routines.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 19, Step 1, 2 & 3: Define orchestrator interface, sequence, and controls.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_minsky_msvar_pipeline(\n",
        "    study_config: Dict[str, Any],\n",
        "    raw_datasets: Dict[str, pd.DataFrame]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the complete MS-VAR estimation and analysis pipeline.\n",
        "\n",
        "    This function executes the research workflow from raw data validation to\n",
        "    final regime analysis, ensuring strict adherence to the defined methodology.\n",
        "\n",
        "    Sequence:\n",
        "    1.  Validation (Config & Data)\n",
        "    2.  Data Engineering (Cleansing, Log Transform, HP Filter)\n",
        "    3.  Econometric Setup (Bivariate Systems, ADF Tests)\n",
        "    4.  Estimation (Initialization, EM Algorithm)\n",
        "    5.  Diagnostics (Residuals, Minsky Conditions)\n",
        "    6.  Analysis (Regime Probabilities)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    study_config : Dict[str, Any]\n",
        "        The master configuration dictionary.\n",
        "    raw_datasets : Dict[str, pd.DataFrame]\n",
        "        The raw country datasets.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A comprehensive results dictionary containing all intermediate and final outputs.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    PipelineError\n",
        "        If a critical stage of the pipeline fails.\n",
        "    \"\"\"\n",
        "    start_time = datetime.now()\n",
        "    logger.info(\"Starting Minsky MS-VAR Research Pipeline...\")\n",
        "\n",
        "    # Reproducibility\n",
        "    np.random.seed(42) # Fixed seed for any stochastic elements (e.g. initialization perturbation)\n",
        "\n",
        "    results = {\n",
        "        \"config\": study_config,\n",
        "        \"timestamp\": start_time.isoformat()\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # --- Phase 1: Validation ---\n",
        "        logger.info(\"Phase 1: Validation\")\n",
        "        # Task 1: Validate Config\n",
        "        validate_study_config(study_config)\n",
        "\n",
        "        # Task 2: Validate Raw Data\n",
        "        validate_raw_datasets(raw_datasets)\n",
        "\n",
        "        # --- Phase 2: Data Engineering ---\n",
        "        logger.info(\"Phase 2: Data Engineering\")\n",
        "        # Task 3: Cleansing\n",
        "        cleansing_output = cleanse_and_align_datasets(raw_datasets)\n",
        "        cleansed_data = cleansing_output[\"cleansed_datasets\"]\n",
        "        results[\"data_summary\"] = cleansing_output[\"summary_statistics\"]\n",
        "\n",
        "        # Task 4: Transformation\n",
        "        logged_data, transform_meta = apply_log_transform(cleansed_data)\n",
        "        results[\"transformation_metadata\"] = transform_meta\n",
        "\n",
        "        # Task 5: Signal Extraction\n",
        "        cycles_data = extract_hp_cycles(logged_data, study_config)\n",
        "\n",
        "        # --- Phase 3: Econometric Setup ---\n",
        "        logger.info(\"Phase 3: Econometric Setup\")\n",
        "        # Task 6: Bivariate Systems\n",
        "        bivariate_systems = construct_bivariate_systems(cycles_data)\n",
        "\n",
        "        # Task 7 & 8: Stationarity Tests\n",
        "        adf_summary = execute_all_adf_tests(bivariate_systems, study_config)\n",
        "        results[\"adf_tests\"] = adf_summary\n",
        "\n",
        "        # Check stationarity critical failure\n",
        "        # We proceed even if some fail, but log it heavily (as per Task 8 logic)\n",
        "        if not adf_summary[\"all_stationary\"]:\n",
        "            logger.warning(\"Some series failed stationarity tests. Proceeding with caution.\")\n",
        "\n",
        "        # --- Phase 4: Estimation ---\n",
        "        logger.info(\"Phase 4: Estimation\")\n",
        "        # Task 10: Initialization\n",
        "        initial_params = initialize_em_parameters(bivariate_systems)\n",
        "\n",
        "        # Task 15: Batch Estimation (includes Tasks 11-14 internally)\n",
        "        msvar_estimates = estimate_all_msvar_models(bivariate_systems, initial_params, study_config)\n",
        "        results[\"msvar_estimates\"] = msvar_estimates\n",
        "\n",
        "        if not msvar_estimates:\n",
        "            raise PipelineError(\"No models were successfully estimated.\")\n",
        "\n",
        "        # --- Phase 5: Diagnostics & Analysis ---\n",
        "        logger.info(\"Phase 5: Diagnostics & Analysis\")\n",
        "        # Task 16: Residual Diagnostics\n",
        "        lb_diagnostics = compute_ljung_box_diagnostics(msvar_estimates, bivariate_systems, study_config)\n",
        "        results[\"ljung_box_tests\"] = lb_diagnostics\n",
        "\n",
        "        # Task 17: Minsky Conditions\n",
        "        minsky_analysis = analyze_minsky_conditions(msvar_estimates)\n",
        "        results[\"minsky_analysis\"] = minsky_analysis\n",
        "\n",
        "        # Task 18: Regime Probabilities\n",
        "        regime_probs = extract_regime_probabilities(msvar_estimates, bivariate_systems)\n",
        "        results[\"regime_probabilities\"] = regime_probs\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - start_time\n",
        "        logger.info(f\"Pipeline completed successfully in {duration}.\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Pipeline failed: {e}\")\n",
        "        raise PipelineError(f\"Critical failure in pipeline execution: {e}\")\n"
      ],
      "metadata": {
        "id": "8xg-Lj5SjG21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20: Design Monte Carlo Robustness Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Design Monte Carlo Robustness Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "class MonteCarloError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during Monte Carlo simulation.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for runtime errors\n",
        "        encountered during stochastic simulation processes, such as the generation\n",
        "        of Generalized Impulse Response Functions (GIRFs) or the bootstrapping of\n",
        "        standard errors. It isolates failures in the Data Generating Process (DGP)\n",
        "        reconstruction or random number generation (e.g., Cholesky decomposition\n",
        "        failures on simulated covariance matrices) from deterministic estimation errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        simulation failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy to facilitate targeted error\n",
        "        handling during computationally intensive resampling steps.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `MonteCarloError` instance up the call\n",
        "        stack, signaling that the stochastic component of the analysis has failed\n",
        "        and associated confidence intervals or impulse responses are invalid.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 20, Step 1, 2 & 3: Orchestrate MC simulations and aggregation.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_monte_carlo_robustness(\n",
        "    msvar_estimates: Dict[Tuple[str, str], Dict[str, Any]],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[Tuple[str, str], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Executes parametric bootstrap Monte Carlo simulations to assess parameter stability.\n",
        "\n",
        "    For each estimated model:\n",
        "    1. Simulates N synthetic datasets using the estimated parameters.\n",
        "    2. Re-estimates the MS-VAR model on each synthetic dataset.\n",
        "    3. Aggregates parameter estimates to compute means, standard deviations, and confidence intervals.\n",
        "    4. Assesses the robustness of the Minsky classification (sign patterns and oscillation).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    msvar_estimates : Dict\n",
        "        The dictionary of estimated models from Task 15.\n",
        "    study_config : Dict\n",
        "        The study configuration containing MC settings.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "        A dictionary keyed by (country, system) containing MC statistics.\n",
        "    \"\"\"\n",
        "    # Check if enabled\n",
        "    mc_config = study_config[\"model_physics\"][\"monte_carlo_simulation\"]\n",
        "    if not mc_config[\"enabled\"]:\n",
        "        logger.info(\"Monte Carlo simulation disabled in configuration.\")\n",
        "        return {}\n",
        "\n",
        "    iterations = mc_config[\"iterations\"]\n",
        "    logger.info(f\"Starting Monte Carlo robustness analysis ({iterations} iterations per model)...\")\n",
        "\n",
        "    mc_results = {}\n",
        "\n",
        "    # Load masks once\n",
        "    try:\n",
        "        mask1 = np.array(study_config[\"model_physics\"][\"regime_constraints\"][\"regime_1_mask\"], dtype=int)\n",
        "        mask2 = np.array(study_config[\"model_physics\"][\"regime_constraints\"][\"regime_2_mask\"], dtype=int)\n",
        "        masks = {0: mask1, 1: mask2}\n",
        "    except KeyError:\n",
        "        raise MonteCarloError(\"Regime masks missing in config.\")\n",
        "\n",
        "    for key, estimate in msvar_estimates.items():\n",
        "        country, system_name = key\n",
        "        logger.info(f\"Running MC for {country} - {system_name}\")\n",
        "\n",
        "        # Retrieve estimated parameters (True parameters for DGP)\n",
        "        true_params = estimate[\"params\"]\n",
        "        T = len(estimate[\"smoothed_probs\"]) # Sample size\n",
        "\n",
        "        # Storage for replications\n",
        "        replications = []\n",
        "\n",
        "        success_count = 0\n",
        "\n",
        "        for r in range(iterations):\n",
        "            try:\n",
        "                # 1. Simulate Data (Task 21)\n",
        "                # simulate_msvar_path is defined in Task 21 context\n",
        "                sim_data, _ = simulate_msvar_path(T, true_params)\n",
        "\n",
        "                # 2. Re-estimate (Task 15 logic)\n",
        "                # We use the true parameters as initialization to speed up convergence\n",
        "                # and avoid label switching issues common in mixture models.\n",
        "                # estimate_single_model is defined in Task 15 context\n",
        "                sim_result = estimate_single_model(sim_data, true_params, masks, study_config)\n",
        "\n",
        "                replications.append(sim_result[\"params\"])\n",
        "                success_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                # Log failure but continue\n",
        "                # logger.debug(f\"MC replication {r} failed: {e}\")\n",
        "                pass\n",
        "\n",
        "        if success_count < iterations * 0.5:\n",
        "            logger.warning(f\"High failure rate in MC for {key}: {success_count}/{iterations} successful.\")\n",
        "\n",
        "        # 3. Aggregate Results (Task 22)\n",
        "        # aggregate_mc_results is defined in Task 22 context\n",
        "        if success_count > 0:\n",
        "            stats = aggregate_mc_results(replications)\n",
        "            mc_results[key] = stats\n",
        "        else:\n",
        "            logger.error(f\"All MC replications failed for {key}.\")\n",
        "\n",
        "    logger.info(\"Monte Carlo analysis completed.\")\n",
        "    return mc_results\n"
      ],
      "metadata": {
        "id": "YARRMg7uotPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21: Implement Monte Carlo Data Generation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Implement Monte Carlo Data Generation\n",
        "# ==============================================================================\n",
        "\n",
        "class SimulationError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during data simulation.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for runtime errors\n",
        "        encountered when generating synthetic data from the estimated MS-VAR\n",
        "        model parameters (e.g., for validating model recovery or stress testing).\n",
        "        It distinguishes failures in the Data Generating Process (DGP) logic—such\n",
        "        as unstable regime transitions or explosive autoregressive processes\n",
        "        in the simulated paths—from standard estimation errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        simulation constraint violation.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy to facilitate targeted error\n",
        "        handling during model validation and counterfactual analysis steps.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `SimulationError` instance up the call\n",
        "        stack, signaling that the generation of the synthetic trajectory has\n",
        "        failed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Define regime simulation from estimated transition matrix.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def simulate_regime_path(T: int, P: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Simulates a sequence of regimes based on the Markov transition matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    T : int\n",
        "        Length of the sequence.\n",
        "    P : np.ndarray\n",
        "        (2, 2) transition matrix.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        (T,) array of integer regimes (0 or 1).\n",
        "    \"\"\"\n",
        "    # 1. Compute Stationary Distribution\n",
        "    # Solve pi = pi * P => (I - P.T) * pi = 0\n",
        "    eigvals, eigvecs = np.linalg.eig(P.T)\n",
        "    idx = np.argmin(np.abs(eigvals - 1.0))\n",
        "    pi = np.real(eigvecs[:, idx])\n",
        "    pi /= np.sum(pi)\n",
        "\n",
        "    # 2. Simulate Path\n",
        "    regimes = np.zeros(T, dtype=int)\n",
        "\n",
        "    # Initial state\n",
        "    regimes[0] = np.random.choice([0, 1], p=pi)\n",
        "\n",
        "    # Subsequent states\n",
        "    for t in range(1, T):\n",
        "        prev = regimes[t-1]\n",
        "        probs = P[prev]\n",
        "        # Ensure probs sum to 1 (handle float errors)\n",
        "        probs = probs / np.sum(probs)\n",
        "        regimes[t] = np.random.choice([0, 1], p=probs)\n",
        "\n",
        "    return regimes\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 2: Simulate observables from regime-dependent VAR.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_var_observables(\n",
        "    regimes: np.ndarray,\n",
        "    params: Dict[str, np.ndarray]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Simulates bivariate time series data given a regime sequence and VAR parameters.\n",
        "\n",
        "    Model: y_t = A_{s_t} * y_{t-1} + epsilon_t\n",
        "           epsilon_t ~ N(0, Sigma_{s_t})\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    regimes : np.ndarray\n",
        "        (T,) array of regimes.\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Dictionary containing 'A' and 'Sigma'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        (T, 2) array of simulated data.\n",
        "    \"\"\"\n",
        "    T = len(regimes)\n",
        "    K = 2\n",
        "    A = params[\"A\"]\n",
        "    Sigma = params[\"Sigma\"]\n",
        "\n",
        "    data = np.zeros((T, K))\n",
        "\n",
        "    # Initialize y_0 at the unconditional mean (0 for cycles)\n",
        "    # Or draw from stationary distribution of first regime\n",
        "    # For simplicity and robustness, we start at 0.\n",
        "    data[0] = np.zeros(K)\n",
        "\n",
        "    for t in range(1, T):\n",
        "        s_t = regimes[t]\n",
        "\n",
        "        # Draw innovation\n",
        "        # Check PD of Sigma before drawing? Assumed valid from estimation.\n",
        "        try:\n",
        "            epsilon = np.random.multivariate_normal(np.zeros(K), Sigma[s_t])\n",
        "        except np.linalg.LinAlgError:\n",
        "            # Fallback for near-singular matrices in simulation\n",
        "            # Add small ridge\n",
        "            Sigma_reg = Sigma[s_t] + 1e-6 * np.eye(K)\n",
        "            epsilon = np.random.multivariate_normal(np.zeros(K), Sigma_reg)\n",
        "\n",
        "        # Autoregressive update\n",
        "        # y_t = A[s_t] @ y_{t-1} + epsilon\n",
        "        data[t] = A[s_t] @ data[t-1] + epsilon\n",
        "\n",
        "    return data\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Step 3: Verify simulated data properties.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def verify_simulation(data: np.ndarray) -> bool:\n",
        "    \"\"\"\n",
        "    Checks simulated data for validity (no explosion, no NaNs).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        Simulated data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if valid.\n",
        "    \"\"\"\n",
        "    if not np.isfinite(data).all():\n",
        "        return False\n",
        "\n",
        "    # Check for explosion\n",
        "    # If values exceed a reasonable threshold (e.g., 100 standard deviations of typical cycles)\n",
        "    # Typical cycle std is ~0.02 (log GDP). 100 * 0.02 = 2.0.\n",
        "    # Let's set a loose bound of 10.0 for log-levels/ratios to catch true divergence.\n",
        "    if np.max(np.abs(data)) > 10.0:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def simulate_msvar_path(\n",
        "    T: int,\n",
        "    params: Dict[str, np.ndarray],\n",
        "    max_retries: int = 5\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the simulation of a synthetic MS-VAR path.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    T : int\n",
        "        Length of simulation.\n",
        "    params : Dict[str, np.ndarray]\n",
        "        Model parameters.\n",
        "    max_retries : int\n",
        "        Number of attempts to generate a stable path.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[np.ndarray, np.ndarray]\n",
        "        - Simulated data (T, 2).\n",
        "        - Regime sequence (T,).\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    SimulationError\n",
        "        If valid data cannot be generated after max_retries.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # 1. Regimes\n",
        "            regimes = simulate_regime_path(T, params[\"P\"])\n",
        "\n",
        "            # 2. Observables\n",
        "            data = generate_var_observables(regimes, params)\n",
        "\n",
        "            # 3. Verify\n",
        "            if verify_simulation(data):\n",
        "                return data, regimes\n",
        "\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    raise SimulationError(f\"Failed to generate stable simulation after {max_retries} attempts.\")\n"
      ],
      "metadata": {
        "id": "QmJ7s4iAsLMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22: Aggregate Monte Carlo Results\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Aggregate Monte Carlo Results\n",
        "# ==============================================================================\n",
        "\n",
        "class AggregationError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during MC aggregation.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for runtime errors\n",
        "        encountered when compiling, summarizing, or reducing the results of\n",
        "        numerous Monte Carlo iterations (e.g., calculating Generalized Impulse\n",
        "        Response Functions or confidence intervals). It flags issues such as\n",
        "        dimensionality mismatches when stacking result arrays, memory exhaustion\n",
        "        during large-scale tensor operations, or failures in quantile calculations\n",
        "        due to insufficient valid simulation draws.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        aggregation failure (e.g., \"Shape mismatch during GIRF stacking\").\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the simulation\n",
        "        controller to distinguish between errors in individual simulation runs\n",
        "        and critical errors in the final synthesis of the distribution.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates an `AggregationError` instance up the call\n",
        "        stack, signaling that the statistical consolidation of the simulation\n",
        "        outputs has failed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 1: Compute parameter summary statistics.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_parameter_stats(replications: List[Dict[str, np.ndarray]]) -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Computes mean, standard deviation, and confidence intervals for all parameters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    replications : List[Dict[str, np.ndarray]]\n",
        "        List of parameter dictionaries from MC simulations.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Dict[str, np.ndarray]]\n",
        "        Nested dictionary: param_name -> {'mean': ..., 'std': ..., 'ci_lower': ..., 'ci_upper': ...}\n",
        "    \"\"\"\n",
        "    if not replications:\n",
        "        raise AggregationError(\"No replications to aggregate.\")\n",
        "\n",
        "    # Stack parameters\n",
        "    # keys: 'A', 'Sigma', 'P'\n",
        "    keys = replications[0].keys()\n",
        "    stacked = {k: [] for k in keys}\n",
        "\n",
        "    for rep in replications:\n",
        "        for k in keys:\n",
        "            stacked[k].append(rep[k])\n",
        "\n",
        "    stats = {}\n",
        "    for k in keys:\n",
        "        # Shape: (N_reps, ...)\n",
        "        arr = np.array(stacked[k])\n",
        "\n",
        "        stats[k] = {\n",
        "            \"mean\": np.mean(arr, axis=0),\n",
        "            \"std\": np.std(arr, axis=0, ddof=1),\n",
        "            \"ci_lower\": np.percentile(arr, 2.5, axis=0),\n",
        "            \"ci_upper\": np.percentile(arr, 97.5, axis=0)\n",
        "        }\n",
        "\n",
        "    return stats\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 2: Assess Minsky condition robustness.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def assess_minsky_robustness(replications: List[Dict[str, np.ndarray]]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes the fraction of MC replications that satisfy Minsky conditions.\n",
        "\n",
        "    Conditions:\n",
        "    1. Sign Pattern: beta1 > 0 AND alpha2 < 0\n",
        "    2. Oscillation: Delta' < 0\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    replications : List[Dict[str, np.ndarray]]\n",
        "        List of parameter dictionaries.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, float]\n",
        "        - 'frac_sign': Fraction satisfying sign pattern.\n",
        "        - 'frac_osc': Fraction satisfying oscillation condition.\n",
        "        - 'frac_robust': Fraction satisfying both.\n",
        "    \"\"\"\n",
        "    count_sign = 0\n",
        "    count_osc = 0\n",
        "    count_both = 0\n",
        "    n = len(replications)\n",
        "\n",
        "    for rep in replications:\n",
        "        # Extract Regime 1 parameters\n",
        "        A1 = rep[\"A\"][0]\n",
        "        alpha1 = A1[0, 0]\n",
        "        alpha2 = A1[0, 1]\n",
        "        beta1 = A1[1, 0]\n",
        "        beta2 = A1[1, 1]\n",
        "\n",
        "        # Check signs\n",
        "        sign_valid = (beta1 > 0) and (alpha2 < 0)\n",
        "\n",
        "        # Check oscillation\n",
        "        delta_prime = (alpha1 - beta2)**2 + 4 * alpha2 * beta1\n",
        "        osc_valid = delta_prime < 0\n",
        "\n",
        "        if sign_valid:\n",
        "            count_sign += 1\n",
        "        if osc_valid:\n",
        "            count_osc += 1\n",
        "        if sign_valid and osc_valid:\n",
        "            count_both += 1\n",
        "\n",
        "    return {\n",
        "        \"frac_sign\": count_sign / n,\n",
        "        \"frac_osc\": count_osc / n,\n",
        "        \"frac_robust\": count_both / n\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Step 3: Organize results in replication tables.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def create_summary_table(stats: Dict[str, Dict[str, np.ndarray]]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Flattens parameter statistics into a readable DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stats : Dict\n",
        "        Output from compute_parameter_stats.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Table with columns: Parameter, Mean, Std, 95% CI Lower, 95% CI Upper.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    # Helper to add rows\n",
        "    def add_rows(param_name, stat_dict):\n",
        "        mean = stat_dict[\"mean\"]\n",
        "        std = stat_dict[\"std\"]\n",
        "        lower = stat_dict[\"ci_lower\"]\n",
        "        upper = stat_dict[\"ci_upper\"]\n",
        "\n",
        "        it = np.nditer(mean, flags=['multi_index'])\n",
        "        for _ in it:\n",
        "            idx = it.multi_index\n",
        "            rows.append({\n",
        "                \"Parameter\": f\"{param_name}{idx}\",\n",
        "                \"Mean\": mean[idx],\n",
        "                \"Std\": std[idx],\n",
        "                \"CI_Lower\": lower[idx],\n",
        "                \"CI_Upper\": upper[idx]\n",
        "            })\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        add_rows(k, v)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def aggregate_mc_results(replications: List[Dict[str, np.ndarray]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the aggregation of Monte Carlo simulation results.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    replications : List[Dict[str, np.ndarray]]\n",
        "        List of parameter dictionaries from MC runs.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        - 'parameter_stats': Nested dict of stats.\n",
        "        - 'robustness_metrics': Dict of Minsky robustness fractions.\n",
        "        - 'summary_table': DataFrame of parameter stats.\n",
        "    \"\"\"\n",
        "    if not replications:\n",
        "        return {}\n",
        "\n",
        "    # Step 1: Parameter Stats\n",
        "    param_stats = compute_parameter_stats(replications)\n",
        "\n",
        "    # Step 2: Robustness\n",
        "    robustness = assess_minsky_robustness(replications)\n",
        "\n",
        "    # Step 3: Table\n",
        "    table = create_summary_table(param_stats)\n",
        "\n",
        "    return {\n",
        "        \"parameter_stats\": param_stats,\n",
        "        \"robustness_metrics\": robustness,\n",
        "        \"summary_table\": table\n",
        "    }\n"
      ],
      "metadata": {
        "id": "5VekTrr1tMQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23: Cross-Validate Results Against Paper Tables\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Cross-Validate Results Against Paper Tables\n",
        "# ==============================================================================\n",
        "\n",
        "class ValidationReportError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during cross-validation.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for runtime errors\n",
        "        encountered during the model validation stage. It flags failures in\n",
        "        generating performance metrics (e.g., RMSE, MAE) across cross-validation\n",
        "        folds, or errors in data splitting logic (e.g., leakage between training\n",
        "        and testing sets) that compromise the integrity of the out-of-sample\n",
        "        evaluation.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        reporting failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the validation\n",
        "        framework to abort defective evaluation runs without crashing the\n",
        "        underlying estimation engine, ensuring that invalid performance reports\n",
        "        are not generated.\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `ValidationReportError` instance up the call\n",
        "        stack, signaling that the cross-validation or reporting routine has\n",
        "        failed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Any, Tuple, List\n",
        "import logging\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Cross-Validate Results Against Paper Tables\n",
        "# ==============================================================================\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 1: Extract paper benchmark values.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def load_paper_benchmarks() -> Dict[Tuple[str, str], Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Loads benchmark parameter estimates from the original paper's tables.\n",
        "\n",
        "    This function serves as the ground truth repository for cross-validation.\n",
        "    It contains hardcoded values extracted directly from the published results\n",
        "    (e.g., Table 1 for GDP/NFCD) to allow for automated comparison with the\n",
        "    replication's estimates.\n",
        "\n",
        "    Note: In a production environment, this data would typically be loaded from\n",
        "    an external CSV or JSON file to separate data from code.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[Tuple[str, str], Dict[str, float]]\n",
        "        A dictionary mapping a (country, system) tuple to a dictionary of\n",
        "        parameter values (e.g., 'alpha1_r1', 'beta1_r1', 'p11').\n",
        "    \"\"\"\n",
        "    # Values from Table 1: Estimation Results for GDP/NFCD\n",
        "    benchmarks = {\n",
        "        (\"USA\", \"GDP_NFCD\"): {\n",
        "            \"alpha1_r1\": 0.8215, \"alpha2_r1\": -0.1066,\n",
        "            \"beta1_r1\": 1.6348,  \"beta2_r1\": 0.4027,\n",
        "            \"psi1_r2\": 0.5389,   \"omega2_r2\": 0.9884,\n",
        "            \"p11\": 0.837,        \"p22\": 0.868\n",
        "        },\n",
        "        (\"UK\", \"GDP_NFCD\"): {\n",
        "            \"alpha1_r1\": 0.8541, \"alpha2_r1\": -0.0924,\n",
        "            \"beta1_r1\": 1.4605,  \"beta2_r1\": 0.7026,\n",
        "            \"psi1_r2\": 0.04429,  \"omega2_r2\": 0.5760,\n",
        "            \"p11\": 0.900,        \"p22\": 0.690\n",
        "        },\n",
        "        (\"Germany\", \"GDP_NFCD\"): {\n",
        "            \"alpha1_r1\": 0.5682, \"alpha2_r1\": -0.1550,\n",
        "            \"beta1_r1\": 0.59835, \"beta2_r1\": 0.8093,\n",
        "            \"psi1_r2\": 0.6848,   \"omega2_r2\": 0.5400,\n",
        "            \"p11\": 0.945,        \"p22\": 0.850\n",
        "        }\n",
        "        # Add other countries/systems as needed from paper tables\n",
        "    }\n",
        "    return benchmarks\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 2: Compute discrepancies and classify alignment.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compute_relative_errors(\n",
        "    estimates: Dict[str, Any],\n",
        "    benchmarks: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes relative errors between estimated parameters and paper benchmarks.\n",
        "\n",
        "    This function calculates the relative error (RE) for each parameter to quantify\n",
        "    the deviation of the replication results from the published values. It also\n",
        "    classifies the alignment quality based on predefined thresholds.\n",
        "\n",
        "    Formula:\n",
        "        RE = |estimate - benchmark| / (|benchmark| + epsilon)\n",
        "        where epsilon = 1e-10 to prevent division by zero.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimates : Dict[str, Any]\n",
        "        The dictionary of estimated model results, containing 'params' which holds\n",
        "        the 'A' and 'P' matrices.\n",
        "    benchmarks : Dict[str, float]\n",
        "        The dictionary of benchmark values for the specific model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing:\n",
        "        - 'relative_errors': Dict mapping parameter names to their RE values.\n",
        "        - 'classifications': Dict mapping parameter names to alignment categories\n",
        "          (Exact, Close, Moderate, Large).\n",
        "        - 'estimated_values': Dict of the extracted estimate values used for comparison.\n",
        "        - 'benchmark_values': The input benchmark values.\n",
        "    \"\"\"\n",
        "    params = estimates[\"params\"]\n",
        "    A = params[\"A\"]\n",
        "    P = params[\"P\"]\n",
        "\n",
        "    # Map internal array structure to benchmark keys\n",
        "    # Regime 1 (Index 0)\n",
        "    est_map = {\n",
        "        \"alpha1_r1\": A[0, 0, 0], \"alpha2_r1\": A[0, 0, 1],\n",
        "        \"beta1_r1\": A[0, 1, 0],  \"beta2_r1\": A[0, 1, 1],\n",
        "\n",
        "        # Regime 2 (Index 1) - Diagonal\n",
        "        \"psi1_r2\": A[1, 0, 0],   \"omega2_r2\": A[1, 1, 1],\n",
        "\n",
        "        # Transition Matrix\n",
        "        \"p11\": P[0, 0],          \"p22\": P[1, 1]\n",
        "    }\n",
        "\n",
        "    errors = {}\n",
        "    classifications = {}\n",
        "\n",
        "    # Extract benchmarks\n",
        "    for key, paper_val in benchmarks.items():\n",
        "        if key in est_map:\n",
        "            est_val = est_map[key]\n",
        "            re = abs(est_val - paper_val) / (abs(paper_val) + 1e-10)\n",
        "            errors[key] = re\n",
        "\n",
        "            if re < 0.01:\n",
        "                classifications[key] = \"Exact Match\"\n",
        "            elif re < 0.05:\n",
        "                classifications[key] = \"Close Match\"\n",
        "            elif re < 0.10:\n",
        "                classifications[key] = \"Moderate Discrepancy\"\n",
        "            else:\n",
        "                classifications[key] = \"Large Discrepancy\"\n",
        "\n",
        "    return {\n",
        "        \"relative_errors\": errors,\n",
        "        \"classifications\": classifications,\n",
        "        \"estimated_values\": est_map,\n",
        "        \"benchmark_values\": benchmarks\n",
        "    }\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Step 3: Document and diagnose discrepancies.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_validation_report(\n",
        "    comparison_results: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a text report summarizing the cross-validation results.\n",
        "\n",
        "    This function creates a human-readable summary of the alignment between the\n",
        "    replication and the benchmark. It aggregates the counts of alignment classifications\n",
        "    and explicitly lists any parameters with large discrepancies (>10% RE) to facilitate\n",
        "    diagnosis.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    comparison_results : Dict[str, Any]\n",
        "        The output dictionary from `compute_relative_errors`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        A formatted string containing the validation summary and details of discrepancies.\n",
        "    \"\"\"\n",
        "    report = []\n",
        "    report.append(\"--- Cross-Validation Report ---\")\n",
        "\n",
        "    classifications = comparison_results[\"classifications\"]\n",
        "    errors = comparison_results[\"relative_errors\"]\n",
        "\n",
        "    # Count discrepancies\n",
        "    counts = {\"Exact Match\": 0, \"Close Match\": 0, \"Moderate Discrepancy\": 0, \"Large Discrepancy\": 0}\n",
        "    for c in classifications.values():\n",
        "        counts[c] += 1\n",
        "\n",
        "    report.append(f\"Summary: {counts}\")\n",
        "\n",
        "    # List large discrepancies\n",
        "    large_discrepancies = {k: v for k, v in errors.items() if v >= 0.10}\n",
        "    if large_discrepancies:\n",
        "        report.append(\"\\nLarge Discrepancies (>10% RE):\")\n",
        "        for k, re in large_discrepancies.items():\n",
        "            est = comparison_results[\"estimated_values\"][k]\n",
        "            paper = comparison_results[\"benchmark_values\"][k]\n",
        "            report.append(f\"  {k}: Est={est:.4f}, Paper={paper:.4f}, RE={re:.4f}\")\n",
        "\n",
        "    return \"\\n\".join(report)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def cross_validate_results(\n",
        "    msvar_estimates: Dict[Tuple[str, str], Dict[str, Any]]\n",
        ") -> Dict[Tuple[str, str], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the cross-validation of results against paper benchmarks.\n",
        "\n",
        "    This function manages the validation workflow: loading benchmarks, computing\n",
        "    errors for each estimated model, generating reports, and logging the outcomes.\n",
        "    It ensures that the replication results are systematically compared against\n",
        "    the published findings.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    msvar_estimates : Dict[Tuple[str, str], Dict[str, Any]]\n",
        "        The dictionary of estimated MS-VAR models from Task 15.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[Tuple[str, str], Dict[str, Any]]\n",
        "        A dictionary of validation results keyed by (country, system), containing\n",
        "        metrics and the text report.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting cross-validation against paper benchmarks...\")\n",
        "\n",
        "    benchmarks = load_paper_benchmarks()\n",
        "    validation_results = {}\n",
        "\n",
        "    # Extract benchmarks\n",
        "    for key, benchmark_vals in benchmarks.items():\n",
        "        if key in msvar_estimates:\n",
        "            estimate = msvar_estimates[key]\n",
        "\n",
        "            # Compute Errors\n",
        "            comp_res = compute_relative_errors(estimate, benchmark_vals)\n",
        "\n",
        "            # Generate Report\n",
        "            report_text = generate_validation_report(comp_res)\n",
        "\n",
        "            validation_results[key] = {\n",
        "                \"metrics\": comp_res,\n",
        "                \"report\": report_text\n",
        "            }\n",
        "\n",
        "            logging.info(f\"Validated {key}. Report:\\n{report_text}\")\n",
        "        else:\n",
        "            logging.warning(f\"Benchmark available for {key} but no estimate found.\")\n",
        "\n",
        "    logging.info(\"Cross-validation completed.\")\n",
        "    return validation_results\n"
      ],
      "metadata": {
        "id": "TmWkC_DewhYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24: Final Synthesis and Documentation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Final Synthesis and Documentation\n",
        "# ==============================================================================\n",
        "\n",
        "class ReportingError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception for failures during report generation.\n",
        "\n",
        "    Purpose:\n",
        "        This class serves as a specific exception identifier for runtime errors\n",
        "        encountered when compiling the final analytical outputs. It distinguishes\n",
        "        failures in the presentation or I/O layer—such as permission errors when\n",
        "        saving LaTeX tables, formatting errors in CSV exports, or missing\n",
        "        dependencies for PDF generation—from the core mathematical estimation\n",
        "        or simulation errors.\n",
        "\n",
        "    Inputs:\n",
        "        No specific input parameters are enforced by this class definition; it\n",
        "        accepts the standard arguments passed to the base `Exception` class,\n",
        "        typically a descriptive error message string detailing the specific\n",
        "        reporting failure.\n",
        "\n",
        "    Processes:\n",
        "        The class inherits the standard behavior and attributes of the Python\n",
        "        `Exception` class. It does not implement custom logic but establishes\n",
        "        a unique type in the exception hierarchy. This allows the main execution\n",
        "        script to gracefully handle output failures (e.g., by logging the error\n",
        "        without losing the calculated results in memory).\n",
        "\n",
        "    Outputs:\n",
        "        When raised, it propagates a `ReportingError` instance up the call\n",
        "        stack, signaling that the creation of the final project artifacts has\n",
        "        failed.\n",
        "    \"\"\"\n",
        "    # Execute the pass statement to maintain valid class syntax\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 1: Compile comprehensive results structure.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def compile_master_results(\n",
        "    config: Dict[str, Any],\n",
        "    data_artifacts: Dict[str, Any],\n",
        "    estimates: Dict[str, Any],\n",
        "    diagnostics: Dict[str, Any],\n",
        "    analysis: Dict[str, Any],\n",
        "    mc_results: Dict[str, Any],\n",
        "    validation: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Aggregates all pipeline outputs into a single master results dictionary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict\n",
        "        Study configuration.\n",
        "    data_artifacts : Dict\n",
        "        Cleansed data, cycles, systems.\n",
        "    estimates : Dict\n",
        "        MS-VAR parameter estimates.\n",
        "    diagnostics : Dict\n",
        "        Residual diagnostics.\n",
        "    analysis : Dict\n",
        "        Minsky condition analysis and regime probabilities.\n",
        "    mc_results : Dict\n",
        "        Monte Carlo robustness stats.\n",
        "    validation : Dict\n",
        "        Cross-validation results.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "        The master results object.\n",
        "    \"\"\"\n",
        "    # Convert tuple keys to strings for JSON serialization compatibility if needed later\n",
        "    # (Tuple keys are valid in Python dicts but not JSON)\n",
        "    # We keep tuple keys for internal consistency, but a serializer would need to handle them.\n",
        "\n",
        "    master = {\n",
        "        \"metadata\": {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"description\": \"Final results for Minsky MS-VAR Replication\"\n",
        "        },\n",
        "        \"configuration\": config,\n",
        "        \"data\": data_artifacts,\n",
        "        \"estimation\": {\n",
        "            \"msvar_models\": estimates\n",
        "        },\n",
        "        \"diagnostics\": {\n",
        "            \"ljung_box\": diagnostics\n",
        "        },\n",
        "        \"analysis\": {\n",
        "            \"minsky_conditions\": analysis[\"minsky_conditions\"],\n",
        "            \"regime_probabilities\": analysis[\"regime_probabilities\"]\n",
        "        },\n",
        "        \"robustness\": {\n",
        "            \"monte_carlo\": mc_results\n",
        "        },\n",
        "        \"validation\": {\n",
        "            \"paper_comparison\": validation\n",
        "        }\n",
        "    }\n",
        "    return master\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 2: Generate publication-ready tables.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_latex_tables(master_results: Dict[str, Any]) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Generates LaTeX-formatted tables for parameters and diagnostics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    master_results : Dict\n",
        "        The master results object.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, str]\n",
        "        Dictionary mapping table names to LaTeX source strings.\n",
        "    \"\"\"\n",
        "    tables = {}\n",
        "\n",
        "    # 1. Parameter Estimates Table (Simplified view)\n",
        "    # We iterate over models and extract key parameters\n",
        "    rows = []\n",
        "    estimates = master_results[\"estimation\"][\"msvar_models\"]\n",
        "\n",
        "    for key, res in estimates.items():\n",
        "        country, system = key\n",
        "        params = res[\"params\"]\n",
        "        A = params[\"A\"]\n",
        "        P = params[\"P\"]\n",
        "\n",
        "        # Extract specific coefficients for reporting (Regime 1)\n",
        "        alpha1 = A[0, 0, 0]\n",
        "        alpha2 = A[0, 0, 1]\n",
        "        beta1 = A[0, 1, 0]\n",
        "        beta2 = A[0, 1, 1]\n",
        "        p11 = P[0, 0]\n",
        "        p22 = P[1, 1]\n",
        "\n",
        "        rows.append({\n",
        "            \"Country\": country,\n",
        "            \"System\": system,\n",
        "            \"$\\\\alpha_1$\": f\"{alpha1:.3f}\",\n",
        "            \"$\\\\alpha_2$\": f\"{alpha2:.3f}\",\n",
        "            \"$\\\\beta_1$\": f\"{beta1:.3f}\",\n",
        "            \"$\\\\beta_2$\": f\"{beta2:.3f}\",\n",
        "            \"$p_{11}$\": f\"{p11:.3f}\",\n",
        "            \"$p_{22}$\": f\"{p22:.3f}\"\n",
        "        })\n",
        "\n",
        "    df_params = pd.DataFrame(rows)\n",
        "    tables[\"parameter_estimates\"] = df_params.to_latex(index=False, escape=False)\n",
        "\n",
        "    # 2. Minsky Classification Table\n",
        "    minsky_res = master_results[\"analysis\"][\"minsky_conditions\"]\n",
        "    rows_minsky = []\n",
        "    for key, res in minsky_res.items():\n",
        "        country, system = key\n",
        "        rows_minsky.append({\n",
        "            \"Country\": country,\n",
        "            \"System\": system,\n",
        "            \"Oscillatory\": \"Yes\" if res[\"dynamics\"][\"is_oscillatory\"] else \"No\",\n",
        "            \"Sign Pattern\": \"Yes\" if res[\"signs\"][\"pattern_valid\"] else \"No\",\n",
        "            \"Significant\": \"Yes\" if res[\"significance\"][\"valid\"] else \"No\",\n",
        "            \"Is Minsky\": \"YES\" if res[\"is_minsky\"] else \"NO\"\n",
        "        })\n",
        "\n",
        "    df_minsky = pd.DataFrame(rows_minsky)\n",
        "    tables[\"minsky_classification\"] = df_minsky.to_latex(index=False)\n",
        "\n",
        "    return tables\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Step 3: Document implementation choices.\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def generate_technical_appendix(config: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Generates a text-based technical appendix documenting implementation details.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    config : Dict\n",
        "        Study configuration.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The technical appendix text.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    lines.append(\"# Technical Appendix: Minsky MS-VAR Replication\")\n",
        "    lines.append(f\"Generated: {datetime.now().isoformat()}\")\n",
        "    lines.append(\"\\n## 1. Data Processing\")\n",
        "    lines.append(\"- **Log Transformation**: Applied to Real GDP only.\")\n",
        "    lines.append(\"- **HP Filter**: Lambda = 100.0 (Annual data).\")\n",
        "    lines.append(\"- **Cleansing**: Linear interpolation for single-year gaps; truncation for multi-year gaps.\")\n",
        "\n",
        "    lines.append(\"\\n## 2. Model Specification\")\n",
        "    lines.append(\"- **MS-VAR(1)**: Zero intercept.\")\n",
        "    lines.append(\"- **Regime 1**: Unrestricted interaction.\")\n",
        "    lines.append(\"- **Regime 2**: Diagonal coefficient matrix (No interaction).\")\n",
        "    lines.append(\"- **Estimation**: EM Algorithm with Hamilton Filter and Kim Smoother.\")\n",
        "\n",
        "    lines.append(\"\\n## 3. Statistical Tests\")\n",
        "    lines.append(\"- **ADF Test**: Constant only (no trend). Critical value: -1.94.\")\n",
        "    lines.append(\"- **Ljung-Box**: Lags = 3. Critical value: 6.63.\")\n",
        "\n",
        "    lines.append(\"\\n## 4. Robustness\")\n",
        "    mc_conf = config[\"model_physics\"][\"monte_carlo_simulation\"]\n",
        "    lines.append(f\"- **Monte Carlo**: {mc_conf['iterations']} iterations, Parametric Bootstrap.\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def synthesize_final_report(\n",
        "    config: Dict[str, Any],\n",
        "    data_artifacts: Dict[str, Any],\n",
        "    estimates: Dict[str, Any],\n",
        "    diagnostics: Dict[str, Any],\n",
        "    analysis: Dict[str, Any],\n",
        "    mc_results: Dict[str, Any],\n",
        "    validation: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the final synthesis of the project.\n",
        "\n",
        "    Sequence:\n",
        "    1. Compile master results dictionary.\n",
        "    2. Generate LaTeX tables.\n",
        "    3. Generate technical documentation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    [All pipeline outputs]\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict\n",
        "        Contains 'master_results', 'latex_tables', 'technical_appendix'.\n",
        "    \"\"\"\n",
        "    logger.info(\"Synthesizing final report...\")\n",
        "\n",
        "    # 1. Master Results\n",
        "    master = compile_master_results(\n",
        "        config, data_artifacts, estimates, diagnostics, analysis, mc_results, validation\n",
        "    )\n",
        "\n",
        "    # 2. Tables\n",
        "    latex_tables = generate_latex_tables(master)\n",
        "\n",
        "    # 3. Documentation\n",
        "    appendix = generate_technical_appendix(config)\n",
        "\n",
        "    logger.info(\"Synthesis complete.\")\n",
        "\n",
        "    return {\n",
        "        \"master_results\": master,\n",
        "        \"latex_tables\": latex_tables,\n",
        "        \"technical_appendix\": appendix\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "aP0r0qywxbEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Top-Level Project Orchestration\n",
        "# ==============================================================================\n",
        "\n",
        "def execute_minsky_research_project(\n",
        "    study_config: Dict[str, Any],\n",
        "    raw_datasets: Dict[str, pd.DataFrame]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research project for \"Regime Changes and Real-Financial Cycles\".\n",
        "\n",
        "    This top-level orchestrator manages the execution of the four major research phases:\n",
        "    1.  **Main Estimation Pipeline (Task 19)**: Validation, Data Engineering, Estimation, Diagnostics, Analysis.\n",
        "    2.  **Robustness Analysis (Task 20)**: Monte Carlo simulations (invoking Tasks 21 & 22 internally).\n",
        "    3.  **Cross-Validation (Task 23)**: Comparison against published benchmarks.\n",
        "    4.  **Synthesis (Task 24)**: Generation of final reports, tables, and documentation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    study_config : Dict[str, Any]\n",
        "        The master configuration dictionary defining model physics and data schemas.\n",
        "    raw_datasets : Dict[str, pd.DataFrame]\n",
        "        The raw input data for all countries.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A comprehensive dictionary containing the outputs of every stage of the project.\n",
        "    \"\"\"\n",
        "    project_start_time = datetime.now()\n",
        "    logger.info(\"INITIATING MINSKY RESEARCH PROJECT EXECUTION\")\n",
        "\n",
        "    try:\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 1: Main Estimation Pipeline (Task 19)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\">>> Executing Phase 1: Main Estimation Pipeline (Task 19)\")\n",
        "        # run_minsky_msvar_pipeline is the orchestrator for Tasks 1-18\n",
        "        pipeline_results = run_minsky_msvar_pipeline(study_config, raw_datasets)\n",
        "\n",
        "        # Extract key artifacts needed for subsequent phases\n",
        "        msvar_estimates = pipeline_results[\"msvar_estimates\"]\n",
        "        bivariate_systems = pipeline_results[\"data\"][\"bivariate_systems\"] # Implicitly available in data artifacts\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 2: Robustness Analysis (Task 20)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\">>> Executing Phase 2: Monte Carlo Robustness (Task 20)\")\n",
        "        # run_monte_carlo_robustness orchestrates Tasks 21 (Simulation) and 22 (Aggregation)\n",
        "        mc_results = run_monte_carlo_robustness(msvar_estimates, study_config)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 3: Cross-Validation (Task 23)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\">>> Executing Phase 3: Cross-Validation (Task 23)\")\n",
        "        validation_results = cross_validate_results(msvar_estimates)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Phase 4: Final Synthesis (Task 24)\n",
        "        # ----------------------------------------------------------------------\n",
        "        logger.info(\">>> Executing Phase 4: Final Synthesis (Task 24)\")\n",
        "\n",
        "        # Extract components for synthesis\n",
        "        data_artifacts = pipeline_results.get(\"data\", {}) # Cleansed data, cycles\n",
        "        diagnostics = pipeline_results.get(\"ljung_box_tests\", {})\n",
        "        analysis = {\n",
        "            \"minsky_conditions\": pipeline_results.get(\"minsky_analysis\", {}),\n",
        "            \"regime_probabilities\": pipeline_results.get(\"regime_probabilities\", {})\n",
        "        }\n",
        "\n",
        "        final_report = synthesize_final_report(\n",
        "            config=study_config,\n",
        "            data_artifacts=data_artifacts,\n",
        "            estimates=msvar_estimates,\n",
        "            diagnostics=diagnostics,\n",
        "            analysis=analysis,\n",
        "            mc_results=mc_results,\n",
        "            validation=validation_results\n",
        "        )\n",
        "\n",
        "        project_end_time = datetime.now()\n",
        "        duration = project_end_time - project_start_time\n",
        "\n",
        "        logger.info(f\"PROJECT EXECUTION COMPLETE. Total Duration: {duration}\")\n",
        "\n",
        "        return final_report\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Project execution failed: {e}\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "QAbITWCK_Nzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "f6P9ab7mOFWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "HyW1219BN8ET"
      }
    }
  ]
}